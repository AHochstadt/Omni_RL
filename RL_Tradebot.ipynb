{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import in_notebook, utcToChi, getChiTimeNow\n",
    "from Initial_Combination_Data import getCombDF\n",
    "from Data_Preparation import locateConfigDir, processData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_chg_incs = [1, 3, 5] # number of days to calculate for day_cols\n",
    "minute_incs = [1, 5, 15, 30, 60] # number of minutes in the past to calculate for minute cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeled after https://github.com/pskrunner14/trading-bot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os, pytz, datetime\n",
    "\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "\n",
    "if in_notebook():\n",
    "    from tqdm import tqdm_notebook as tqdm\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "omni_dir = '/home/andrew/All_Trading/Studies/Omni_Project/'\n",
    "rl_dir = omni_dir + 'RL/'\n",
    "# config_dir = omni_dir + 'Primary_Assets/US_Dollar_Index/Config1/'\n",
    "tdm_dir = '/media/andrew/FreeAgent Drive/Market_Data/Tick_Data_Manager/'\n",
    "regression_results_fn = '/media/andrew/FreeAgent Drive/Market_Data/Tick_Data_Manager/regression_results.csv'\n",
    "data_summary_fn = tdm_dir+'data_summary.csv'\n",
    "data_summaryDF = pd.read_csv(data_summary_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Structure"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- Omni_Project/\n",
    "-- Omni.py\n",
    "-- RL.py\n",
    "-- ...\n",
    "-- Primary_Assets/\n",
    "--- 3M_CO/\n",
    "---- Config1/\n",
    "----- Data/\n",
    "------ all_minutes.csv\n",
    "------ daily_summary.csv\n",
    "------ sec_guide.csv\n",
    "----- Deep_Models/\n",
    "------ all_progress_summary.csv\n",
    "------ [Dense_64_rel][Drop_0.2][Dense_32_rel][Dense_3_lin]/\n",
    "------- Saved_Models/\n",
    "-------- 0.h5\n",
    "-------- 1.h5\n",
    "------- Trade_Logs/\n",
    "-------- 0.csv\n",
    "------- progress_summary.csv\n",
    "----- Plots/ ??\n",
    "----- Postprocessed_Data/\n",
    "----- log.txt\n",
    "----- progress_notes.txt\n",
    "----- summary_sheet.html\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Things models have to share:\n",
    "- Same ModelConfig (str)\n",
    "- Same column inputs\n",
    "\n",
    "Things models don't have to share:\n",
    "- model compilation loss\n",
    "- model compilation loss params (huber clip delta)\n",
    "- train (or val) dates\n",
    "- model compilation optimizer\n",
    "- RL params\n",
    "-- strategy\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Things to record in addition to (Things models don't have to share) above:\n",
    "- huber loss clip delta\n",
    "\n",
    "\n",
    "- strategy ('t-dqn')\n",
    "- reset_every\n",
    "\n",
    "- self.epsilon = 1.0\n",
    "- self.epsilon_min = 0.01\n",
    "- self.epsilon_decay = 0.995\n",
    "- self.learning_rate = 0.001\n",
    "\n",
    "Things to record each episode:\n",
    "- how many loss updates\n",
    "- how many target network updates\n",
    "- human time taken\n",
    "- num trades\n",
    "- sharpe\n",
    "- total data iterations\n",
    "- timesteps spent short\n",
    "- timesteps spent long\n",
    "- timesteps spent flat\n",
    "- batch_size (exp replay)\n",
    "- (some measure of how weights changed)\n",
    "- max drawdown\n",
    "- max drawup\n",
    "- isEval\n",
    "- data_start\n",
    "- data_end\n",
    "- data_skip\n",
    "- likelihood info for crossing large spreads\n",
    "- train or eval session\n",
    "- loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine which combinations of data to try\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting combDF using  /media/andrew/FreeAgent Drive/Market_Data/Tick_Data_Manager/regression_results.csv and /media/andrew/FreeAgent Drive/Market_Data/Tick_Data_Manager/data_summary.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/anaconda3/envs/env1/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (5,6,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 rows have R2 not found:\n",
      "\n",
      "            Sec1         Sec2\n",
      "66174  AETNA_INC  Natural_Gas\n",
      "found 539 unique securities in combDF\n",
      "[]\n",
      "num_days < min_num_days_total! Skipping row for Natural_Gas\n",
      "num_days < min_num_days_total! Skipping row for Coloplast_AS\n"
     ]
    }
   ],
   "source": [
    "combDF = getCombDF(regression_results_fn, data_summary_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Model Config </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    def __init__(self, input_size, model_array=None, model_str=None, actions_size=3):\n",
    "        self.input_size = input_size # aka shape_size\n",
    "        self.actions_size = actions_size\n",
    "        self.model_array = model_array\n",
    "        self.model_str = model_str\n",
    "        \n",
    "        if model_array is not None and model_str is not None:\n",
    "            raise ValueError('Please supply only one: model_str or model_array')\n",
    "        if model_array is not None:\n",
    "            self.model_array = model_array\n",
    "            self.model_str = self.getModelStr()\n",
    "        elif model_str is not None:\n",
    "            self.model_str = model_str\n",
    "            self.model_array = self.getModelArray()\n",
    "        else:\n",
    "            raise ValueError('You have to supply either a model string or a model array')\n",
    "            \n",
    "        self.checkInputAndOutput()\n",
    "        \n",
    "        self.model = self.makeKerasModel()\n",
    "        self.html_id_str = self.getHTMLIDStr()\n",
    "    \n",
    "    def getModelStr(self):\n",
    "        if self.model_str is not None:\n",
    "            return self.model_str\n",
    "        return_str = ''\n",
    "        for layer_dict in self.model_array:\n",
    "            layer_str = 'not set yet'\n",
    "            if layer_dict['layer'] == 'Dense':\n",
    "                layer_str = 'Dense_'+str(layer_dict['units'])+'_'+layer_dict['activation'][:3]\n",
    "            elif layer_dict['layer'] == 'Dropout':\n",
    "                layer_str = 'Drop_'+str(layer_dict['rate'])\n",
    "            else:\n",
    "                raise ValueError(layer_dict['layer']+' not recognized.')\n",
    "            return_str += '['+layer_str+']'\n",
    "        return return_str\n",
    "    \n",
    "    def getModelArray(self):\n",
    "        if self.model_array is not None:\n",
    "            return self.model_array\n",
    "        raise NotImplemented\n",
    "        \n",
    "    def makeKerasModel(self):\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Dense(units=self.model_array[0]['units'], activation=self.model_array[0]['activation'], input_shape=(self.input_size,)))\n",
    "        \n",
    "        for layer_dict in self.model_array[1:]:\n",
    "            if layer_dict['layer'] == 'Dense':\n",
    "                model.add(Dense(units=layer_dict['units'], activation=layer_dict['activation']))\n",
    "            elif layer_dict['layer'] == 'Dropout':\n",
    "                model.add(Dropout(rate=layer_dict['rate']))\n",
    "            else:\n",
    "                raise ValueError(layer_dict['layer']+' not recognized.')\n",
    "        return model\n",
    "        \n",
    "    def checkInputAndOutput(self):\n",
    "        assert self.model_array[0]['layer'] == 'Dense'\n",
    "        assert self.model_array[-1]['layer'] == 'Dense'\n",
    "        assert self.model_array[-1]['activation'] in ['linear', 'softmax']\n",
    "        assert self.model_array[-1]['units'] == self.actions_size\n",
    "        \n",
    "    def getHTMLIDStr(self):\n",
    "        out_str = ''\n",
    "        layer_reps = [i.split('[')[1] for i in self.model_str.split(']') if i!='']\n",
    "        for l in layer_reps:\n",
    "            out_str += l.replace('.', 'dp')+'___'\n",
    "        return out_str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataObj:\n",
    "    def __init__(self, config_dir, \n",
    "                 train_close_pricesDF=None, trainDF=None, val_close_pricesDF=None, valDF=None,\n",
    "                 load_val=True, nrows=None):\n",
    "        \"\"\"Data class. You have the option of loading all four dataframes to avoid reloading them.\"\"\"\n",
    "        self.train_close_pricesDF = None\n",
    "        self.trainDF = None\n",
    "        self.val_close_pricesDF = None\n",
    "        self.valDF = None\n",
    "        self.train_minutes = None\n",
    "        self.val_minutes = None\n",
    "        self.config_dir = config_dir\n",
    "        self.load_val = load_val\n",
    "        \n",
    "        if (train_close_pricesDF is not None and trainDF is not None and \n",
    "                 val_close_pricesDF is not None and valDF is not None):\n",
    "            self.train_close_pricesDF = train_close_pricesDF.copy()\n",
    "            self.trainDF = trainDF.copy()\n",
    "            self.val_close_pricesDF = val_close_pricesDF.copy()\n",
    "            self.valDF = valDF.copy()\n",
    "            if nrows is not None:\n",
    "                print('presupplied data with nrows =',nrows,'... subsetting the DFs.')\n",
    "                self.train_close_pricesDF = self.train_close_pricesDF[:nrows]\n",
    "                self.trainDF = self.trainDF[:nrows]\n",
    "                self.val_close_pricesDF = self.val_close_pricesDF[:nrows]\n",
    "                self.valDF = self.valDF[:nrows]\n",
    "                \n",
    "        else:\n",
    "            print('loading train_close_pricesDF')\n",
    "            self.train_close_pricesDF = pd.read_csv(config_dir+'Postprocessed_Data/train_close_prices.csv', nrows=nrows)\n",
    "            print('loading trainDF')\n",
    "            self.trainDF = pd.read_csv(config_dir+'Postprocessed_Data/train_minutesDF.csv', nrows=nrows)\n",
    "            print('load train complete')\n",
    "            \n",
    "            if load_val:\n",
    "                print('loading val_close_pricesDF')\n",
    "                self.val_close_pricesDF = pd.read_csv(config_dir+'Postprocessed_Data/val_close_prices.csv', nrows=nrows)\n",
    "                print('loading valDF')\n",
    "                self.valDF = pd.read_csv(config_dir+'Postprocessed_Data/val_minutesDF.csv', nrows=nrows)\n",
    "                print('load val complete')\n",
    "            \n",
    "        self.splitMinutes()\n",
    "        \n",
    "    def getStateSize(self):\n",
    "        return len(self.trainDF.columns)+1 #+1 for position\n",
    "    \n",
    "    def splitMinutes(self):\n",
    "        print('splitting minutes')\n",
    "        self.train_minutes = self.trainDF.Minute\n",
    "        self.trainDF.drop(columns=['Minute'], inplace=True)\n",
    "        assert all(self.train_close_pricesDF.Minute.values == self.train_minutes.values)\n",
    "        self.train_close_pricesDF.drop(columns=['Minute'], inplace=True)\n",
    "        \n",
    "        if self.valDF is not None:\n",
    "            self.val_minutes = self.valDF.Minute\n",
    "            self.valDF.drop(columns=['Minute'], inplace=True)\n",
    "            assert all(self.val_close_pricesDF.Minute.values == self.val_minutes.values)\n",
    "            self.val_close_pricesDF.drop(columns=['Minute'], inplace=True)\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Agent </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  agent.py\n",
    "import random\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import clone_model\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def huber_loss(y_true, y_pred, clip_delta=1.0):\n",
    "    \"\"\"Huber loss - Custom Loss Function for Q Learning\n",
    "    \n",
    "    Links: \thttps://en.wikipedia.org/wiki/Huber_loss\n",
    "            https://jaromiru.com/2017/05/27/on-using-huber-loss-in-deep-q-learning/\n",
    "    \"\"\"\n",
    "    error = y_true - y_pred\n",
    "    cond = K.abs(error) <= clip_delta\n",
    "    squared_loss = 0.5 * K.square(error)\n",
    "    quadratic_loss = 0.5 * K.square(clip_delta) + clip_delta * (K.abs(error) - clip_delta)\n",
    "    return K.mean(tf.where(cond, squared_loss, quadratic_loss))\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\" Stock Trading Bot \"\"\"\n",
    "    def __init__(self, model_cfg, config_dir, strategy=\"t-dqn\", reset_every=200, searchForPretrained=True, action_size=3):\n",
    "        self.model_cfg = model_cfg\n",
    "        self.config_dir = config_dir\n",
    "        self.model_dir = self.config_dir + 'Deep_Models/' + self.model_cfg.model_str + '/'\n",
    "        if not os.path.exists(self.model_dir):\n",
    "            print(self.model_dir, 'not found. Creating it.')\n",
    "            os.mkdir(self.model_dir)\n",
    "            os.mkdir(self.model_dir+'Saved_Models/')\n",
    "            os.mkdir(self.model_dir+'Trade_Logs/')\n",
    "        \n",
    "        self.strategy = strategy\n",
    "        self.state_size = self.model_cfg.input_size\n",
    "        self.action_size = action_size\n",
    "        self.trade_log = None\n",
    "        self.reset()\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.first_iter = True\n",
    "        \n",
    "        self.position = 0\n",
    "        \n",
    "        self.gamma = 0.95 # affinity for long term reward\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.loss = huber_loss\n",
    "#         self.custom_objects = {\"huber_loss\": huber_loss}  # important for loading the model from memory\n",
    "        self.optimizer = Adam(lr=self.learning_rate)\n",
    "    \n",
    "        self.num_loss_updates = 0\n",
    "        self.num_target_updates = 0\n",
    "    \n",
    "        \n",
    "        self.episode = 0\n",
    "        self.ts_seen = 1\n",
    "        if searchForPretrained:\n",
    "            print('searching for pretrained model')\n",
    "            self.model = self.loadModelWeights()\n",
    "        else:\n",
    "            self.model = self.model_cfg.model\n",
    "            \n",
    "        self.model.compile(loss=self.loss, optimizer=self.optimizer)\n",
    "        \n",
    "        # strategy config\n",
    "        if self.strategy in [\"t-dqn\", \"double-dqn\"]:\n",
    "            self.reset_every = reset_every\n",
    "            \n",
    "            # target network\n",
    "            self.target_model = clone_model(self.model)\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Adds relevant data to memory\n",
    "        \"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        self.ts_seen += 1\n",
    "    \n",
    "    def act(self, state, is_eval=False):\n",
    "        \"\"\"Take action from given possible set of actions\n",
    "        \"\"\"\n",
    "        # take random action in order to diversify experience at the beginning\n",
    "        if not is_eval and random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        if self.first_iter:\n",
    "            self.first_iter = False\n",
    "            return 1 # make a definite buy on the first iter\n",
    "        \n",
    "        action_probs = self.model.predict(state)\n",
    "        return np.argmax(action_probs[0])\n",
    "    \n",
    "    def getQValues(self, state):\n",
    "        q_values = self.model.predict(state)\n",
    "        return q_values\n",
    "    \n",
    "    def train_experience_replay(self, batch_size):\n",
    "        \"\"\"Train on previous experiences in memory\n",
    "        \"\"\"\n",
    "        sample_dist = np.array([i for i in range(len(self.memory))])\n",
    "        sample_dist = sample_dist/sum(sample_dist)\n",
    "        \n",
    "        mini_batch_idx = np.random.choice(range(len(self.memory)), batch_size, p=sample_dist)\n",
    "        mini_batch = [self.memory[i] for i in mini_batch_idx]\n",
    "        X_train, y_train = [], []\n",
    "        \n",
    "        # DQN\n",
    "        if self.strategy == \"dqn\":\n",
    "            for state, action, reward, next_state, done in mini_batch:\n",
    "                if done:\n",
    "                    target = reward\n",
    "                else:\n",
    "                    # approximate deep q-learning equation\n",
    "                    target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "                \n",
    "                # estimate q-values based on current state\n",
    "                q_values = self.model.predict(state)\n",
    "                # update the target for current action based on discounted reward\n",
    "                q_values[0][action] = target\n",
    "                \n",
    "                X_train.append(state[0])\n",
    "                y_train.append(q_values[0])\n",
    "        \n",
    "        # DQN with fixed targets\n",
    "        elif self.strategy == \"t-dqn\":\n",
    "#             if self.ts_seen % self.reset_every == 0:\n",
    "            if self.ts_seen % self.reset_every < batch_size: # this is valid because now we're only training every batch_size timesteps\n",
    "                # reset target model weights\n",
    "                self.target_model.set_weights(self.model.get_weights())\n",
    "                self.num_target_updates += 1\n",
    "            \n",
    "            for state, action, reward, next_state, done in mini_batch:\n",
    "                if done:\n",
    "                    target = reward\n",
    "                else:\n",
    "                    # approximate deep q-learning equation with fixed targets\n",
    "                    target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])\n",
    "                \n",
    "                # estimate q-values based on current state\n",
    "                q_values = self.model.predict(state)\n",
    "                # update the target for current action based on discounted reward\n",
    "                q_values[0][action] = target\n",
    "                \n",
    "                X_train.append(state[0])\n",
    "                y_train.append(q_values[0])\n",
    "        \n",
    "        # Double DQN\n",
    "        elif self.strategy == \"double-dqn\":\n",
    "#             if self.ts_seen % self.reset_every == 0:\n",
    "            if self.ts_seen % self.reset_every < batch_size: # this is valid because now we're only training every batch_size timesteps\n",
    "                # reset target model weights\n",
    "                self.target_model.set_weights(self.model.get_weights())\n",
    "                self.num_target_updates += 1\n",
    "                \n",
    "            for state, action, reward, next_state, done in mini_batch:\n",
    "                if done:\n",
    "                    target = reward\n",
    "                else:\n",
    "                    # approximate double deep q-learning equation\n",
    "                    target = reward + self.gamma * self.target_model.predict(next_state)[0][np.argmax(self.model.predict(next_state)[0])]\n",
    "                \n",
    "                # estimate q-values based on current state\n",
    "                q_values = self.model.predict(state)\n",
    "                # update the target for current action based on discounted reward\n",
    "                q_values[0][action] = target\n",
    "                \n",
    "                X_train.append(state[0])\n",
    "                y_train.append(q_values[0])\n",
    "                \n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        \n",
    "        # update q-function parameters based on huber loss gradient\n",
    "        loss = self.model.fit(\n",
    "            np.array(X_train), np.array(y_train),\n",
    "            epochs=1, verbose=0\n",
    "        ).history[\"loss\"][0]\n",
    "        \n",
    "        # as the training goes on we want the agent to\n",
    "        # make less random and more optimal decisions\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "        self.num_loss_updates += 1\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def saveTradeLog(self, sess_type='train'):\n",
    "        # if there's already a file there, move it\n",
    "        fn = self.model_dir+'Trade_Logs/' + str(self.episode) + '.csv'\n",
    "        if sess_type != 'train':\n",
    "            fn = self.model_dir+'Trade_Logs/' + sess_type + str(self.episode) + '.csv'\n",
    "        if os.path.exists(fn):\n",
    "            print(fn, 'already exists! Moving to Previous/')\n",
    "            prev_dir = self.model_dir+'Trade_Logs/Previous/'\n",
    "            if not os.path.exists(prev_dir):\n",
    "                os.mkdir(prev_dir)\n",
    "            os.replace(fn, prev_dir+str(self.episode) + str(getChiTimeNow())[:10] + '.csv')\n",
    "        self.trade_log.to_csv(fn, index=False)\n",
    "    \n",
    "    def saveModelWeights(self):\n",
    "        # if there's already a file there, move it\n",
    "        fn = self.model_dir+'Saved_Models/'+str(self.episode)+'.h5'\n",
    "        if os.path.exists(fn):\n",
    "            print(fn, 'already exists! Moving to Previous/')\n",
    "            prev_dir = self.model_dir+'Saved_Models/Previous/'\n",
    "            if not os.path.exists(prev_dir):\n",
    "                os.mkdir(prev_dir)\n",
    "            os.replace(fn, prev_dir+str(self.episode) + str(getChiTimeNow())[:10] + '.h5')\n",
    "        self.model.save_weights(fn)\n",
    "        \n",
    "    \n",
    "    def loadModelWeights(self):\n",
    "        saved_models_dir = self.model_dir + 'Saved_Models/'\n",
    "        saved_models = os.listdir(saved_models_dir)\n",
    "        model = self.model_cfg.model\n",
    "        if len(saved_models) == 0:\n",
    "            print('No models saved. Starting from episode 0 with a new model.')\n",
    "        else:\n",
    "            episodes_saved = [int(i.split('.')[0]) for i in saved_models if i.split('.')[0].isdigit()]\n",
    "            print('Found models. Loading episode', max(episodes_saved))\n",
    "            model.load_weights(saved_models_dir+str(max(episodes_saved))+'.h5')\n",
    "            self.episode = max(episodes_saved) +1\n",
    "        return model\n",
    "    \n",
    "    def reset(self):\n",
    "        self.trade_log = pd.DataFrame(columns = ['Minute', 'DesiredAction', 'ActualAction', 'NewPosition', 'C_B', 'C_A'])\n",
    "        self.position = 0\n",
    "        self.num_loss_updates = 0\n",
    "        self.num_target_updates = 0\n",
    "        self.ts_seen = 1\n",
    "    \n",
    "    def updateTradeLog(self, minute, action, actual_action, new_position, close_prices):\n",
    "        self.trade_log = self.trade_log.append({'Minute': minute, \n",
    "                                                'DesiredAction': action, \n",
    "                                                'ActualAction': actual_action,\n",
    "                                                'NewPosition': new_position, \n",
    "                                                'C_B': close_prices.C_B1, \n",
    "                                                'C_A': close_prices.C_A1}, ignore_index=True)\n",
    "    def getNumTrades(self):\n",
    "        return(int((self.trade_log.ActualAction != 0).sum()))\n",
    "    \n",
    "    def viewResults(self):\n",
    "        print('viewing results')\n",
    "        # construct pnl\n",
    "        full_logDF = self.trade_log.copy()\n",
    "        full_logDF['Midpt'] = (full_logDF.C_B + full_logDF.C_A)/2\n",
    "        full_logDF['DayPNL'] = (full_logDF.ActualAction!=0)*(full_logDF.C_B - full_logDF.Midpt) #it's always negative half the bid-ask spread\n",
    "        full_logDF['OpenPNL'] = full_logDF.NewPosition.shift() * (full_logDF.Midpt - full_logDF.Midpt.shift())\n",
    "        full_logDF.OpenPNL.loc[0] = 0\n",
    "        full_logDF['TotalPNL'] = full_logDF.DayPNL + full_logDF.OpenPNL\n",
    "        full_logDF['CumPNL'] = full_logDF.TotalPNL.cumsum()\n",
    "        # [x] graph (from http://kitchingroup.cheme.cmu.edu/blog/2013/09/13/Plotting-two-datasets-with-very-different-scales/)\n",
    "        buys = full_logDF.loc[full_logDF.ActualAction == 1][['Minute', 'C_A']]\n",
    "        sells = full_logDF.loc[full_logDF.ActualAction == 2][['Minute', 'C_B']]\n",
    "        print('viewing figure')\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        ax1 = fig.add_subplot(111)\n",
    "        ax1.plot(full_logDF.Minute, full_logDF.Midpt, 'k-', linewidth=0.5)\n",
    "        ax1.plot(buys.Minute, buys.C_A, 'g+')\n",
    "        ax1.plot(sells.Minute, sells.C_B, 'r+')\n",
    "        ax1.set_ylabel('Price')\n",
    "        \n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(full_logDF.Minute, full_logDF.CumPNL, 'b-')\n",
    "        ax2.set_ylabel('PNL')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        # [] check that the total PNL sum is close\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = Agent(model_cfg, config_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ops.py\n",
    "# import os\n",
    "import math\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Performs sigmoid operation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if x < 0:\n",
    "            return 1 - 1 / (1 + math.exp(x))\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "    except Exception as err:\n",
    "        print(\"Error in sigmoid: \" + err)\n",
    "\n",
    "def get_state(agent, dataObj, t):\n",
    "    \"\"\"Returns the position and the t-th row of the data\"\"\"\n",
    "    ret_array = [agent.position] + list(dataObj.trainDF.iloc[t].values)\n",
    "    ret_array = np.reshape(ret_array, len(ret_array))\n",
    "    return(np.array([ret_array]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "\n",
    "# import os\n",
    "# import math\n",
    "# import logging\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# import keras.backend as K\n",
    "\n",
    "\n",
    "# Formats Position\n",
    "format_position = lambda price: ('-$' if price < 0 else '+$') + '{0:.2f}'.format(abs(price))\n",
    "\n",
    "\n",
    "# Formats Currency\n",
    "format_currency = lambda price: '${0:.2f}'.format(abs(price))\n",
    "\n",
    "\n",
    "def show_train_result(result, val_position, initial_offset):\n",
    "    \"\"\" Displays training results\n",
    "    \"\"\"\n",
    "    if val_position == initial_offset or val_position == 0.0:\n",
    "        logging.info('Episode {}/{} - Train Position: {}  Val Position: USELESS  Train Loss: {:.4f}'\n",
    "                     .format(result[0], result[1], format_position(result[2]), result[3]))\n",
    "    else:\n",
    "        logging.info('Episode {}/{} - Train Position: {}  Val Position: {}  Train Loss: {:.4f})'\n",
    "                     .format(result[0], result[1], format_position(result[2]), format_position(val_position), result[3],))\n",
    "\n",
    "\n",
    "def show_eval_result(model_name, profit, initial_offset):\n",
    "    \"\"\" Displays eval results\n",
    "    \"\"\"\n",
    "    if profit == initial_offset or profit == 0.0:\n",
    "        logging.info('{}: USELESS\\n'.format(model_name))\n",
    "    else:\n",
    "        logging.info('{}: {}\\n'.format(model_name, format_position(profit)))\n",
    "\n",
    "\n",
    "# def get_stock_data(stock_file):\n",
    "#     \"\"\"Reads stock data from csv file\n",
    "#     \"\"\"\n",
    "#     df = pd.read_csv(stock_file)\n",
    "#     return list(df['Adj Close'])\n",
    "\n",
    "\n",
    "def switch_k_backend_device():\n",
    "    \"\"\" Switches `keras` backend from GPU to CPU if required.\n",
    "    Faster computation on CPU (if using tensorflow-gpu).\n",
    "    \"\"\"\n",
    "    if K.backend() == \"tensorflow\":\n",
    "        logging.debug(\"switching to TensorFlow for CPU\")\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcOpenPnl(agent, dataObj, t):\n",
    "    last_t_midpt = sum(dataObj.train_close_pricesDF[['C_B1', 'C_A1']].iloc[t-1].values)/2\n",
    "    t_midpt = sum(dataObj.train_close_pricesDF[['C_B1', 'C_A1']].iloc[t].values)/2\n",
    "    return (agent.position * (t_midpt - last_t_midpt))\n",
    "\n",
    "def evaluateAction(action, agent, dataObj, t):\n",
    "    \"\"\"returns day_pnl and updates agent's position and trade_log\"\"\"\n",
    "    close_prices = dataObj.train_close_pricesDF[['C_B1', 'C_A1']].iloc[t]\n",
    "    t_midpt = sum(close_prices.values)/2\n",
    "    minute = dataObj.train_minutes.iloc[t]\n",
    "    \n",
    "    actual_action = np.nan\n",
    "    new_position = agent.position\n",
    "    day_pnl = 0\n",
    "    \n",
    "    # BUY\n",
    "    if action == 1:\n",
    "        if agent.position != 1: #actually take the action\n",
    "            actual_action = 1\n",
    "            day_pnl = t_midpt - close_prices['C_A1']\n",
    "            new_position = agent.position + 1\n",
    "        else:\n",
    "            actual_action = 0\n",
    "            \n",
    "    # SELL\n",
    "    elif action == 2:\n",
    "        if agent.position != -1: #actually take the action\n",
    "            actual_action = 2\n",
    "            day_pnl = close_prices['C_B1'] - t_midpt\n",
    "            new_position = agent.position - 1\n",
    "        else:\n",
    "            actual_action = 0\n",
    "    \n",
    "    # HOLD\n",
    "    elif action == 0:\n",
    "        actual_action = 0\n",
    "    \n",
    "    else:\n",
    "        raise ValueError('Action '+str(action)+' not recognized.')\n",
    "    \n",
    "    agent.position = new_position\n",
    "    agent.updateTradeLog(minute, action, actual_action, new_position, close_prices)\n",
    "    return day_pnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxDraw(pnl_series, direction):\n",
    "    direction = direction.lower()\n",
    "    assert direction in ['down', 'up'], 'invalid direction: '+str(direction)\n",
    "    cum_pnl = pnl_series.cumsum()\n",
    "    result = 0\n",
    "    if direction == 'down':\n",
    "        highest_peak = 0\n",
    "        for c in cum_pnl.values:\n",
    "            result = min(result, c-highest_peak)\n",
    "            highest_peak = max(highest_peak, c)\n",
    "    if direction == 'up':\n",
    "        lowest_trough = 0\n",
    "        for c in cum_pnl.values:\n",
    "            result = max(result, c-lowest_trough)\n",
    "            lowest_trough = min(lowest_trough, c)\n",
    "    return result\n",
    "\n",
    "def calcSharpe(trade_log):\n",
    "    daily_log = trade_log.groupby(trade_log.Minute.str[:10]).agg({'TotalPNL': 'sum'})\n",
    "    trading_days = 252\n",
    "    return np.sqrt(trading_days)*(daily_log.TotalPNL.mean()/daily_log.TotalPNL.std())\n",
    "\n",
    "def getTradeLogStats(agent):\n",
    "    trade_log = agent.trade_log.copy()\n",
    "    trade_log['Midpt'] = (trade_log.C_B + trade_log.C_A)/2\n",
    "    trade_log['DayPNL'] = (trade_log.ActualAction!=0)*(trade_log.C_B - trade_log.Midpt) #it's always negative half the bid-ask spread\n",
    "    trade_log['OpenPNL'] = trade_log.NewPosition.shift() * (trade_log.Midpt - trade_log.Midpt.shift())\n",
    "    trade_log.OpenPNL.loc[0] = 0\n",
    "    trade_log['TotalPNL'] = trade_log.DayPNL + trade_log.OpenPNL\n",
    "    \n",
    "    pnl = trade_log.TotalPNL.sum()\n",
    "    num_days = len(trade_log.Minute.str[:10].unique())\n",
    "    pnl_per_day = pnl/num_days\n",
    "    sharpe = calcSharpe(trade_log)\n",
    "    num_trades = (trade_log.ActualAction!=0).sum()\n",
    "    drawdown = getMaxDraw(trade_log.TotalPNL, 'down')\n",
    "    drawup = getMaxDraw(trade_log.TotalPNL, 'up')\n",
    "    ts_seen = len(trade_log)\n",
    "    pct_flat = (trade_log.NewPosition == 0).sum()/ts_seen\n",
    "    pct_long = (trade_log.NewPosition == 1).sum()/ts_seen\n",
    "    pct_short = (trade_log.NewPosition == -1).sum()/ts_seen\n",
    "    data_start = trade_log.Minute.iloc[0][:10]\n",
    "    data_end = trade_log.Minute.iloc[-1][:10]\n",
    "    \n",
    "    return pnl, pnl_per_day, sharpe, num_trades, drawdown, drawup, ts_seen, pct_flat, pct_long, pct_short, data_start, data_end\n",
    "\n",
    "def getSprCrossInfo(agent, dataObj):   \n",
    "        \n",
    "    trade_log = agent.trade_log.copy()\n",
    "    assert trade_log.Minute.iloc[-1] in dataObj.train_minutes.values or trade_log.Minute.iloc[0] in dataObj.val_minutes.values, trade_log.Minute\n",
    "    train_or_val = 'train'\n",
    "    \n",
    "    dataDF = dataObj.trainDF\n",
    "    minutes = dataObj.train_minutes\n",
    "    close_pricesDF = dataObj.train_close_pricesDF\n",
    "    \n",
    "    if trade_log.Minute.iloc[0] in dataObj.val_minutes.values:\n",
    "        train_or_val = 'val'\n",
    "        dataDF = dataObj.valDF\n",
    "        minutesDF = dataObj.val_minutes\n",
    "        close_pricesDF = dataObj.val_close_pricesDF\n",
    "        \n",
    "    # get large spread threshold\n",
    "    avg_spr = (close_pricesDF.C_A1 - close_pricesDF.C_B1).mean()\n",
    "    spr_quantile = (close_pricesDF.C_A1 - close_pricesDF.C_B1).quantile(q=.8)\n",
    "    lrg_spr_threshold = max(avg_spr*5, spr_quantile)\n",
    "    \n",
    "    # get stats\n",
    "    lrg_spr_trade_log = trade_log.loc[(trade_log.C_A - trade_log.C_B) > lrg_spr_threshold]\n",
    "    if len(lrg_spr_trade_log) == 0:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    \n",
    "    lrg_spr_minutes = lrg_spr_trade_log.Minute\n",
    "    lrg_spr_cross_pct = (lrg_spr_trade_log.ActualAction != 0).sum()/len(lrg_spr_trade_log)\n",
    "    \n",
    "    t_idx = minutes.loc[minutes.isin(lrg_spr_minutes)].index\n",
    "    q_value_array = [agent.getQValues(get_state(agent, dataObj, t)) for t in t_idx]\n",
    "    lrg_spr_cross_likelihood = np.mean([(q[0][0] != max(q[0]))  for q in q_value_array]) #needs q[0] because output is wrapped in unnecessary list\n",
    "    lrg_spr_hold_preference = np.mean([q[0][0]-max(q[0][1:]) for q in q_value_array])\n",
    "    \n",
    "    return lrg_spr_cross_pct, lrg_spr_cross_likelihood, lrg_spr_hold_preference\n",
    "\n",
    "def saveProgressSummary(agent, dataObj, batch_size, epsilon_start, epsilon_end, avg_loss_array, session_type, start_time, end_time):\n",
    "    pnl, pnl_per_day, sharpe, num_trades, drawdown, drawup, \\\n",
    "        ts_seen, pct_flat, pct_long, pct_short, data_start, data_end = getTradeLogStats(agent)\n",
    "    \n",
    "    lrg_spr_cross_pct, lrg_spr_cross_likelihood, lrg_spr_hold_preference = getSprCrossInfo(agent, dataObj)\n",
    "    \n",
    "    new_row_dict = {'Epi': agent.episode,\n",
    "                    'SessType': session_type,\n",
    "                    'Strat': agent.strategy, \n",
    "                    'PNL': round(pnl, 2),\n",
    "                    'PNLpDay': round(pnl_per_day, 2),\n",
    "                    'Sharpe': round(sharpe, 3),\n",
    "                    'AvgLoss': round(np.mean(avg_loss_array), 5),\n",
    "                    'NumTrades': num_trades,\n",
    "                    'DrawDown': round(drawdown, 2),\n",
    "                    'DrawUp': round(drawup, 2),\n",
    "                    'LrgSprPct': round(lrg_spr_cross_pct, 3),\n",
    "                    'LrgSprLikely': round(lrg_spr_cross_likelihood, 4),\n",
    "                    'LrgSprPref': round(lrg_spr_hold_preference, 5),\n",
    "                    'TimeTakenMin': round((end_time-start_time).seconds/60),\n",
    "                    'TsSeen': ts_seen,\n",
    "                    'PctFlat': round(pct_flat, 2),\n",
    "                    'PctLong': round(pct_long, 2),\n",
    "                    'PctShort': round(pct_short, 2),\n",
    "                    'DataStart': data_start,\n",
    "                    'DataEnd': data_end,\n",
    "                    'LossUpd': agent.num_loss_updates,\n",
    "                    'TargetUpd': agent.num_target_updates,\n",
    "                    'Gamma': agent.gamma,\n",
    "                    'EpsilonStart': round(epsilon_start, 2),\n",
    "                    'EpsilonEnd': round(epsilon_end, 2),\n",
    "                    'EpsilonDecay': round(agent.epsilon_decay, 3),\n",
    "                    'ResetEvery': agent.reset_every,\n",
    "                    'LossType': str(agent.loss).split('function ')[1].split(' ')[0].split('_loss')[0],\n",
    "                    'LossParam': np.nan, #maybe should actually do this later\n",
    "                    'OptType': str(agent.model.optimizer.__class__).split('.')[-1].replace('\\'', '').replace('>', ''), #https://stackoverflow.com/questions/49785536/get-learning-rate-of-keras-model\n",
    "                    'OptLR': K.eval(agent.model.optimizer.lr),\n",
    "                    'StartTime': start_time,\n",
    "                    'EndTime': end_time\n",
    "                   }\n",
    "    \n",
    "    col_order = ['Epi', 'SessType', 'Strat', \n",
    "                'PNL', 'PNLpDay', 'Sharpe', 'AvgLoss', 'NumTrades',\n",
    "                'DrawDown', 'DrawUp', 'LrgSprPct', 'LrgSprLikely', 'LrgSprPref',\n",
    "                'TimeTakenMin', 'TsSeen', 'PctFlat', 'PctLong', 'PctShort',\n",
    "                'DataStart', 'DataEnd', 'LossUpd', 'TargetUpd',\n",
    "                'Gamma', 'EpsilonStart', 'EpsilonEnd', 'EpsilonDecay', 'ResetEvery', \n",
    "                'LossType', 'LossParam', 'OptType', 'OptLR', 'StartTime', 'EndTime']\n",
    "    assert(set(col_order) == set(new_row_dict.keys()))\n",
    "    \n",
    "    progress_summary_fn = agent.model_dir+'progress_summary.csv'\n",
    "    progress_summaryDF = pd.DataFrame(new_row_dict, index=[0])\n",
    "    if os.path.exists(progress_summary_fn):\n",
    "        progress_summaryDF = pd.read_csv(progress_summary_fn)\n",
    "        progress_summaryDF = progress_summaryDF.append(new_row_dict, ignore_index=True)\n",
    "    progress_summaryDF[col_order].to_csv(progress_summary_fn, index=False)\n",
    "    \n",
    "    all_progress_summary_fn = agent.config_dir+'Deep_Models/all_progress_summary.csv'\n",
    "    col_order = ['Model'] + col_order\n",
    "    new_row_dict['Model'] = agent.model_cfg.model_str\n",
    "    \n",
    "    all_progress_summaryDF = pd.DataFrame(new_row_dict, index=[0])\n",
    "    \n",
    "    if os.path.exists(all_progress_summary_fn):\n",
    "        all_progress_summaryDF = pd.read_csv(all_progress_summary_fn)\n",
    "        all_progress_summaryDF = all_progress_summaryDF.append(new_row_dict, ignore_index=True)\n",
    "    all_progress_summaryDF[col_order].to_csv(all_progress_summary_fn, index=False)\n",
    "    print_keys = ['PNL', 'PNLpDay', 'Sharpe', 'AvgLoss', 'NumTrades', 'DrawDown', 'DrawUp', 'LrgSprPct', 'LrgSprLikely', 'LrgSprPref', 'TimeTakenMin', 'TsSeen', 'PctFlat', 'PctLong', 'PctShort', 'LossUpd', 'TargetUpd', 'EpsilonStart', 'EpsilonEnd']\n",
    "    print('\\t'.join([key+': '+str(new_row_dict[key]) for key in print_keys]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods.py\n",
    "# import os\n",
    "# import logging\n",
    "\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "def endEpisode(agent, dataObj, batch_size, epsilon_start, avg_loss_array, sess_type, start_time):\n",
    "    agent.saveTradeLog(sess_type)\n",
    "    \n",
    "#         if agent.episode % 10 == 0:\n",
    "    if agent.episode % 1 == 0 and sess_type == 'train':\n",
    "        agent.saveModelWeights()\n",
    "    \n",
    "    epsilon_end = agent.epsilon if sess_type == 'train' else np.nan\n",
    "    end_time = getChiTimeNow()\n",
    "    \n",
    "    saveProgressSummary(agent, dataObj, batch_size, epsilon_start, epsilon_end, avg_loss_array, \\\n",
    "                        sess_type, start_time, end_time)\n",
    "    agent.episode += 1\n",
    "\n",
    "def train_model(agent, dataObj, ep_count=20, batch_size=32):\n",
    "    start_ep = agent.episode+0\n",
    "    end_ep = start_ep + ep_count\n",
    "    for i in range(ep_count):\n",
    "        start_time = getChiTimeNow()\n",
    "        epsilon_start = agent.epsilon\n",
    "        total_profit = 0\n",
    "        data_length = len(dataObj.trainDF) - 1\n",
    "        avg_loss_array = []\n",
    "        agent.reset()\n",
    "        \n",
    "        state = get_state(agent, dataObj, 0)\n",
    "        try:\n",
    "            for t in tqdm(range(data_length), total=data_length, leave=True, desc='Episode {}/{}'.format(agent.episode, end_ep)):        \n",
    "                reward = 0\n",
    "                \n",
    "                # select an action\n",
    "                action = agent.act(state)\n",
    "                \n",
    "                open_pnl = calcOpenPnl(agent, dataObj, t+1)\n",
    "                day_pnl = evaluateAction(action, agent, dataObj, t+1)\n",
    "                reward = open_pnl + day_pnl\n",
    "                total_profit += reward\n",
    "                \n",
    "                next_state = get_state(agent, dataObj, t+1)\n",
    "                done = (t == data_length - 1)\n",
    "                agent.remember(state, action, reward, next_state, done)\n",
    "                \n",
    "                if len(agent.memory) > batch_size:\n",
    "                    # train every batch_size\n",
    "                    if t % batch_size == 0:\n",
    "                        loss = agent.train_experience_replay(batch_size)\n",
    "                        avg_loss_array.append(loss)\n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "            endEpisode(agent, dataObj, batch_size, epsilon_start, avg_loss_array, \\\n",
    "                                'train', start_time)\n",
    "        except (KeyboardInterrupt, SystemExit):\n",
    "            print('KeyboardInterrupt or SystemExit. Ending current episode.')\n",
    "            endEpisode(agent, dataObj, batch_size, epsilon_start, avg_loss_array, \\\n",
    "                                'train', start_time)\n",
    "            raise\n",
    "        except:\n",
    "            print('Unknown error...Ending current episode.')\n",
    "            endEpisode(agent, dataObj, batch_size, epsilon_start, avg_loss_array, \\\n",
    "                                'train', start_time)\n",
    "            raise\n",
    "\n",
    "def evaluate_model(agent, dataObj, debug=False, batch_size=32):\n",
    "    print('Evaluating Model')\n",
    "    start_time = getChiTimeNow()\n",
    "    epsilon_start = np.nan\n",
    "    total_profit = 0\n",
    "    data_length = len(dataObj.trainDF) - 1\n",
    "    avg_loss_array = []\n",
    "    agent.reset()\n",
    "    \n",
    "    state = get_state(agent, dataObj, 0)\n",
    "    \n",
    "    try:\n",
    "        for t in tqdm(range(data_length)):\n",
    "            reward = 0\n",
    "            # select an action\n",
    "            action = agent.act(state, is_eval=True)\n",
    "            \n",
    "            open_pnl = calcOpenPnl(agent, dataObj, t+1)\n",
    "            day_pnl = evaluateAction(action, agent, dataObj, t+1)\n",
    "            reward = open_pnl + day_pnl\n",
    "            total_profit += reward\n",
    "            \n",
    "            next_state = get_state(agent, dataObj, t+1)\n",
    "            \n",
    "            done = (t == data_length - 1)\n",
    "            \n",
    "    #         agent.memory.append((state, action, reward, next_state, done)) # don't know why this line was here instead of the below\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "        endEpisode(agent, dataObj, batch_size, epsilon_start, avg_loss_array, \\\n",
    "                   'eval', start_time)\n",
    "    except (KeyboardInterrupt, SystemExit):\n",
    "        print('KeyboardInterrupt or SystemExit. Ending current episode.')\n",
    "        endEpisode(agent, dataObj, batch_size, epsilon_start, avg_loss_array, \\\n",
    "                            'eval', start_time)\n",
    "        raise\n",
    "    except:\n",
    "        print('Unknown error...Ending current episode.')\n",
    "        endEpisode(agent, dataObj, batch_size, epsilon_start, avg_loss_array, \\\n",
    "                            'eval', start_time)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.episode = 0\n",
    "# dataObj = DataObj(config_dir, nrows=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_model(agent, dataObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [] save results to summary_sheet.html\n",
    "# [x] determine when to start and when to stop training each model\n",
    "# [] convert chg columns to EMA\n",
    "# [] move it all to Ext hard drive\n",
    "# [x] why don't we have any target updates?\n",
    "# [NA] why is LrgSprLikely likely the converse? It is (possibly) sadly likely correct\n",
    "# [x] print some statistics in endEpisode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = 3 # HOLD, BUY, SELL\n",
    "\n",
    "keras_models = [\n",
    "    [{'layer': 'Dense', 'activation': 'relu', 'units': 64},\n",
    "     {'layer': 'Dense', 'activation': 'linear', 'units': action_size}],\n",
    "    \n",
    "    [{'layer': 'Dense', 'activation': 'relu', 'units': 256},\n",
    "     {'layer': 'Dense', 'activation': 'linear', 'units': action_size}],\n",
    "    \n",
    "    [{'layer': 'Dense', 'activation': 'relu', 'units': 128},\n",
    "     {'layer': 'Dense', 'activation': 'relu', 'units': 64},\n",
    "     {'layer': 'Dense', 'activation': 'linear', 'units': action_size}],\n",
    "    \n",
    "    [{'layer': 'Dense', 'activation': 'relu', 'units': 256},\n",
    "     {'layer': 'Dense', 'activation': 'relu', 'units': 128},\n",
    "     {'layer': 'Dense', 'activation': 'linear', 'units': action_size}],\n",
    "    \n",
    "    [{'layer': 'Dense', 'activation': 'relu', 'units': 256},\n",
    "     {'layer': 'Dense', 'activation': 'relu', 'units': 128},\n",
    "     {'layer': 'Dense', 'activation': 'relu', 'units': 64},\n",
    "     {'layer': 'Dense', 'activation': 'linear', 'units': action_size}],\n",
    "\n",
    "    [{'layer': 'Dense', 'activation': 'relu', 'units': 256},\n",
    "     {'layer': 'Dense', 'activation': 'relu', 'units': 128},\n",
    "     {'layer': 'Dense', 'activation': 'relu', 'units': 64},\n",
    "     {'layer': 'Dense', 'activation': 'relu', 'units': 64},\n",
    "     {'layer': 'Dense', 'activation': 'relu', 'units': 64},\n",
    "     {'layer': 'Dense', 'activation': 'linear', 'units': action_size}],\n",
    "    \n",
    "    [{'layer': 'Dense', 'activation': 'relu', 'units': 256},\n",
    "     {'layer': 'Dense', 'activation': 'relu', 'units': 128},\n",
    "     {'layer': 'Dropout', 'rate': 0.2},\n",
    "     {'layer': 'Dense', 'activation': 'relu', 'units': 64},\n",
    "     {'layer': 'Dense', 'activation': 'relu', 'units': 64},\n",
    "     {'layer': 'Dense', 'activation': 'relu', 'units': 64},\n",
    "     {'layer': 'Dense', 'activation': 'linear', 'units': action_size}]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sec = 'TYSON_FOODS_INC-CL_A'\n",
    "# comb_subDF = combDF.loc[combDF.Sec1 == sec].reset_index(drop=True)\n",
    "# comb_row = comb_subDF.iloc[0]\n",
    "# config_dir = locateConfigDir(comb_row, omni_dir, day_chg_incs, minute_incs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_close_pricesDF, train_minutesDF, val_close_pricesDF, val_minutesDF = \\\n",
    "#             processData(comb_row, config_dir, day_chg_incs, minute_incs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataObj = DataObj(config_dir, train_close_pricesDF, train_minutesDF, val_close_pricesDF, val_minutesDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_array = keras_models[0]\n",
    "# model_cfg = ModelConfig(dataObj.getStateSize(), model_array=model_array)\n",
    "# model_cfg.model_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelHasPromise(model_str, config_dir, exhaustiveness_level):\n",
    "    print('Determining if model has promise:',model_str)\n",
    "    all_progress_summary_fn = config_dir + 'Deep_Models/all_progress_summary.csv'\n",
    "    if not os.path.exists(all_progress_summary_fn):\n",
    "        print('all_progress_summary doesnt exist. Still has promise.')\n",
    "        return True\n",
    "    \n",
    "    all_progress_summaryDF = pd.read_csv(all_progress_summary_fn)\n",
    "    progress_subDF = all_progress_summaryDF.loc[all_progress_summaryDF.Model == model_str]\n",
    "    progress_subDF = progress_subDF.loc[progress_subDF.SessType == 'train']\n",
    "    progress_subDF = progress_subDF.loc[progress_subDF.TsSeen >= 10000]\n",
    "    progress_subDF.reset_index(inplace=True, drop=True)\n",
    "    if len(progress_subDF) < 10:\n",
    "        print(\"We've only had\", len(progress_subDF), 'sufficient episodes. Still has promise.')\n",
    "        return True\n",
    "    \n",
    "    min_loss_updates = 10000*exhaustiveness_level\n",
    "    if progress_subDF.LossUpd.sum() < min_loss_updates:\n",
    "        print('We need', min_loss_updates-progress_subDF.LossUpd.sum(),'/', min_loss_updates, 'more loss updates for this exhaustiveness level.')\n",
    "        return True\n",
    "    \n",
    "    min_target_updates = round(min_loss_updates*.7)\n",
    "    if progress_subDF.TargetUpd.sum() < min_target_updates:\n",
    "        print('We need', min_target_updates-progress_subDF.TargetUpd.sum(),'/', min_target_updates, 'more target updates for this exhaustiveness level.')\n",
    "        return True\n",
    "    \n",
    "    # check if the trajectory is improving. Make sure to divide loss and pnl ts_seen\n",
    "    loss_tolerance_pct = .02*exhaustiveness_level\n",
    "    pnl_tolerance_pct = .02*exhaustiveness_level\n",
    "    \n",
    "    # check if the best of the past 3 trials is at least <tolerance> better than the avg of (t-5, t-10)\n",
    "    best_recent_loss = (progress_subDF.iloc[-3:].AvgLoss/progress_subDF.iloc[-3:].TsSeen).min()\n",
    "    best_recent_pnl = (progress_subDF.iloc[-3:].PNL/progress_subDF.iloc[-3:].TsSeen).max()\n",
    "    avg_prev_loss = (progress_subDF.iloc[-10:-5].AvgLoss/progress_subDF.iloc[-10:-5].TsSeen).mean()\n",
    "    avg_prev_pnl = (progress_subDF.iloc[-10:-5].PNL/progress_subDF.iloc[-10:-5].TsSeen).mean()\n",
    "    loss_improvement = (avg_prev_loss - best_recent_loss)/avg_prev_loss\n",
    "    pnl_improvement = (best_recent_pnl - avg_prev_pnl)/avg_prev_pnl\n",
    "    if pnl_improvement < pnl_tolerance_pct:\n",
    "        print('PNL improvement is only ',round(pnl_improvement*100, 3),'%. Does not qualify as promising.')\n",
    "        return False\n",
    "    \n",
    "    if loss_improvement < loss_tolerance_pct:\n",
    "        print('Loss improvement is only ',round(loss_improvement*100, 3),'%. Does not qualify as promising.')\n",
    "        return False\n",
    "    print('PNL and Loss Improvement checks passed with values', round(pnl_improvement*100, 3), \n",
    "          '% and', round(loss_improvement*100, 3),'%. Still has promise.')\n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoTrain(sec, combDF=combDF, nrows=None, exhaustiveness_level=2, keras_models=keras_models, omni_dir=omni_dir, \n",
    "              day_chg_incs=day_chg_incs, minute_incs=minute_incs, email_progress=False):\n",
    "    \"\"\"\n",
    "    1. Locates or prepares config_dir\n",
    "    2. Locates or prepares postprepared data\n",
    "    3. Check model progress. If model doesn't look promising, move to next model.\n",
    "            If model still has promise, keep training it\n",
    "    4. Optionally emails results\n",
    "    \"\"\"\n",
    "    assert exhaustiveness_level in [1,2,3,4,5]\n",
    "    \n",
    "    comb_row = combDF.loc[combDF.Sec1 == sec].iloc[0]\n",
    "    config_dir = locateConfigDir(comb_row, omni_dir, day_chg_incs, minute_incs)\n",
    "    train_close_pricesDF, train_minutesDF, val_close_pricesDF, val_minutesDF = \\\n",
    "            processData(comb_row, config_dir, day_chg_incs, minute_incs)\n",
    "    dataObj = DataObj(config_dir, train_close_pricesDF, train_minutesDF, val_close_pricesDF, val_minutesDF, nrows=nrows)\n",
    "    # [x] Check model progress\n",
    "    for model_array in keras_models:\n",
    "        model_cfg = ModelConfig(dataObj.getStateSize(), model_array=model_array)\n",
    "        agent = Agent(model_cfg, config_dir)\n",
    "        model_has_promise = modelHasPromise(model_cfg.model_str, config_dir, exhaustiveness_level)\n",
    "        while model_has_promise: \n",
    "            for i in range(5):\n",
    "                train_model(agent, dataObj, ep_count=5)\n",
    "                evaluate_model(agent, dataObj)\n",
    "            model_has_promise = modelHasPromise(model_cfg.model_str, config_dir, exhaustiveness_level)\n",
    "        else:\n",
    "            print('Model does not have promise. Moving on.')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sec = 'US_Dollar_Index'\n",
    "sec = 'TYSON_FOODS_INC-CL_A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing matching config dir found: /home/andrew/All_Trading/Studies/Omni_Project/Primary_Assets/TYSON_FOODS_INC-CL_A/Config1\n",
      "All Postprocessed_Data files exist. Checking columns\n",
      "Columns passed the check. Loading postprocessed data.\n",
      "loading train_close_pricesDF\n",
      "loading train_minutesDF\n",
      "loading val_close_pricesDF\n",
      "loading val_minutesDF\n",
      "loading complete.\n",
      "presupplied data with nrows = 15000 ... subsetting the DFs.\n",
      "splitting minutes\n",
      "searching for pretrained model\n",
      "Found models. Loading episode 61\n",
      "Determining if model has promise: [Dense_64_rel][Dense_3_lin]\n",
      "We need 3541 more loss updates for this exhaustiveness level.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d1aa30d745142aa978321832023a56b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Episode 62/67', max=14999, style=ProgressStyle(description_wi"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PNL: -84.71\tPNLpDay: -2.17\tSharpe: -21.969\tAvgLoss: 0.32373\tNumTrades: 3983\tDrawDown: -87.69\tDrawUp: 3.2\tLrgSprPct: 0.406\tLrgSprLikely: 0.971\tLrgSprPref: -0.55951\tTimeTakenMin: 2\tTsSeen: 14999\tPctFlat: 0.25\tPctLong: 0.42\tPctShort: 0.33\tLossUpd: 468\tTargetUpd: 74\tEpsilonStart: 1.0\tEpsilonEnd: 0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117e1d7c3cbc499b8b34c24c752dc2ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Episode 63/67', max=14999, style=ProgressStyle(description_wi"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PNL: -27.31\tPNLpDay: -0.7\tSharpe: -8.527\tAvgLoss: 0.15948\tNumTrades: 1321\tDrawDown: -35.15\tDrawUp: 4.69\tLrgSprPct: 0.319\tLrgSprLikely: 0.3768\tLrgSprPref: 0.15974\tTimeTakenMin: 3\tTsSeen: 14999\tPctFlat: 0.1\tPctLong: 0.52\tPctShort: 0.37\tLossUpd: 469\tTargetUpd: 75\tEpsilonStart: 0.1\tEpsilonEnd: 0.01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b2458bb66f415684da4a4e74322b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Episode 64/67', max=14999, style=ProgressStyle(description_wi"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PNL: -18.56\tPNLpDay: -0.48\tSharpe: -5.892\tAvgLoss: 0.1829\tNumTrades: 979\tDrawDown: -27.26\tDrawUp: 7.51\tLrgSprPct: 0.261\tLrgSprLikely: 1.0\tLrgSprPref: -0.50929\tTimeTakenMin: 2\tTsSeen: 14999\tPctFlat: 0.07\tPctLong: 0.55\tPctShort: 0.39\tLossUpd: 469\tTargetUpd: 75\tEpsilonStart: 0.01\tEpsilonEnd: 0.01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d95167340941eea8c771ce469432cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Episode 65/67', max=14999, style=ProgressStyle(description_wi"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PNL: -17.88\tPNLpDay: -0.46\tSharpe: -7.856\tAvgLoss: 0.15196\tNumTrades: 1053\tDrawDown: -20.53\tDrawUp: 3.11\tLrgSprPct: 0.29\tLrgSprLikely: 0.3188\tLrgSprPref: 0.07689\tTimeTakenMin: 2\tTsSeen: 14999\tPctFlat: 0.08\tPctLong: 0.45\tPctShort: 0.46\tLossUpd: 469\tTargetUpd: 75\tEpsilonStart: 0.01\tEpsilonEnd: 0.01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac7aabe63f4469785e5f25620fb591d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Episode 66/67', max=14999, style=ProgressStyle(description_wi"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PNL: -9.11\tPNLpDay: -0.23\tSharpe: -3.516\tAvgLoss: 0.14946\tNumTrades: 767\tDrawDown: -15.66\tDrawUp: 6.9\tLrgSprPct: 0.232\tLrgSprLikely: 0.5507\tLrgSprPref: -0.07032\tTimeTakenMin: 3\tTsSeen: 14999\tPctFlat: 0.06\tPctLong: 0.53\tPctShort: 0.41\tLossUpd: 469\tTargetUpd: 75\tEpsilonStart: 0.01\tEpsilonEnd: 0.01\n",
      "Evaluating Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "756022124ada419a9cea5c4af0dc5bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=14999), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "autoTrain(sec, nrows=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoTrain(sec, nrows=15000*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env1)",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
