{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [x] Determine which combinations of data to try\n",
    "# [] getData fxn\n",
    "# [x] Determine which sets of dates to try\n",
    "# [] Determine types of models to try\n",
    "# [] Try models\n",
    "# [] Save results\n",
    "# [] Use reinforcement learning to come up with bots (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting combDF using  /media/andrew/FreeAgent Drive/Market_Data/Tick_Data_Manager/regression_results.csv and /media/andrew/FreeAgent Drive/Market_Data/Tick_Data_Manager/data_summary.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/anaconda3/envs/env1/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (5,6,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 rows have R2 not found:\n",
      "\n",
      "            Sec1         Sec2\n",
      "66174  AETNA_INC  Natural_Gas\n",
      "found 539 unique securities in combDF\n",
      "[]\n",
      "num_days < min_num_days_total! Skipping row for Natural_Gas\n",
      "num_days < min_num_days_total! Skipping row for Coloplast_AS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sec1</th>\n",
       "      <th>Sec2</th>\n",
       "      <th>Sec3</th>\n",
       "      <th>Sec4</th>\n",
       "      <th>Sec5</th>\n",
       "      <th>Sec6</th>\n",
       "      <th>Sec7</th>\n",
       "      <th>Sec8</th>\n",
       "      <th>Sec9</th>\n",
       "      <th>R2_2</th>\n",
       "      <th>...</th>\n",
       "      <th>TotalEndDate7</th>\n",
       "      <th>TotalStartDate8</th>\n",
       "      <th>TotalEndDate8</th>\n",
       "      <th>TotalStartDate9</th>\n",
       "      <th>TotalEndDate9</th>\n",
       "      <th>LastStartDate</th>\n",
       "      <th>FirstEndDate</th>\n",
       "      <th>TrainStartDate</th>\n",
       "      <th>ValStartDate</th>\n",
       "      <th>ValEndDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3M_CO</td>\n",
       "      <td>iShares_Russell_1000_Value_ETF</td>\n",
       "      <td>iShares_Russell_1000_Growth_ETF</td>\n",
       "      <td>iShares_Core_S&amp;P_Mid-Cap_ETF</td>\n",
       "      <td>STATE_STREET_CORP</td>\n",
       "      <td>BB&amp;T_CORP</td>\n",
       "      <td>T_ROWE_PRICE_GROUP_INC</td>\n",
       "      <td>DISCOVER_FINANCIAL_SERVICES</td>\n",
       "      <td>STANLEY_BLACK_&amp;_DECKER_INC</td>\n",
       "      <td>0.660427</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-12-19</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-13</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-18</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-06</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-04-21</td>\n",
       "      <td>2019-10-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ABBOTT_LABORATORIES</td>\n",
       "      <td>iShares_Russell_1000_Value_ETF</td>\n",
       "      <td>iShares_Russell_1000_Growth_ETF</td>\n",
       "      <td>iShares_Core_S&amp;P_Mid-Cap_ETF</td>\n",
       "      <td>STATE_STREET_CORP</td>\n",
       "      <td>DISCOVER_FINANCIAL_SERVICES</td>\n",
       "      <td>BECTON_DICKINSON_AND_CO</td>\n",
       "      <td>BB&amp;T_CORP</td>\n",
       "      <td>PRUDENTIAL_FINANCIAL_INC</td>\n",
       "      <td>0.682331</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-12-17</td>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>2019-12-06</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-13</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-06</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-04-21</td>\n",
       "      <td>2019-10-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ACTIVISION_BLIZZARD_INC</td>\n",
       "      <td>iShares_Russell_1000_Growth_ETF</td>\n",
       "      <td>iShares_Russell_1000_Value_ETF</td>\n",
       "      <td>iShares_Core_S&amp;P_Mid-Cap_ETF</td>\n",
       "      <td>Vanguard_FTSE_Europe_ETF</td>\n",
       "      <td>XILINX_INC</td>\n",
       "      <td>AUTODESK_INC</td>\n",
       "      <td>ELECTRONIC_ARTS_INC</td>\n",
       "      <td>T_ROWE_PRICE_GROUP_INC</td>\n",
       "      <td>0.617346</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-12-19</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2019-12-13</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-19</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-10</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-04-23</td>\n",
       "      <td>2019-10-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ADOBE_SYSTEMS_INC</td>\n",
       "      <td>iShares_Russell_1000_Growth_ETF</td>\n",
       "      <td>AUTODESK_INC</td>\n",
       "      <td>iShares_Russell_1000_Value_ETF</td>\n",
       "      <td>iShares_Core_S&amp;P_Mid-Cap_ETF</td>\n",
       "      <td>T_ROWE_PRICE_GROUP_INC</td>\n",
       "      <td>Vanguard_FTSE_Europe_ETF</td>\n",
       "      <td>BB&amp;T_CORP</td>\n",
       "      <td>STATE_STREET_CORP</td>\n",
       "      <td>0.768484</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-12-19</td>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>2019-12-06</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-18</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-06</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-04-21</td>\n",
       "      <td>2019-10-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ADVANCED_MICRO_DEVICES</td>\n",
       "      <td>iShares_Russell_1000_Growth_ETF</td>\n",
       "      <td>iShares_Russell_1000_Value_ETF</td>\n",
       "      <td>iShares_Core_S&amp;P_Mid-Cap_ETF</td>\n",
       "      <td>XILINX_INC</td>\n",
       "      <td>Vanguard_FTSE_Europe_ETF</td>\n",
       "      <td>STATE_STREET_CORP</td>\n",
       "      <td>SUNTRUST_BANKS_INC</td>\n",
       "      <td>BB&amp;T_CORP</td>\n",
       "      <td>0.462562</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-12-18</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-06</td>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>2019-12-06</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-06</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-04-21</td>\n",
       "      <td>2019-10-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>491</td>\n",
       "      <td>Vinci_SA</td>\n",
       "      <td>Europe_50_Index</td>\n",
       "      <td>Air_Liquide_SA</td>\n",
       "      <td>Danone_SA</td>\n",
       "      <td>BASF_SE</td>\n",
       "      <td>L'Oreal_SA</td>\n",
       "      <td>Airbus_Group_SE</td>\n",
       "      <td>LVMH_Moet_Hennessy_Louis_Vuitton_SA</td>\n",
       "      <td>Allianz_SE</td>\n",
       "      <td>0.409605</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-12-23</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>2019-12-23</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>2019-12-23</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2019-12-10</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2019-04-06</td>\n",
       "      <td>2019-10-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>492</td>\n",
       "      <td>Vivendi_SA</td>\n",
       "      <td>iShares_Russell_1000_Growth_ETF</td>\n",
       "      <td>Vanguard_FTSE_Europe_ETF</td>\n",
       "      <td>Publicis_Groupe_SA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.202415</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-19</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-04-29</td>\n",
       "      <td>2019-10-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>493</td>\n",
       "      <td>Voestalpine_AG</td>\n",
       "      <td>ALCOA_INC</td>\n",
       "      <td>INTUITIVE_SURGICAL_INC</td>\n",
       "      <td>Vanguard_FTSE_Europe_ETF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.197433</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-19</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-04-29</td>\n",
       "      <td>2019-10-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>494</td>\n",
       "      <td>Volvo_AB</td>\n",
       "      <td>Atlas_Copco_AB</td>\n",
       "      <td>SKF_AB</td>\n",
       "      <td>Sandvik_AB</td>\n",
       "      <td>INTUITIVE_SURGICAL_INC</td>\n",
       "      <td>BASF_SE</td>\n",
       "      <td>iShares_Russell_1000_Growth_ETF</td>\n",
       "      <td>Vanguard_FTSE_Europe_ETF</td>\n",
       "      <td>Europe_50_Index</td>\n",
       "      <td>0.479507</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-12-20</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-19</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2019-12-10</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-10</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-04-23</td>\n",
       "      <td>2019-10-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495</td>\n",
       "      <td>Vonovia_SE</td>\n",
       "      <td>Air_Liquide_SA</td>\n",
       "      <td>Givaudan_SA</td>\n",
       "      <td>ACTIVISION_BLIZZARD_INC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.124196</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2019-12-10</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2019-04-06</td>\n",
       "      <td>2019-10-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>496 rows × 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Sec1                             Sec2  \\\n",
       "0                      3M_CO   iShares_Russell_1000_Value_ETF   \n",
       "1        ABBOTT_LABORATORIES   iShares_Russell_1000_Value_ETF   \n",
       "2    ACTIVISION_BLIZZARD_INC  iShares_Russell_1000_Growth_ETF   \n",
       "3          ADOBE_SYSTEMS_INC  iShares_Russell_1000_Growth_ETF   \n",
       "4     ADVANCED_MICRO_DEVICES  iShares_Russell_1000_Growth_ETF   \n",
       "..                       ...                              ...   \n",
       "491                 Vinci_SA                  Europe_50_Index   \n",
       "492               Vivendi_SA  iShares_Russell_1000_Growth_ETF   \n",
       "493           Voestalpine_AG                        ALCOA_INC   \n",
       "494                 Volvo_AB                   Atlas_Copco_AB   \n",
       "495               Vonovia_SE                   Air_Liquide_SA   \n",
       "\n",
       "                                Sec3                            Sec4  \\\n",
       "0    iShares_Russell_1000_Growth_ETF    iShares_Core_S&P_Mid-Cap_ETF   \n",
       "1    iShares_Russell_1000_Growth_ETF    iShares_Core_S&P_Mid-Cap_ETF   \n",
       "2     iShares_Russell_1000_Value_ETF    iShares_Core_S&P_Mid-Cap_ETF   \n",
       "3                       AUTODESK_INC  iShares_Russell_1000_Value_ETF   \n",
       "4     iShares_Russell_1000_Value_ETF    iShares_Core_S&P_Mid-Cap_ETF   \n",
       "..                               ...                             ...   \n",
       "491                   Air_Liquide_SA                       Danone_SA   \n",
       "492         Vanguard_FTSE_Europe_ETF              Publicis_Groupe_SA   \n",
       "493           INTUITIVE_SURGICAL_INC        Vanguard_FTSE_Europe_ETF   \n",
       "494                           SKF_AB                      Sandvik_AB   \n",
       "495                      Givaudan_SA         ACTIVISION_BLIZZARD_INC   \n",
       "\n",
       "                             Sec5                         Sec6  \\\n",
       "0               STATE_STREET_CORP                    BB&T_CORP   \n",
       "1               STATE_STREET_CORP  DISCOVER_FINANCIAL_SERVICES   \n",
       "2        Vanguard_FTSE_Europe_ETF                   XILINX_INC   \n",
       "3    iShares_Core_S&P_Mid-Cap_ETF       T_ROWE_PRICE_GROUP_INC   \n",
       "4                      XILINX_INC     Vanguard_FTSE_Europe_ETF   \n",
       "..                            ...                          ...   \n",
       "491                       BASF_SE                   L'Oreal_SA   \n",
       "492                           NaN                          NaN   \n",
       "493                           NaN                          NaN   \n",
       "494        INTUITIVE_SURGICAL_INC                      BASF_SE   \n",
       "495                           NaN                          NaN   \n",
       "\n",
       "                                Sec7                                 Sec8  \\\n",
       "0             T_ROWE_PRICE_GROUP_INC          DISCOVER_FINANCIAL_SERVICES   \n",
       "1            BECTON_DICKINSON_AND_CO                            BB&T_CORP   \n",
       "2                       AUTODESK_INC                  ELECTRONIC_ARTS_INC   \n",
       "3           Vanguard_FTSE_Europe_ETF                            BB&T_CORP   \n",
       "4                  STATE_STREET_CORP                   SUNTRUST_BANKS_INC   \n",
       "..                               ...                                  ...   \n",
       "491                  Airbus_Group_SE  LVMH_Moet_Hennessy_Louis_Vuitton_SA   \n",
       "492                              NaN                                  NaN   \n",
       "493                              NaN                                  NaN   \n",
       "494  iShares_Russell_1000_Growth_ETF             Vanguard_FTSE_Europe_ETF   \n",
       "495                              NaN                                  NaN   \n",
       "\n",
       "                           Sec9      R2_2  ...  TotalEndDate7  \\\n",
       "0    STANLEY_BLACK_&_DECKER_INC  0.660427  ...     2019-12-19   \n",
       "1      PRUDENTIAL_FINANCIAL_INC  0.682331  ...     2019-12-17   \n",
       "2        T_ROWE_PRICE_GROUP_INC  0.617346  ...     2019-12-19   \n",
       "3             STATE_STREET_CORP  0.768484  ...     2019-12-19   \n",
       "4                     BB&T_CORP  0.462562  ...     2019-12-18   \n",
       "..                          ...       ...  ...            ...   \n",
       "491                  Allianz_SE  0.409605  ...     2019-12-23   \n",
       "492                         NaN  0.202415  ...            NaT   \n",
       "493                         NaN  0.197433  ...            NaT   \n",
       "494             Europe_50_Index  0.479507  ...     2019-12-20   \n",
       "495                         NaN  0.124196  ...            NaT   \n",
       "\n",
       "     TotalStartDate8  TotalEndDate8  TotalStartDate9  TotalEndDate9  \\\n",
       "0         2018-02-01     2019-12-13       2018-02-01     2019-12-18   \n",
       "1         2018-01-31     2019-12-06       2018-02-01     2019-12-13   \n",
       "2         2017-12-11     2019-12-13       2018-02-01     2019-12-19   \n",
       "3         2018-01-31     2019-12-06       2018-02-01     2019-12-18   \n",
       "4         2018-02-01     2019-12-06       2018-01-31     2019-12-06   \n",
       "..               ...            ...              ...            ...   \n",
       "491       2017-12-01     2019-12-23       2017-12-01     2019-12-23   \n",
       "492              NaT            NaT              NaT            NaT   \n",
       "493              NaT            NaT              NaT            NaT   \n",
       "494       2018-02-01     2019-12-19       2017-12-11     2019-12-10   \n",
       "495              NaT            NaT              NaT            NaT   \n",
       "\n",
       "     LastStartDate  FirstEndDate TrainStartDate ValStartDate  ValEndDate  \n",
       "0       2018-02-01    2019-12-06     2018-02-01   2019-04-21  2019-10-07  \n",
       "1       2018-02-01    2019-12-06     2018-02-01   2019-04-21  2019-10-07  \n",
       "2       2018-02-01    2019-12-10     2018-02-01   2019-04-23  2019-10-11  \n",
       "3       2018-02-01    2019-12-06     2018-02-01   2019-04-21  2019-10-07  \n",
       "4       2018-02-01    2019-12-06     2018-02-01   2019-04-21  2019-10-07  \n",
       "..             ...           ...            ...          ...         ...  \n",
       "491     2017-12-11    2019-12-10     2017-12-11   2019-04-06  2019-10-11  \n",
       "492     2018-02-01    2019-12-19     2018-02-01   2019-04-29  2019-10-20  \n",
       "493     2018-02-01    2019-12-19     2018-02-01   2019-04-29  2019-10-20  \n",
       "494     2018-02-01    2019-12-10     2018-02-01   2019-04-23  2019-10-11  \n",
       "495     2017-12-11    2019-12-10     2017-12-11   2019-04-06  2019-10-11  \n",
       "\n",
       "[496 rows x 72 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Initial_Combination_Data import getCombDF\n",
    "regression_results_fn = '/media/andrew/FreeAgent Drive/Market_Data/Tick_Data_Manager/regression_results.csv'\n",
    "data_summary_fn = '/media/andrew/FreeAgent Drive/Market_Data/Tick_Data_Manager/data_summary.csv'\n",
    "getCombDF(regression_results_fn, data_summary_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Initial Setup </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, pytz, tqdm, datetime\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import load_model\n",
    "\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c1e89358037e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtdm_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/media/andrew/FreeAgent Drive/Market_Data/Tick_Data_Manager/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mregression_results_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/media/andrew/FreeAgent Drive/Market_Data/Tick_Data_Manager/regression_results.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata_summaryDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtdm_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'data_summary.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "omni_dir = '/home/andrew/All_Trading/Studies/Omni_Project/'\n",
    "tdm_dir = '/media/andrew/FreeAgent Drive/Market_Data/Tick_Data_Manager/'\n",
    "regression_results_fn = '/media/andrew/FreeAgent Drive/Market_Data/Tick_Data_Manager/regression_results.csv'\n",
    "data_summaryDF = pd.read_csv(tdm_dir+'data_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-01-22 10:28:33'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def utcToChi(utc_dt):\n",
    "    chi_tz = pytz.timezone('America/Chicago')\n",
    "    chi_dt = utc_dt.replace(tzinfo=pytz.utc).astimezone(chi_tz)\n",
    "    return chi_tz.normalize(chi_dt)\n",
    "def getChiTimeNow():\n",
    "    utc_dt = pd.datetime.utcnow()\n",
    "    return(utcToChi(utc_dt))\n",
    "getChiTimeNow().strftime(format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Determine which combinations of data to try </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/anaconda3/envs/env1/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (5,6,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 rows have R2 not found:\n",
      "\n",
      "            Sec1         Sec2\n",
      "66174  AETNA_INC  Natural_Gas\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sec1</th>\n",
       "      <th>Sec2</th>\n",
       "      <th>R2</th>\n",
       "      <th>p_value</th>\n",
       "      <th>NumDatapoints</th>\n",
       "      <th>StartDate</th>\n",
       "      <th>EndDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3M_CO</td>\n",
       "      <td>ABBOTT_LABORATORIES</td>\n",
       "      <td>0.042635</td>\n",
       "      <td>1.16323e-06</td>\n",
       "      <td>545</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2018-01-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3M_CO</td>\n",
       "      <td>ACTIVISION_BLIZZARD_INC</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.943395</td>\n",
       "      <td>545</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2018-01-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3M_CO</td>\n",
       "      <td>ADOBE_SYSTEMS_INC</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.76748</td>\n",
       "      <td>545</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2018-01-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3M_CO</td>\n",
       "      <td>ADVANCED_MICRO_DEVICES</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.700944</td>\n",
       "      <td>545</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2018-01-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3M_CO</td>\n",
       "      <td>AGILENT_TECHNOLOGIES_INC</td>\n",
       "      <td>0.026930</td>\n",
       "      <td>0.000118923</td>\n",
       "      <td>545</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2018-01-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167869</td>\n",
       "      <td>ProSiebenSat.1_Media_AG</td>\n",
       "      <td>PROGRESSIVE_CORP</td>\n",
       "      <td>0.053324</td>\n",
       "      <td>0.00276002</td>\n",
       "      <td>166</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2018-03-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167870</td>\n",
       "      <td>ProSiebenSat.1_Media_AG</td>\n",
       "      <td>Sage_Group_PLC_The</td>\n",
       "      <td>0.033931</td>\n",
       "      <td>0.0047902</td>\n",
       "      <td>233</td>\n",
       "      <td>2016-11-14</td>\n",
       "      <td>2016-12-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167871</td>\n",
       "      <td>ProSiebenSat.1_Media_AG</td>\n",
       "      <td>SPDR_S&amp;P_Oil_&amp;_Gas_Explor_&amp;_Prodtn_ETF</td>\n",
       "      <td>0.024248</td>\n",
       "      <td>0.0562295</td>\n",
       "      <td>151</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2018-01-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167872</td>\n",
       "      <td>ProSiebenSat.1_Media_AG</td>\n",
       "      <td>Swiss_Re_AG</td>\n",
       "      <td>0.027485</td>\n",
       "      <td>0.00823583</td>\n",
       "      <td>253</td>\n",
       "      <td>2017-11-14</td>\n",
       "      <td>2017-12-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167873</td>\n",
       "      <td>ProSiebenSat.1_Media_AG</td>\n",
       "      <td>ThyssenKrupp_AG</td>\n",
       "      <td>0.026108</td>\n",
       "      <td>0.0174727</td>\n",
       "      <td>216</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>2017-12-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167874 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Sec1                                    Sec2  \\\n",
       "0                         3M_CO                     ABBOTT_LABORATORIES   \n",
       "1                         3M_CO                 ACTIVISION_BLIZZARD_INC   \n",
       "2                         3M_CO                       ADOBE_SYSTEMS_INC   \n",
       "3                         3M_CO                  ADVANCED_MICRO_DEVICES   \n",
       "4                         3M_CO                AGILENT_TECHNOLOGIES_INC   \n",
       "...                         ...                                     ...   \n",
       "167869  ProSiebenSat.1_Media_AG                        PROGRESSIVE_CORP   \n",
       "167870  ProSiebenSat.1_Media_AG                      Sage_Group_PLC_The   \n",
       "167871  ProSiebenSat.1_Media_AG  SPDR_S&P_Oil_&_Gas_Explor_&_Prodtn_ETF   \n",
       "167872  ProSiebenSat.1_Media_AG                             Swiss_Re_AG   \n",
       "167873  ProSiebenSat.1_Media_AG                         ThyssenKrupp_AG   \n",
       "\n",
       "              R2      p_value NumDatapoints   StartDate     EndDate  \n",
       "0       0.042635  1.16323e-06           545  2017-12-11  2018-01-10  \n",
       "1       0.000009     0.943395           545  2017-12-11  2018-01-10  \n",
       "2       0.000161      0.76748           545  2017-12-11  2018-01-10  \n",
       "3       0.000272     0.700944           545  2017-12-11  2018-01-10  \n",
       "4       0.026930  0.000118923           545  2017-12-11  2018-01-10  \n",
       "...          ...          ...           ...         ...         ...  \n",
       "167869  0.053324   0.00276002           166  2018-02-01  2018-03-02  \n",
       "167870  0.033931    0.0047902           233  2016-11-14  2016-12-14  \n",
       "167871  0.024248    0.0562295           151  2017-12-11  2018-01-10  \n",
       "167872  0.027485   0.00823583           253  2017-11-14  2017-12-14  \n",
       "167873  0.026108    0.0174727           216  2017-12-01  2017-12-28  \n",
       "\n",
       "[167874 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load prepare regressionDF\n",
    "regression_cols = ['Sec1', 'Sec2', 'R2', 'p_value', 'NumDatapoints', 'StartDate', 'EndDate']\n",
    "\n",
    "regressionDF = pd.read_csv(regression_results_fn)\n",
    "for col in regressionDF.columns: \n",
    "    if 'Unnamed' in col: \n",
    "        regressionDF.drop(columns=[col], inplace=True)\n",
    "\n",
    "assert(set(regression_cols) == set(regressionDF.columns))\n",
    "regressionDF = regressionDF[regression_cols]\n",
    "        \n",
    "print(str(len(regressionDF.loc[regressionDF.R2 == 'not found']))+' rows have R2 not found:\\n')\n",
    "print(regressionDF.loc[regressionDF.R2 == 'not found'][['Sec1', 'Sec2']])\n",
    "        \n",
    "regressionDF = regressionDF.loc[regressionDF.R2 != 'not found'].reset_index(drop=True)\n",
    "regressionDF.R2 = regressionDF.R2.astype(float)\n",
    "regressionDF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sec1</th>\n",
       "      <th>Sec2</th>\n",
       "      <th>R2</th>\n",
       "      <th>p_value</th>\n",
       "      <th>NumDatapoints</th>\n",
       "      <th>StartDate</th>\n",
       "      <th>EndDate</th>\n",
       "      <th>Asset1</th>\n",
       "      <th>Asset2</th>\n",
       "      <th>Adjusted_R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3M_CO</td>\n",
       "      <td>ALLSTATE_CORP</td>\n",
       "      <td>0.360383</td>\n",
       "      <td>6.62076e-57</td>\n",
       "      <td>568</td>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>2018-03-02</td>\n",
       "      <td>Security</td>\n",
       "      <td>Security</td>\n",
       "      <td>0.360383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3M_CO</td>\n",
       "      <td>HONEYWELL_INTERNATIONAL_INC</td>\n",
       "      <td>0.222743</td>\n",
       "      <td>1.39948e-31</td>\n",
       "      <td>545</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>Security</td>\n",
       "      <td>Security</td>\n",
       "      <td>0.222743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3M_CO</td>\n",
       "      <td>HP_INC</td>\n",
       "      <td>0.336388</td>\n",
       "      <td>2.62459e-50</td>\n",
       "      <td>545</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2018-03-02</td>\n",
       "      <td>Security</td>\n",
       "      <td>Security</td>\n",
       "      <td>0.336388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3M_CO</td>\n",
       "      <td>Industrial_Select_Sector_SPDR_Fund</td>\n",
       "      <td>0.278624</td>\n",
       "      <td>1.99807e-40</td>\n",
       "      <td>545</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>Security</td>\n",
       "      <td>Security</td>\n",
       "      <td>0.278624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>3M_CO</td>\n",
       "      <td>MARATHON_PETROLEUM_CORP</td>\n",
       "      <td>0.366013</td>\n",
       "      <td>1.03755e-55</td>\n",
       "      <td>545</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2018-03-02</td>\n",
       "      <td>Security</td>\n",
       "      <td>Security</td>\n",
       "      <td>0.366013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167578</td>\n",
       "      <td>JM_SMUCKER_COMPANY</td>\n",
       "      <td>SPDR_S&amp;P_500_ETF</td>\n",
       "      <td>0.232402</td>\n",
       "      <td>6.85352e-33</td>\n",
       "      <td>542</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2018-03-02</td>\n",
       "      <td>Security</td>\n",
       "      <td>Security</td>\n",
       "      <td>0.232402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167653</td>\n",
       "      <td>JM_SMUCKER_COMPANY</td>\n",
       "      <td>AT&amp;T_INC</td>\n",
       "      <td>0.251665</td>\n",
       "      <td>6.89741e-36</td>\n",
       "      <td>542</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2018-03-02</td>\n",
       "      <td>Security</td>\n",
       "      <td>Security</td>\n",
       "      <td>0.251665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167663</td>\n",
       "      <td>JM_SMUCKER_COMPANY</td>\n",
       "      <td>AUTOMATIC_DATA_PROCESSING</td>\n",
       "      <td>0.221045</td>\n",
       "      <td>3.70592e-31</td>\n",
       "      <td>542</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2018-03-02</td>\n",
       "      <td>Security</td>\n",
       "      <td>Security</td>\n",
       "      <td>0.221045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167732</td>\n",
       "      <td>JM_SMUCKER_COMPANY</td>\n",
       "      <td>ALTRIA_GROUP_INC</td>\n",
       "      <td>0.259102</td>\n",
       "      <td>4.58492e-37</td>\n",
       "      <td>542</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2018-03-02</td>\n",
       "      <td>Security</td>\n",
       "      <td>Security</td>\n",
       "      <td>0.259102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167757</td>\n",
       "      <td>JM_SMUCKER_COMPANY</td>\n",
       "      <td>SYSCO_CORP</td>\n",
       "      <td>0.237878</td>\n",
       "      <td>1.12427e-33</td>\n",
       "      <td>541</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2018-03-02</td>\n",
       "      <td>Security</td>\n",
       "      <td>Security</td>\n",
       "      <td>0.237878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13116 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Sec1                                Sec2        R2  \\\n",
       "8                    3M_CO                       ALLSTATE_CORP  0.360383   \n",
       "23                   3M_CO         HONEYWELL_INTERNATIONAL_INC  0.222743   \n",
       "25                   3M_CO                              HP_INC  0.336388   \n",
       "27                   3M_CO  Industrial_Select_Sector_SPDR_Fund  0.278624   \n",
       "41                   3M_CO             MARATHON_PETROLEUM_CORP  0.366013   \n",
       "...                    ...                                 ...       ...   \n",
       "167578  JM_SMUCKER_COMPANY                    SPDR_S&P_500_ETF  0.232402   \n",
       "167653  JM_SMUCKER_COMPANY                            AT&T_INC  0.251665   \n",
       "167663  JM_SMUCKER_COMPANY           AUTOMATIC_DATA_PROCESSING  0.221045   \n",
       "167732  JM_SMUCKER_COMPANY                    ALTRIA_GROUP_INC  0.259102   \n",
       "167757  JM_SMUCKER_COMPANY                          SYSCO_CORP  0.237878   \n",
       "\n",
       "            p_value NumDatapoints   StartDate     EndDate    Asset1    Asset2  \\\n",
       "8       6.62076e-57           568  2018-01-31  2018-03-02  Security  Security   \n",
       "23      1.39948e-31           545  2017-12-11  2018-01-10  Security  Security   \n",
       "25      2.62459e-50           545  2018-02-01  2018-03-02  Security  Security   \n",
       "27      1.99807e-40           545  2017-12-11  2018-01-10  Security  Security   \n",
       "41      1.03755e-55           545  2018-02-01  2018-03-02  Security  Security   \n",
       "...             ...           ...         ...         ...       ...       ...   \n",
       "167578  6.85352e-33           542  2018-02-01  2018-03-02  Security  Security   \n",
       "167653  6.89741e-36           542  2018-02-01  2018-03-02  Security  Security   \n",
       "167663  3.70592e-31           542  2018-02-01  2018-03-02  Security  Security   \n",
       "167732  4.58492e-37           542  2018-02-01  2018-03-02  Security  Security   \n",
       "167757  1.12427e-33           541  2018-02-01  2018-03-02  Security  Security   \n",
       "\n",
       "        Adjusted_R2  \n",
       "8          0.360383  \n",
       "23         0.222743  \n",
       "25         0.336388  \n",
       "27         0.278624  \n",
       "41         0.366013  \n",
       "...             ...  \n",
       "167578     0.232402  \n",
       "167653     0.251665  \n",
       "167663     0.221045  \n",
       "167732     0.259102  \n",
       "167757     0.237878  \n",
       "\n",
       "[13116 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_threshold = .2\n",
    "currency_r2_mult = 3 #give currencies a boost because a priori I think they should matter\n",
    "max_sec_bundle = 8 #sec1 can have <= max_sec_bundle other secs in its bundle\n",
    "min_sec_bundle = 3 #sec1 must have >= max_sec_bundle other secs in its bundle\n",
    "\n",
    "regressionDF['Asset1'] = 'Security'\n",
    "regressionDF['Asset2'] = 'Security'\n",
    "regressionDF.loc[regressionDF.Sec1.isin(['Natural_Gas', 'US_Brent_Crude_Oil', 'US_Dollar_Index', 'US_Light_Crude_Oil']), 'Asset1'] = 'Commodity'\n",
    "regressionDF.loc[regressionDF.Sec2.isin(['Natural_Gas', 'US_Brent_Crude_Oil', 'US_Dollar_Index', 'US_Light_Crude_Oil']), 'Asset2'] = 'Commodity'\n",
    "currencies = ['AUD',  'CAD',  'CHF',  'EUR',  'GBP',  'NZD',  'SGD',  'TRY',  'USD',  'ZAR',  'NOK',  'SEK',  'PLN',  'JPY']\n",
    "regressionDF.loc[(regressionDF.Sec1.str[:3].isin(currencies)) & (regressionDF.Sec1.str[-3:].isin(currencies)), 'Asset1'] = 'Currency'\n",
    "regressionDF.loc[(regressionDF.Sec2.str[:3].isin(currencies)) & (regressionDF.Sec2.str[-3:].isin(currencies)), 'Asset2'] = 'Currency'\n",
    "\n",
    "# apply currency_r2_mult where only one asset is a currency\n",
    "regressionDF['Adjusted_R2'] = regressionDF.R2 * currency_r2_mult**(((regressionDF.Asset1 == 'Currency') & (regressionDF.Asset2 != 'Currency')) | ((regressionDF.Asset1 != 'Currency') & (regressionDF.Asset2 == 'Currency')))\n",
    "\n",
    "# [] identify and organize funds, ETFs, ETNs, Indexes, etc\n",
    "\n",
    "comb_cols = ['Sec1']+['Sec'+str(i) for i in range(2,(max_sec_bundle+2))] \\\n",
    "    +['R2_'+str(i) for i in range(2,(max_sec_bundle+2))] \\\n",
    "    +['p_value'+str(i) for i in range(2,(max_sec_bundle+2))] \\\n",
    "    +['StartDate'+str(i) for i in range(2,(max_sec_bundle+2))] \\\n",
    "    +['EndDate'+str(i) for i in range(2,(max_sec_bundle+2))] \\\n",
    "    +['NumDatapoints'+str(i) for i in range(2,(max_sec_bundle+2))]\n",
    "\n",
    "combDF = pd.DataFrame(columns = comb_cols)\n",
    "\n",
    "regressionDF.loc[regressionDF.R2 >= r2_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sec1</th>\n",
       "      <th>Sec2</th>\n",
       "      <th>R2</th>\n",
       "      <th>p_value</th>\n",
       "      <th>NumDatapoints</th>\n",
       "      <th>StartDate</th>\n",
       "      <th>EndDate</th>\n",
       "      <th>Asset1</th>\n",
       "      <th>Asset2</th>\n",
       "      <th>Adjusted_R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9212</td>\n",
       "      <td>EURGBP</td>\n",
       "      <td>GBPCHF</td>\n",
       "      <td>7.318309e-01</td>\n",
       "      <td>8.37808e-199</td>\n",
       "      <td>690</td>\n",
       "      <td>2016-11-14</td>\n",
       "      <td>2016-12-14</td>\n",
       "      <td>Currency</td>\n",
       "      <td>Currency</td>\n",
       "      <td>7.318309e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7960</td>\n",
       "      <td>EURCAD</td>\n",
       "      <td>CADCHF</td>\n",
       "      <td>7.129402e-01</td>\n",
       "      <td>1.25521e-188</td>\n",
       "      <td>690</td>\n",
       "      <td>2016-11-14</td>\n",
       "      <td>2016-12-14</td>\n",
       "      <td>Currency</td>\n",
       "      <td>Currency</td>\n",
       "      <td>7.129402e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7853</td>\n",
       "      <td>EURCAD</td>\n",
       "      <td>USDCAD</td>\n",
       "      <td>6.782721e-01</td>\n",
       "      <td>2.17015e-149</td>\n",
       "      <td>600</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>Currency</td>\n",
       "      <td>Currency</td>\n",
       "      <td>6.782721e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1209</td>\n",
       "      <td>NZDUSD</td>\n",
       "      <td>AUDUSD</td>\n",
       "      <td>6.555073e-01</td>\n",
       "      <td>1.66561e-140</td>\n",
       "      <td>600</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>Currency</td>\n",
       "      <td>Currency</td>\n",
       "      <td>6.555073e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1497</td>\n",
       "      <td>AUDUSD</td>\n",
       "      <td>NZDUSD</td>\n",
       "      <td>6.555073e-01</td>\n",
       "      <td>1.66561e-140</td>\n",
       "      <td>600</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>Currency</td>\n",
       "      <td>Currency</td>\n",
       "      <td>6.555073e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>494</td>\n",
       "      <td>TARGET_CORP</td>\n",
       "      <td>GBPUSD</td>\n",
       "      <td>1.739061e-09</td>\n",
       "      <td>0.999225</td>\n",
       "      <td>545</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>Security</td>\n",
       "      <td>Currency</td>\n",
       "      <td>5.217184e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1917</td>\n",
       "      <td>GBPUSD</td>\n",
       "      <td>TARGET_CORP</td>\n",
       "      <td>1.739061e-09</td>\n",
       "      <td>0.999225</td>\n",
       "      <td>545</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>Currency</td>\n",
       "      <td>Security</td>\n",
       "      <td>5.217183e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8134</td>\n",
       "      <td>EURCHF</td>\n",
       "      <td>Ageas</td>\n",
       "      <td>4.098721e-12</td>\n",
       "      <td>0.999973</td>\n",
       "      <td>276</td>\n",
       "      <td>2016-11-21</td>\n",
       "      <td>2016-12-21</td>\n",
       "      <td>Currency</td>\n",
       "      <td>Security</td>\n",
       "      <td>1.229616e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5619</td>\n",
       "      <td>Ageas</td>\n",
       "      <td>EURCHF</td>\n",
       "      <td>4.098721e-12</td>\n",
       "      <td>0.9999732855387345</td>\n",
       "      <td>276</td>\n",
       "      <td>2016-11-21</td>\n",
       "      <td>2016-12-21</td>\n",
       "      <td>Security</td>\n",
       "      <td>Currency</td>\n",
       "      <td>1.229616e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6446</td>\n",
       "      <td>Swedbank_AB</td>\n",
       "      <td>AUDCHF</td>\n",
       "      <td>1.960210e-12</td>\n",
       "      <td>0.999982</td>\n",
       "      <td>251</td>\n",
       "      <td>2016-11-14</td>\n",
       "      <td>2016-12-14</td>\n",
       "      <td>Security</td>\n",
       "      <td>Currency</td>\n",
       "      <td>5.880629e-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10294 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Sec1         Sec2            R2             p_value  \\\n",
       "9212       EURGBP       GBPCHF  7.318309e-01        8.37808e-199   \n",
       "7960       EURCAD       CADCHF  7.129402e-01        1.25521e-188   \n",
       "7853       EURCAD       USDCAD  6.782721e-01        2.17015e-149   \n",
       "1209       NZDUSD       AUDUSD  6.555073e-01        1.66561e-140   \n",
       "1497       AUDUSD       NZDUSD  6.555073e-01        1.66561e-140   \n",
       "...           ...          ...           ...                 ...   \n",
       "494   TARGET_CORP       GBPUSD  1.739061e-09            0.999225   \n",
       "1917       GBPUSD  TARGET_CORP  1.739061e-09            0.999225   \n",
       "8134       EURCHF        Ageas  4.098721e-12            0.999973   \n",
       "5619        Ageas       EURCHF  4.098721e-12  0.9999732855387345   \n",
       "6446  Swedbank_AB       AUDCHF  1.960210e-12            0.999982   \n",
       "\n",
       "     NumDatapoints   StartDate     EndDate    Asset1    Asset2   Adjusted_R2  \n",
       "9212           690  2016-11-14  2016-12-14  Currency  Currency  7.318309e-01  \n",
       "7960           690  2016-11-14  2016-12-14  Currency  Currency  7.129402e-01  \n",
       "7853           600  2017-12-11  2018-01-09  Currency  Currency  6.782721e-01  \n",
       "1209           600  2017-12-11  2018-01-09  Currency  Currency  6.555073e-01  \n",
       "1497           600  2017-12-11  2018-01-09  Currency  Currency  6.555073e-01  \n",
       "...            ...         ...         ...       ...       ...           ...  \n",
       "494            545  2017-12-11  2018-01-10  Security  Currency  5.217184e-09  \n",
       "1917           545  2017-12-11  2018-01-10  Currency  Security  5.217183e-09  \n",
       "8134           276  2016-11-21  2016-12-21  Currency  Security  1.229616e-11  \n",
       "5619           276  2016-11-21  2016-12-21  Security  Currency  1.229616e-11  \n",
       "6446           251  2016-11-14  2016-12-14  Security  Currency  5.880629e-12  \n",
       "\n",
       "[10294 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currencyDF = regressionDF.loc[(regressionDF.Asset1 == 'Currency') | (regressionDF.Asset2 == 'Currency') ].reset_index(drop=True)\n",
    "currencyDF.sort_values(by='R2', ascending=False)#.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sec1</th>\n",
       "      <th>Sec2</th>\n",
       "      <th>Sec3</th>\n",
       "      <th>Sec4</th>\n",
       "      <th>Sec5</th>\n",
       "      <th>Sec6</th>\n",
       "      <th>Sec7</th>\n",
       "      <th>Sec8</th>\n",
       "      <th>Sec9</th>\n",
       "      <th>R2_2</th>\n",
       "      <th>...</th>\n",
       "      <th>EndDate8</th>\n",
       "      <th>EndDate9</th>\n",
       "      <th>NumDatapoints2</th>\n",
       "      <th>NumDatapoints3</th>\n",
       "      <th>NumDatapoints4</th>\n",
       "      <th>NumDatapoints5</th>\n",
       "      <th>NumDatapoints6</th>\n",
       "      <th>NumDatapoints7</th>\n",
       "      <th>NumDatapoints8</th>\n",
       "      <th>NumDatapoints9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3M_CO</td>\n",
       "      <td>iShares_Russell_1000_Value_ETF</td>\n",
       "      <td>iShares_Russell_1000_Growth_ETF</td>\n",
       "      <td>iShares_Core_S&amp;P_Mid-Cap_ETF</td>\n",
       "      <td>STATE_STREET_CORP</td>\n",
       "      <td>BB&amp;T_CORP</td>\n",
       "      <td>T_ROWE_PRICE_GROUP_INC</td>\n",
       "      <td>DISCOVER_FINANCIAL_SERVICES</td>\n",
       "      <td>STANLEY_BLACK_&amp;_DECKER_INC</td>\n",
       "      <td>0.660427</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-03-02</td>\n",
       "      <td>2018-03-02</td>\n",
       "      <td>545</td>\n",
       "      <td>545</td>\n",
       "      <td>545</td>\n",
       "      <td>541</td>\n",
       "      <td>563</td>\n",
       "      <td>541</td>\n",
       "      <td>545</td>\n",
       "      <td>541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ABBOTT_LABORATORIES</td>\n",
       "      <td>iShares_Russell_1000_Value_ETF</td>\n",
       "      <td>iShares_Russell_1000_Growth_ETF</td>\n",
       "      <td>iShares_Core_S&amp;P_Mid-Cap_ETF</td>\n",
       "      <td>STATE_STREET_CORP</td>\n",
       "      <td>DISCOVER_FINANCIAL_SERVICES</td>\n",
       "      <td>BECTON_DICKINSON_AND_CO</td>\n",
       "      <td>BB&amp;T_CORP</td>\n",
       "      <td>PRUDENTIAL_FINANCIAL_INC</td>\n",
       "      <td>0.682331</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-03-02</td>\n",
       "      <td>2018-03-02</td>\n",
       "      <td>545</td>\n",
       "      <td>545</td>\n",
       "      <td>545</td>\n",
       "      <td>541</td>\n",
       "      <td>545</td>\n",
       "      <td>563</td>\n",
       "      <td>563</td>\n",
       "      <td>543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ACTIVISION_BLIZZARD_INC</td>\n",
       "      <td>iShares_Russell_1000_Growth_ETF</td>\n",
       "      <td>iShares_Russell_1000_Value_ETF</td>\n",
       "      <td>iShares_Core_S&amp;P_Mid-Cap_ETF</td>\n",
       "      <td>Vanguard_FTSE_Europe_ETF</td>\n",
       "      <td>XILINX_INC</td>\n",
       "      <td>AUTODESK_INC</td>\n",
       "      <td>ELECTRONIC_ARTS_INC</td>\n",
       "      <td>T_ROWE_PRICE_GROUP_INC</td>\n",
       "      <td>0.617346</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>2018-03-02</td>\n",
       "      <td>545</td>\n",
       "      <td>545</td>\n",
       "      <td>545</td>\n",
       "      <td>541</td>\n",
       "      <td>541</td>\n",
       "      <td>564</td>\n",
       "      <td>545</td>\n",
       "      <td>541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ADOBE_SYSTEMS_INC</td>\n",
       "      <td>iShares_Russell_1000_Growth_ETF</td>\n",
       "      <td>AUTODESK_INC</td>\n",
       "      <td>iShares_Russell_1000_Value_ETF</td>\n",
       "      <td>iShares_Core_S&amp;P_Mid-Cap_ETF</td>\n",
       "      <td>T_ROWE_PRICE_GROUP_INC</td>\n",
       "      <td>Vanguard_FTSE_Europe_ETF</td>\n",
       "      <td>BB&amp;T_CORP</td>\n",
       "      <td>STATE_STREET_CORP</td>\n",
       "      <td>0.768484</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-03-02</td>\n",
       "      <td>2018-03-02</td>\n",
       "      <td>545</td>\n",
       "      <td>564</td>\n",
       "      <td>545</td>\n",
       "      <td>545</td>\n",
       "      <td>541</td>\n",
       "      <td>541</td>\n",
       "      <td>563</td>\n",
       "      <td>541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ADVANCED_MICRO_DEVICES</td>\n",
       "      <td>iShares_Russell_1000_Growth_ETF</td>\n",
       "      <td>iShares_Russell_1000_Value_ETF</td>\n",
       "      <td>iShares_Core_S&amp;P_Mid-Cap_ETF</td>\n",
       "      <td>XILINX_INC</td>\n",
       "      <td>Vanguard_FTSE_Europe_ETF</td>\n",
       "      <td>STATE_STREET_CORP</td>\n",
       "      <td>SUNTRUST_BANKS_INC</td>\n",
       "      <td>BB&amp;T_CORP</td>\n",
       "      <td>0.462562</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-03-02</td>\n",
       "      <td>2018-03-02</td>\n",
       "      <td>545</td>\n",
       "      <td>545</td>\n",
       "      <td>545</td>\n",
       "      <td>541</td>\n",
       "      <td>541</td>\n",
       "      <td>541</td>\n",
       "      <td>542</td>\n",
       "      <td>563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>431</td>\n",
       "      <td>Vallourec_SA</td>\n",
       "      <td>Vanguard_FTSE_Europe_ETF</td>\n",
       "      <td>EMERSON_ELECTRIC_CO</td>\n",
       "      <td>PPG_INDUSTRIES_INC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.097994</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>163</td>\n",
       "      <td>167</td>\n",
       "      <td>159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>Veolia_Environnement_SA</td>\n",
       "      <td>EDP_-_Energias_de_Portugal_SA</td>\n",
       "      <td>United_Utilities_Group_PLC</td>\n",
       "      <td>E.ON_SE</td>\n",
       "      <td>Centrica_PLC</td>\n",
       "      <td>Gas_Natural_SDG_SA</td>\n",
       "      <td>SSE_PLC</td>\n",
       "      <td>Engie</td>\n",
       "      <td>Severn_Trent_PLC</td>\n",
       "      <td>0.380371</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-12-14</td>\n",
       "      <td>2016-12-14</td>\n",
       "      <td>266</td>\n",
       "      <td>276</td>\n",
       "      <td>276</td>\n",
       "      <td>276</td>\n",
       "      <td>275</td>\n",
       "      <td>269</td>\n",
       "      <td>276</td>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>433</td>\n",
       "      <td>Investor_AB</td>\n",
       "      <td>Allianz_SE</td>\n",
       "      <td>Atlas_Copco_AB</td>\n",
       "      <td>BASF_SE</td>\n",
       "      <td>Skandinaviska_Enskilda_Banken_AB</td>\n",
       "      <td>Europe_50_Index</td>\n",
       "      <td>Air_Liquide_SA</td>\n",
       "      <td>BNP_Paribas_SA</td>\n",
       "      <td>LVMH_Moet_Hennessy_Louis_Vuitton_SA</td>\n",
       "      <td>0.370738</td>\n",
       "      <td>...</td>\n",
       "      <td>2017-12-29</td>\n",
       "      <td>2017-12-29</td>\n",
       "      <td>198</td>\n",
       "      <td>253</td>\n",
       "      <td>198</td>\n",
       "      <td>250</td>\n",
       "      <td>207</td>\n",
       "      <td>208</td>\n",
       "      <td>209</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>434</td>\n",
       "      <td>JM_SMUCKER_COMPANY</td>\n",
       "      <td>Consumer_Staples_Select_Sector_SPDR_Fund</td>\n",
       "      <td>CONAGRA_FOODS_INC</td>\n",
       "      <td>GENERAL_MILLS_INC</td>\n",
       "      <td>KELLOGG_CO</td>\n",
       "      <td>PEPSICO_INC</td>\n",
       "      <td>PROCTER_&amp;_GAMBLE_CO_THE</td>\n",
       "      <td>MONDELEZ_INTERNATIONAL_INC-A</td>\n",
       "      <td>COCA-COLA_CO_THE</td>\n",
       "      <td>0.456070</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-03-02</td>\n",
       "      <td>2018-03-02</td>\n",
       "      <td>542</td>\n",
       "      <td>542</td>\n",
       "      <td>542</td>\n",
       "      <td>542</td>\n",
       "      <td>542</td>\n",
       "      <td>542</td>\n",
       "      <td>541</td>\n",
       "      <td>542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435</td>\n",
       "      <td>ProSiebenSat.1_Media_AG</td>\n",
       "      <td>L'Oreal_SA</td>\n",
       "      <td>BANK_OF_NEW_YORK_MELLON_CORP</td>\n",
       "      <td>Air_Liquide_SA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.078682</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>216</td>\n",
       "      <td>151</td>\n",
       "      <td>216</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>436 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Sec1                                      Sec2  \\\n",
       "0                      3M_CO            iShares_Russell_1000_Value_ETF   \n",
       "1        ABBOTT_LABORATORIES            iShares_Russell_1000_Value_ETF   \n",
       "2    ACTIVISION_BLIZZARD_INC           iShares_Russell_1000_Growth_ETF   \n",
       "3          ADOBE_SYSTEMS_INC           iShares_Russell_1000_Growth_ETF   \n",
       "4     ADVANCED_MICRO_DEVICES           iShares_Russell_1000_Growth_ETF   \n",
       "..                       ...                                       ...   \n",
       "431             Vallourec_SA                  Vanguard_FTSE_Europe_ETF   \n",
       "432  Veolia_Environnement_SA             EDP_-_Energias_de_Portugal_SA   \n",
       "433              Investor_AB                                Allianz_SE   \n",
       "434       JM_SMUCKER_COMPANY  Consumer_Staples_Select_Sector_SPDR_Fund   \n",
       "435  ProSiebenSat.1_Media_AG                                L'Oreal_SA   \n",
       "\n",
       "                                Sec3                            Sec4  \\\n",
       "0    iShares_Russell_1000_Growth_ETF    iShares_Core_S&P_Mid-Cap_ETF   \n",
       "1    iShares_Russell_1000_Growth_ETF    iShares_Core_S&P_Mid-Cap_ETF   \n",
       "2     iShares_Russell_1000_Value_ETF    iShares_Core_S&P_Mid-Cap_ETF   \n",
       "3                       AUTODESK_INC  iShares_Russell_1000_Value_ETF   \n",
       "4     iShares_Russell_1000_Value_ETF    iShares_Core_S&P_Mid-Cap_ETF   \n",
       "..                               ...                             ...   \n",
       "431              EMERSON_ELECTRIC_CO              PPG_INDUSTRIES_INC   \n",
       "432       United_Utilities_Group_PLC                         E.ON_SE   \n",
       "433                   Atlas_Copco_AB                         BASF_SE   \n",
       "434                CONAGRA_FOODS_INC               GENERAL_MILLS_INC   \n",
       "435     BANK_OF_NEW_YORK_MELLON_CORP                  Air_Liquide_SA   \n",
       "\n",
       "                                 Sec5                         Sec6  \\\n",
       "0                   STATE_STREET_CORP                    BB&T_CORP   \n",
       "1                   STATE_STREET_CORP  DISCOVER_FINANCIAL_SERVICES   \n",
       "2            Vanguard_FTSE_Europe_ETF                   XILINX_INC   \n",
       "3        iShares_Core_S&P_Mid-Cap_ETF       T_ROWE_PRICE_GROUP_INC   \n",
       "4                          XILINX_INC     Vanguard_FTSE_Europe_ETF   \n",
       "..                                ...                          ...   \n",
       "431                               NaN                          NaN   \n",
       "432                      Centrica_PLC           Gas_Natural_SDG_SA   \n",
       "433  Skandinaviska_Enskilda_Banken_AB              Europe_50_Index   \n",
       "434                        KELLOGG_CO                  PEPSICO_INC   \n",
       "435                               NaN                          NaN   \n",
       "\n",
       "                         Sec7                          Sec8  \\\n",
       "0      T_ROWE_PRICE_GROUP_INC   DISCOVER_FINANCIAL_SERVICES   \n",
       "1     BECTON_DICKINSON_AND_CO                     BB&T_CORP   \n",
       "2                AUTODESK_INC           ELECTRONIC_ARTS_INC   \n",
       "3    Vanguard_FTSE_Europe_ETF                     BB&T_CORP   \n",
       "4           STATE_STREET_CORP            SUNTRUST_BANKS_INC   \n",
       "..                        ...                           ...   \n",
       "431                       NaN                           NaN   \n",
       "432                   SSE_PLC                         Engie   \n",
       "433            Air_Liquide_SA                BNP_Paribas_SA   \n",
       "434   PROCTER_&_GAMBLE_CO_THE  MONDELEZ_INTERNATIONAL_INC-A   \n",
       "435                       NaN                           NaN   \n",
       "\n",
       "                                    Sec9      R2_2  ...    EndDate8  \\\n",
       "0             STANLEY_BLACK_&_DECKER_INC  0.660427  ...  2018-03-02   \n",
       "1               PRUDENTIAL_FINANCIAL_INC  0.682331  ...  2018-03-02   \n",
       "2                 T_ROWE_PRICE_GROUP_INC  0.617346  ...  2018-01-10   \n",
       "3                      STATE_STREET_CORP  0.768484  ...  2018-03-02   \n",
       "4                              BB&T_CORP  0.462562  ...  2018-03-02   \n",
       "..                                   ...       ...  ...         ...   \n",
       "431                                  NaN  0.097994  ...         NaN   \n",
       "432                     Severn_Trent_PLC  0.380371  ...  2016-12-14   \n",
       "433  LVMH_Moet_Hennessy_Louis_Vuitton_SA  0.370738  ...  2017-12-29   \n",
       "434                     COCA-COLA_CO_THE  0.456070  ...  2018-03-02   \n",
       "435                                  NaN  0.078682  ...         NaN   \n",
       "\n",
       "       EndDate9  NumDatapoints2  NumDatapoints3  NumDatapoints4  \\\n",
       "0    2018-03-02             545             545             545   \n",
       "1    2018-03-02             545             545             545   \n",
       "2    2018-03-02             545             545             545   \n",
       "3    2018-03-02             545             564             545   \n",
       "4    2018-03-02             545             545             545   \n",
       "..          ...             ...             ...             ...   \n",
       "431         NaN             163             167             159   \n",
       "432  2016-12-14             266             276             276   \n",
       "433  2017-12-29             198             253             198   \n",
       "434  2018-03-02             542             542             542   \n",
       "435         NaN             216             151             216   \n",
       "\n",
       "     NumDatapoints5  NumDatapoints6 NumDatapoints7 NumDatapoints8  \\\n",
       "0               541             563            541            545   \n",
       "1               541             545            563            563   \n",
       "2               541             541            564            545   \n",
       "3               545             541            541            563   \n",
       "4               541             541            541            542   \n",
       "..              ...             ...            ...            ...   \n",
       "431             NaN             NaN            NaN            NaN   \n",
       "432             276             275            269            276   \n",
       "433             250             207            208            209   \n",
       "434             542             542            542            541   \n",
       "435             NaN             NaN            NaN            NaN   \n",
       "\n",
       "    NumDatapoints9  \n",
       "0              541  \n",
       "1              543  \n",
       "2              541  \n",
       "3              541  \n",
       "4              563  \n",
       "..             ...  \n",
       "431            NaN  \n",
       "432            271  \n",
       "433            209  \n",
       "434            542  \n",
       "435            NaN  \n",
       "\n",
       "[436 rows x 49 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct combDF\n",
    "for sec in regressionDF.Sec1.unique():\n",
    "    sec_sub = regressionDF.loc[regressionDF.Sec1 == sec]\n",
    "    # [] subset for EndDate\n",
    "    r2_sub = sec_sub.loc[sec_sub.Adjusted_R2 > r2_threshold]\n",
    "    final_sub = 'not set yet'\n",
    "    if len(r2_sub) < min_sec_bundle: #just grab the best min_sec_bundle sec2s\n",
    "        final_sub = sec_sub.sort_values(by='Adjusted_R2', ascending=False).reset_index(drop=True).iloc[:min_sec_bundle]\n",
    "    else:\n",
    "        final_sub = r2_sub.sort_values(by='Adjusted_R2', ascending=False).reset_index(drop=True).iloc[:max_sec_bundle]\n",
    "    new_comb_row = {'Sec1': sec}\n",
    "    for i in range(len(final_sub)):\n",
    "        new_comb_row['Sec'+str(i+2)] = final_sub.Sec2.iloc[i]\n",
    "        new_comb_row['R2_'+str(i+2)] = final_sub.R2.iloc[i]\n",
    "        new_comb_row['p_value'+str(i+2)] = final_sub.p_value.iloc[i]\n",
    "        new_comb_row['StartDate'+str(i+2)] = final_sub.StartDate.iloc[i]\n",
    "        new_comb_row['EndDate'+str(i+2)] = final_sub.EndDate.iloc[i]\n",
    "        new_comb_row['NumDatapoints'+str(i+2)] = final_sub.NumDatapoints.iloc[i]\n",
    "    combDF = combDF.append(new_comb_row, ignore_index=True)\n",
    "combDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sec1</th>\n",
       "      <th>Sec2</th>\n",
       "      <th>Sec3</th>\n",
       "      <th>Sec4</th>\n",
       "      <th>Sec5</th>\n",
       "      <th>Sec6</th>\n",
       "      <th>Sec7</th>\n",
       "      <th>Sec8</th>\n",
       "      <th>Sec9</th>\n",
       "      <th>R2_2</th>\n",
       "      <th>...</th>\n",
       "      <th>EndDate8</th>\n",
       "      <th>EndDate9</th>\n",
       "      <th>NumDatapoints2</th>\n",
       "      <th>NumDatapoints3</th>\n",
       "      <th>NumDatapoints4</th>\n",
       "      <th>NumDatapoints5</th>\n",
       "      <th>NumDatapoints6</th>\n",
       "      <th>NumDatapoints7</th>\n",
       "      <th>NumDatapoints8</th>\n",
       "      <th>NumDatapoints9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>US_Dollar_Index</td>\n",
       "      <td>EURUSD</td>\n",
       "      <td>USDCHF</td>\n",
       "      <td>AUDUSD</td>\n",
       "      <td>NZDUSD</td>\n",
       "      <td>USDSEK</td>\n",
       "      <td>USDJPY</td>\n",
       "      <td>GBPUSD</td>\n",
       "      <td>USDCAD</td>\n",
       "      <td>0.650352</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>564</td>\n",
       "      <td>534</td>\n",
       "      <td>534</td>\n",
       "      <td>534</td>\n",
       "      <td>534</td>\n",
       "      <td>534</td>\n",
       "      <td>534</td>\n",
       "      <td>534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Sec1    Sec2    Sec3    Sec4    Sec5    Sec6    Sec7    Sec8  \\\n",
       "190  US_Dollar_Index  EURUSD  USDCHF  AUDUSD  NZDUSD  USDSEK  USDJPY  GBPUSD   \n",
       "\n",
       "       Sec9      R2_2  ...    EndDate8    EndDate9  NumDatapoints2  \\\n",
       "190  USDCAD  0.650352  ...  2018-01-09  2018-01-09             564   \n",
       "\n",
       "     NumDatapoints3  NumDatapoints4  NumDatapoints5  NumDatapoints6  \\\n",
       "190             534             534             534             534   \n",
       "\n",
       "    NumDatapoints7 NumDatapoints8 NumDatapoints9  \n",
       "190            534            534            534  \n",
       "\n",
       "[1 rows x 49 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combDF.loc[combDF.Sec1 == 'US_Dollar_Index']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Determine which sets of dates to try </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 499 unique securities in combDF\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# [x] check that all secs in combDF exist in data_summary\n",
    "secs_cols = ['Sec'+str(i) for i in range(1, max_sec_bundle+2)]\n",
    "comb_secs = []\n",
    "for s in secs_cols:\n",
    "    comb_secs = np.append(comb_secs, combDF[s].values)\n",
    "comb_secs = list(set(comb_secs))\n",
    "comb_secs = [s for s in comb_secs if type(s)==str]\n",
    "print('found '+str(len(comb_secs))+' unique securities in combDF')\n",
    "print([s for s in comb_secs if s not in data_summaryDF.Name.values])\n",
    "assert(np.all([s in data_summaryDF.Name.values for s in comb_secs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove CRH_PLC\n",
    "# raw_regressionDF = pd.read_csv(regression_results_fn)\n",
    "# raw_regressionDF = raw_regressionDF.loc[(raw_regressionDF.Sec1 != 'CRH_PLC') & (raw_regressionDF.Sec2 != 'CRH_PLC')].reset_index(drop=True)\n",
    "# raw_regressionDF.to_csv(regression_results_fn, index=False)S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, max_sec_bundle+2):\n",
    "    combDF['TotalStartDate'+str(i)] = np.nan\n",
    "    combDF['TotalEndDate'+str(i)] = np.nan\n",
    "        \n",
    "for i in range(len(combDF)):\n",
    "    #get total Start and End dates\n",
    "    for j in range(1, max_sec_bundle+2):\n",
    "        if type(combDF['Sec'+str(j)][i]) == str:\n",
    "            combDF.loc[combDF['Sec'+str(j)] == combDF['Sec'+str(j)][i], 'TotalStartDate'+str(j)] = data_summaryDF.loc[data_summaryDF.Name == combDF['Sec'+str(j)][i], 'StartDate'].values[0]\n",
    "            combDF.loc[combDF['Sec'+str(j)] == combDF['Sec'+str(j)][i], 'TotalEndDate'+str(j)] = data_summaryDF.loc[data_summaryDF.Name == combDF['Sec'+str(j)][i], 'EndDate'].values[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sec1</th>\n",
       "      <th>Sec2</th>\n",
       "      <th>Sec3</th>\n",
       "      <th>Sec4</th>\n",
       "      <th>Sec5</th>\n",
       "      <th>Sec6</th>\n",
       "      <th>Sec7</th>\n",
       "      <th>Sec8</th>\n",
       "      <th>Sec9</th>\n",
       "      <th>R2_2</th>\n",
       "      <th>...</th>\n",
       "      <th>TotalStartDate6</th>\n",
       "      <th>TotalEndDate6</th>\n",
       "      <th>TotalStartDate7</th>\n",
       "      <th>TotalEndDate7</th>\n",
       "      <th>TotalStartDate8</th>\n",
       "      <th>TotalEndDate8</th>\n",
       "      <th>TotalStartDate9</th>\n",
       "      <th>TotalEndDate9</th>\n",
       "      <th>LastStartDate</th>\n",
       "      <th>FirstEndDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3M_CO</td>\n",
       "      <td>iShares_Russell_1000_Value_ETF</td>\n",
       "      <td>iShares_Russell_1000_Growth_ETF</td>\n",
       "      <td>iShares_Core_S&amp;P_Mid-Cap_ETF</td>\n",
       "      <td>STATE_STREET_CORP</td>\n",
       "      <td>BB&amp;T_CORP</td>\n",
       "      <td>T_ROWE_PRICE_GROUP_INC</td>\n",
       "      <td>DISCOVER_FINANCIAL_SERVICES</td>\n",
       "      <td>STANLEY_BLACK_&amp;_DECKER_INC</td>\n",
       "      <td>0.660427</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>2019-12-06</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-19</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-13</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-18</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ABBOTT_LABORATORIES</td>\n",
       "      <td>iShares_Russell_1000_Value_ETF</td>\n",
       "      <td>iShares_Russell_1000_Growth_ETF</td>\n",
       "      <td>iShares_Core_S&amp;P_Mid-Cap_ETF</td>\n",
       "      <td>STATE_STREET_CORP</td>\n",
       "      <td>DISCOVER_FINANCIAL_SERVICES</td>\n",
       "      <td>BECTON_DICKINSON_AND_CO</td>\n",
       "      <td>BB&amp;T_CORP</td>\n",
       "      <td>PRUDENTIAL_FINANCIAL_INC</td>\n",
       "      <td>0.682331</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-13</td>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>2019-12-17</td>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>2019-12-06</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-13</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ACTIVISION_BLIZZARD_INC</td>\n",
       "      <td>iShares_Russell_1000_Growth_ETF</td>\n",
       "      <td>iShares_Russell_1000_Value_ETF</td>\n",
       "      <td>iShares_Core_S&amp;P_Mid-Cap_ETF</td>\n",
       "      <td>Vanguard_FTSE_Europe_ETF</td>\n",
       "      <td>XILINX_INC</td>\n",
       "      <td>AUTODESK_INC</td>\n",
       "      <td>ELECTRONIC_ARTS_INC</td>\n",
       "      <td>T_ROWE_PRICE_GROUP_INC</td>\n",
       "      <td>0.617346</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-13</td>\n",
       "      <td>2018-01-30</td>\n",
       "      <td>2019-12-19</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2019-12-13</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-19</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ADOBE_SYSTEMS_INC</td>\n",
       "      <td>iShares_Russell_1000_Growth_ETF</td>\n",
       "      <td>AUTODESK_INC</td>\n",
       "      <td>iShares_Russell_1000_Value_ETF</td>\n",
       "      <td>iShares_Core_S&amp;P_Mid-Cap_ETF</td>\n",
       "      <td>T_ROWE_PRICE_GROUP_INC</td>\n",
       "      <td>Vanguard_FTSE_Europe_ETF</td>\n",
       "      <td>BB&amp;T_CORP</td>\n",
       "      <td>STATE_STREET_CORP</td>\n",
       "      <td>0.768484</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-19</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-19</td>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>2019-12-06</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-18</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ADVANCED_MICRO_DEVICES</td>\n",
       "      <td>iShares_Russell_1000_Growth_ETF</td>\n",
       "      <td>iShares_Russell_1000_Value_ETF</td>\n",
       "      <td>iShares_Core_S&amp;P_Mid-Cap_ETF</td>\n",
       "      <td>XILINX_INC</td>\n",
       "      <td>Vanguard_FTSE_Europe_ETF</td>\n",
       "      <td>STATE_STREET_CORP</td>\n",
       "      <td>SUNTRUST_BANKS_INC</td>\n",
       "      <td>BB&amp;T_CORP</td>\n",
       "      <td>0.462562</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-19</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-18</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-06</td>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>2019-12-06</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>431</td>\n",
       "      <td>Vallourec_SA</td>\n",
       "      <td>Vanguard_FTSE_Europe_ETF</td>\n",
       "      <td>EMERSON_ELECTRIC_CO</td>\n",
       "      <td>PPG_INDUSTRIES_INC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.097994</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>2019-12-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>Veolia_Environnement_SA</td>\n",
       "      <td>EDP_-_Energias_de_Portugal_SA</td>\n",
       "      <td>United_Utilities_Group_PLC</td>\n",
       "      <td>E.ON_SE</td>\n",
       "      <td>Centrica_PLC</td>\n",
       "      <td>Gas_Natural_SDG_SA</td>\n",
       "      <td>SSE_PLC</td>\n",
       "      <td>Engie</td>\n",
       "      <td>Severn_Trent_PLC</td>\n",
       "      <td>0.380371</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-11-14</td>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>2016-11-14</td>\n",
       "      <td>2019-12-24</td>\n",
       "      <td>2016-11-14</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>2016-11-14</td>\n",
       "      <td>2019-12-24</td>\n",
       "      <td>2016-11-14</td>\n",
       "      <td>2019-12-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>433</td>\n",
       "      <td>Investor_AB</td>\n",
       "      <td>Allianz_SE</td>\n",
       "      <td>Atlas_Copco_AB</td>\n",
       "      <td>BASF_SE</td>\n",
       "      <td>Skandinaviska_Enskilda_Banken_AB</td>\n",
       "      <td>Europe_50_Index</td>\n",
       "      <td>Air_Liquide_SA</td>\n",
       "      <td>BNP_Paribas_SA</td>\n",
       "      <td>LVMH_Moet_Hennessy_Louis_Vuitton_SA</td>\n",
       "      <td>0.370738</td>\n",
       "      <td>...</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2019-12-10</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>2019-12-23</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>2019-12-23</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>2019-12-23</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2019-12-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>434</td>\n",
       "      <td>JM_SMUCKER_COMPANY</td>\n",
       "      <td>Consumer_Staples_Select_Sector_SPDR_Fund</td>\n",
       "      <td>CONAGRA_FOODS_INC</td>\n",
       "      <td>GENERAL_MILLS_INC</td>\n",
       "      <td>KELLOGG_CO</td>\n",
       "      <td>PEPSICO_INC</td>\n",
       "      <td>PROCTER_&amp;_GAMBLE_CO_THE</td>\n",
       "      <td>MONDELEZ_INTERNATIONAL_INC-A</td>\n",
       "      <td>COCA-COLA_CO_THE</td>\n",
       "      <td>0.456070</td>\n",
       "      <td>...</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2019-12-09</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2019-12-13</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2019-12-19</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2019-12-16</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435</td>\n",
       "      <td>ProSiebenSat.1_Media_AG</td>\n",
       "      <td>L'Oreal_SA</td>\n",
       "      <td>BANK_OF_NEW_YORK_MELLON_CORP</td>\n",
       "      <td>Air_Liquide_SA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.078682</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2019-12-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>436 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Sec1                                      Sec2  \\\n",
       "0                      3M_CO            iShares_Russell_1000_Value_ETF   \n",
       "1        ABBOTT_LABORATORIES            iShares_Russell_1000_Value_ETF   \n",
       "2    ACTIVISION_BLIZZARD_INC           iShares_Russell_1000_Growth_ETF   \n",
       "3          ADOBE_SYSTEMS_INC           iShares_Russell_1000_Growth_ETF   \n",
       "4     ADVANCED_MICRO_DEVICES           iShares_Russell_1000_Growth_ETF   \n",
       "..                       ...                                       ...   \n",
       "431             Vallourec_SA                  Vanguard_FTSE_Europe_ETF   \n",
       "432  Veolia_Environnement_SA             EDP_-_Energias_de_Portugal_SA   \n",
       "433              Investor_AB                                Allianz_SE   \n",
       "434       JM_SMUCKER_COMPANY  Consumer_Staples_Select_Sector_SPDR_Fund   \n",
       "435  ProSiebenSat.1_Media_AG                                L'Oreal_SA   \n",
       "\n",
       "                                Sec3                            Sec4  \\\n",
       "0    iShares_Russell_1000_Growth_ETF    iShares_Core_S&P_Mid-Cap_ETF   \n",
       "1    iShares_Russell_1000_Growth_ETF    iShares_Core_S&P_Mid-Cap_ETF   \n",
       "2     iShares_Russell_1000_Value_ETF    iShares_Core_S&P_Mid-Cap_ETF   \n",
       "3                       AUTODESK_INC  iShares_Russell_1000_Value_ETF   \n",
       "4     iShares_Russell_1000_Value_ETF    iShares_Core_S&P_Mid-Cap_ETF   \n",
       "..                               ...                             ...   \n",
       "431              EMERSON_ELECTRIC_CO              PPG_INDUSTRIES_INC   \n",
       "432       United_Utilities_Group_PLC                         E.ON_SE   \n",
       "433                   Atlas_Copco_AB                         BASF_SE   \n",
       "434                CONAGRA_FOODS_INC               GENERAL_MILLS_INC   \n",
       "435     BANK_OF_NEW_YORK_MELLON_CORP                  Air_Liquide_SA   \n",
       "\n",
       "                                 Sec5                         Sec6  \\\n",
       "0                   STATE_STREET_CORP                    BB&T_CORP   \n",
       "1                   STATE_STREET_CORP  DISCOVER_FINANCIAL_SERVICES   \n",
       "2            Vanguard_FTSE_Europe_ETF                   XILINX_INC   \n",
       "3        iShares_Core_S&P_Mid-Cap_ETF       T_ROWE_PRICE_GROUP_INC   \n",
       "4                          XILINX_INC     Vanguard_FTSE_Europe_ETF   \n",
       "..                                ...                          ...   \n",
       "431                               NaN                          NaN   \n",
       "432                      Centrica_PLC           Gas_Natural_SDG_SA   \n",
       "433  Skandinaviska_Enskilda_Banken_AB              Europe_50_Index   \n",
       "434                        KELLOGG_CO                  PEPSICO_INC   \n",
       "435                               NaN                          NaN   \n",
       "\n",
       "                         Sec7                          Sec8  \\\n",
       "0      T_ROWE_PRICE_GROUP_INC   DISCOVER_FINANCIAL_SERVICES   \n",
       "1     BECTON_DICKINSON_AND_CO                     BB&T_CORP   \n",
       "2                AUTODESK_INC           ELECTRONIC_ARTS_INC   \n",
       "3    Vanguard_FTSE_Europe_ETF                     BB&T_CORP   \n",
       "4           STATE_STREET_CORP            SUNTRUST_BANKS_INC   \n",
       "..                        ...                           ...   \n",
       "431                       NaN                           NaN   \n",
       "432                   SSE_PLC                         Engie   \n",
       "433            Air_Liquide_SA                BNP_Paribas_SA   \n",
       "434   PROCTER_&_GAMBLE_CO_THE  MONDELEZ_INTERNATIONAL_INC-A   \n",
       "435                       NaN                           NaN   \n",
       "\n",
       "                                    Sec9      R2_2  ...  TotalStartDate6  \\\n",
       "0             STANLEY_BLACK_&_DECKER_INC  0.660427  ...       2018-01-31   \n",
       "1               PRUDENTIAL_FINANCIAL_INC  0.682331  ...       2018-02-01   \n",
       "2                 T_ROWE_PRICE_GROUP_INC  0.617346  ...       2018-02-01   \n",
       "3                      STATE_STREET_CORP  0.768484  ...       2018-02-01   \n",
       "4                              BB&T_CORP  0.462562  ...       2018-02-01   \n",
       "..                                   ...       ...  ...              ...   \n",
       "431                                  NaN  0.097994  ...              NaN   \n",
       "432                     Severn_Trent_PLC  0.380371  ...       2016-11-14   \n",
       "433  LVMH_Moet_Hennessy_Louis_Vuitton_SA  0.370738  ...       2017-12-11   \n",
       "434                     COCA-COLA_CO_THE  0.456070  ...       2017-12-11   \n",
       "435                                  NaN  0.078682  ...              NaN   \n",
       "\n",
       "     TotalEndDate6  TotalStartDate7  TotalEndDate7  TotalStartDate8  \\\n",
       "0       2019-12-06       2018-02-01     2019-12-19       2018-02-01   \n",
       "1       2019-12-13       2018-01-31     2019-12-17       2018-01-31   \n",
       "2       2019-12-13       2018-01-30     2019-12-19       2017-12-11   \n",
       "3       2019-12-19       2018-02-01     2019-12-19       2018-01-31   \n",
       "4       2019-12-19       2018-02-01     2019-12-18       2018-02-01   \n",
       "..             ...              ...            ...              ...   \n",
       "431            NaN              NaN            NaN              NaN   \n",
       "432     2019-12-27       2016-11-14     2019-12-24       2016-11-14   \n",
       "433     2019-12-10       2017-12-01     2019-12-23       2017-12-01   \n",
       "434     2019-12-09       2017-12-11     2019-12-13       2017-12-11   \n",
       "435            NaN              NaN            NaN              NaN   \n",
       "\n",
       "     TotalEndDate8  TotalStartDate9 TotalEndDate9 LastStartDate FirstEndDate  \n",
       "0       2019-12-13       2018-02-01    2019-12-18    2018-02-01   2019-12-06  \n",
       "1       2019-12-06       2018-02-01    2019-12-13    2018-02-01   2019-12-06  \n",
       "2       2019-12-13       2018-02-01    2019-12-19    2018-02-01   2019-12-10  \n",
       "3       2019-12-06       2018-02-01    2019-12-18    2018-02-01   2019-12-06  \n",
       "4       2019-12-06       2018-01-31    2019-12-06    2018-02-01   2019-12-06  \n",
       "..             ...              ...           ...           ...          ...  \n",
       "431            NaN              NaN           NaN    2018-02-02   2019-12-13  \n",
       "432     2020-01-03       2016-11-14    2019-12-24    2016-11-14   2019-12-24  \n",
       "433     2019-12-23       2017-12-01    2019-12-23    2017-12-11   2019-12-10  \n",
       "434     2019-12-19       2017-12-11    2019-12-16    2018-02-01   2019-12-09  \n",
       "435            NaN              NaN           NaN    2017-12-11   2019-12-13  \n",
       "\n",
       "[436 rows x 69 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combDF['LastStartDate'] = np.nan\n",
    "combDF['FirstEndDate'] = np.nan\n",
    "for i in combDF.index:\n",
    "    comb_row = combDF.loc[i]\n",
    "    start_dates = comb_row[[col for col in combDF.columns if 'TotalStartDate' in col]].values\n",
    "    start_dates = [d for d in start_dates if type(d) == str]\n",
    "    last_start_date = max(start_dates)\n",
    "    end_dates = comb_row[[col for col in combDF.columns if 'TotalEndDate' in col]]\n",
    "    end_dates = [d for d in end_dates if type(d) == str]\n",
    "    first_end_date = min(end_dates)\n",
    "    combDF.loc[i, 'LastStartDate'] = last_start_date\n",
    "    combDF.loc[i, 'FirstEndDate'] = first_end_date\n",
    "combDF   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the date columns into actual dates\n",
    "date_cols = [col for col in combDF.columns if 'Date' in col]\n",
    "for col in date_cols:\n",
    "    combDF[col] = pd.to_datetime(combDF[col]).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================num_days < min_num_days_total! Skipping row 55:\n",
      "Sec1                Natural_Gas\n",
      "Sec2                   Linde_AG\n",
      "Sec3                     SAP_AG\n",
      "Sec4              Cap_Gemini_SA\n",
      "Sec5                        NaN\n",
      "                      ...      \n",
      "LastStartDate        2019-12-02\n",
      "FirstEndDate         2019-12-09\n",
      "TrainStartDate              NaN\n",
      "ValStartDate                NaN\n",
      "ValEndDate                  NaN\n",
      "Name: 55, Length: 72, dtype: object\n",
      "\n",
      "==================================================num_days < min_num_days_total! Skipping row 417:\n",
      "Sec1                        Coloplast_AS\n",
      "Sec2              INTUITIVE_SURGICAL_INC\n",
      "Sec3                         Natural_Gas\n",
      "Sec4                     Novo_Nordisk_AS\n",
      "Sec5                                 NaN\n",
      "                           ...          \n",
      "LastStartDate                 2019-12-02\n",
      "FirstEndDate                  2019-12-09\n",
      "TrainStartDate                       NaN\n",
      "ValStartDate                         NaN\n",
      "ValEndDate                           NaN\n",
      "Name: 417, Length: 72, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sec1</th>\n",
       "      <th>Sec2</th>\n",
       "      <th>Sec3</th>\n",
       "      <th>Sec4</th>\n",
       "      <th>Sec5</th>\n",
       "      <th>Sec6</th>\n",
       "      <th>Sec7</th>\n",
       "      <th>Sec8</th>\n",
       "      <th>Sec9</th>\n",
       "      <th>R2_2</th>\n",
       "      <th>...</th>\n",
       "      <th>TotalEndDate7</th>\n",
       "      <th>TotalStartDate8</th>\n",
       "      <th>TotalEndDate8</th>\n",
       "      <th>TotalStartDate9</th>\n",
       "      <th>TotalEndDate9</th>\n",
       "      <th>LastStartDate</th>\n",
       "      <th>FirstEndDate</th>\n",
       "      <th>TrainStartDate</th>\n",
       "      <th>ValStartDate</th>\n",
       "      <th>ValEndDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3M_CO</td>\n",
       "      <td>iShares_Russell_1000_Value_ETF</td>\n",
       "      <td>iShares_Russell_1000_Growth_ETF</td>\n",
       "      <td>iShares_Core_S&amp;P_Mid-Cap_ETF</td>\n",
       "      <td>STATE_STREET_CORP</td>\n",
       "      <td>BB&amp;T_CORP</td>\n",
       "      <td>T_ROWE_PRICE_GROUP_INC</td>\n",
       "      <td>DISCOVER_FINANCIAL_SERVICES</td>\n",
       "      <td>STANLEY_BLACK_&amp;_DECKER_INC</td>\n",
       "      <td>0.660427</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-12-19</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-13</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-18</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-06</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-04-21</td>\n",
       "      <td>2019-10-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ABBOTT_LABORATORIES</td>\n",
       "      <td>iShares_Russell_1000_Value_ETF</td>\n",
       "      <td>iShares_Russell_1000_Growth_ETF</td>\n",
       "      <td>iShares_Core_S&amp;P_Mid-Cap_ETF</td>\n",
       "      <td>STATE_STREET_CORP</td>\n",
       "      <td>DISCOVER_FINANCIAL_SERVICES</td>\n",
       "      <td>BECTON_DICKINSON_AND_CO</td>\n",
       "      <td>BB&amp;T_CORP</td>\n",
       "      <td>PRUDENTIAL_FINANCIAL_INC</td>\n",
       "      <td>0.682331</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-12-17</td>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>2019-12-06</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-13</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-06</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-04-21</td>\n",
       "      <td>2019-10-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ACTIVISION_BLIZZARD_INC</td>\n",
       "      <td>iShares_Russell_1000_Growth_ETF</td>\n",
       "      <td>iShares_Russell_1000_Value_ETF</td>\n",
       "      <td>iShares_Core_S&amp;P_Mid-Cap_ETF</td>\n",
       "      <td>Vanguard_FTSE_Europe_ETF</td>\n",
       "      <td>XILINX_INC</td>\n",
       "      <td>AUTODESK_INC</td>\n",
       "      <td>ELECTRONIC_ARTS_INC</td>\n",
       "      <td>T_ROWE_PRICE_GROUP_INC</td>\n",
       "      <td>0.617346</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-12-19</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2019-12-13</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-19</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-10</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-04-23</td>\n",
       "      <td>2019-10-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ADOBE_SYSTEMS_INC</td>\n",
       "      <td>iShares_Russell_1000_Growth_ETF</td>\n",
       "      <td>AUTODESK_INC</td>\n",
       "      <td>iShares_Russell_1000_Value_ETF</td>\n",
       "      <td>iShares_Core_S&amp;P_Mid-Cap_ETF</td>\n",
       "      <td>T_ROWE_PRICE_GROUP_INC</td>\n",
       "      <td>Vanguard_FTSE_Europe_ETF</td>\n",
       "      <td>BB&amp;T_CORP</td>\n",
       "      <td>STATE_STREET_CORP</td>\n",
       "      <td>0.768484</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-12-19</td>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>2019-12-06</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-18</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-06</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-04-21</td>\n",
       "      <td>2019-10-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ADVANCED_MICRO_DEVICES</td>\n",
       "      <td>iShares_Russell_1000_Growth_ETF</td>\n",
       "      <td>iShares_Russell_1000_Value_ETF</td>\n",
       "      <td>iShares_Core_S&amp;P_Mid-Cap_ETF</td>\n",
       "      <td>XILINX_INC</td>\n",
       "      <td>Vanguard_FTSE_Europe_ETF</td>\n",
       "      <td>STATE_STREET_CORP</td>\n",
       "      <td>SUNTRUST_BANKS_INC</td>\n",
       "      <td>BB&amp;T_CORP</td>\n",
       "      <td>0.462562</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-12-18</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-06</td>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>2019-12-06</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-06</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-04-21</td>\n",
       "      <td>2019-10-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>431</td>\n",
       "      <td>Vallourec_SA</td>\n",
       "      <td>Vanguard_FTSE_Europe_ETF</td>\n",
       "      <td>EMERSON_ELECTRIC_CO</td>\n",
       "      <td>PPG_INDUSTRIES_INC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.097994</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>2019-12-13</td>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>2019-04-26</td>\n",
       "      <td>2019-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>Veolia_Environnement_SA</td>\n",
       "      <td>EDP_-_Energias_de_Portugal_SA</td>\n",
       "      <td>United_Utilities_Group_PLC</td>\n",
       "      <td>E.ON_SE</td>\n",
       "      <td>Centrica_PLC</td>\n",
       "      <td>Gas_Natural_SDG_SA</td>\n",
       "      <td>SSE_PLC</td>\n",
       "      <td>Engie</td>\n",
       "      <td>Severn_Trent_PLC</td>\n",
       "      <td>0.380371</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-12-24</td>\n",
       "      <td>2016-11-14</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>2016-11-14</td>\n",
       "      <td>2019-12-24</td>\n",
       "      <td>2016-11-14</td>\n",
       "      <td>2019-12-24</td>\n",
       "      <td>2016-11-14</td>\n",
       "      <td>2018-12-03</td>\n",
       "      <td>2019-10-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>433</td>\n",
       "      <td>Investor_AB</td>\n",
       "      <td>Allianz_SE</td>\n",
       "      <td>Atlas_Copco_AB</td>\n",
       "      <td>BASF_SE</td>\n",
       "      <td>Skandinaviska_Enskilda_Banken_AB</td>\n",
       "      <td>Europe_50_Index</td>\n",
       "      <td>Air_Liquide_SA</td>\n",
       "      <td>BNP_Paribas_SA</td>\n",
       "      <td>LVMH_Moet_Hennessy_Louis_Vuitton_SA</td>\n",
       "      <td>0.370738</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-12-23</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>2019-12-23</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>2019-12-23</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2019-12-10</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2019-04-06</td>\n",
       "      <td>2019-10-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>434</td>\n",
       "      <td>JM_SMUCKER_COMPANY</td>\n",
       "      <td>Consumer_Staples_Select_Sector_SPDR_Fund</td>\n",
       "      <td>CONAGRA_FOODS_INC</td>\n",
       "      <td>GENERAL_MILLS_INC</td>\n",
       "      <td>KELLOGG_CO</td>\n",
       "      <td>PEPSICO_INC</td>\n",
       "      <td>PROCTER_&amp;_GAMBLE_CO_THE</td>\n",
       "      <td>MONDELEZ_INTERNATIONAL_INC-A</td>\n",
       "      <td>COCA-COLA_CO_THE</td>\n",
       "      <td>0.456070</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-12-13</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2019-12-19</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2019-12-16</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-12-09</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2019-04-23</td>\n",
       "      <td>2019-10-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435</td>\n",
       "      <td>ProSiebenSat.1_Media_AG</td>\n",
       "      <td>L'Oreal_SA</td>\n",
       "      <td>BANK_OF_NEW_YORK_MELLON_CORP</td>\n",
       "      <td>Air_Liquide_SA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.078682</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2019-12-13</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>2019-04-08</td>\n",
       "      <td>2019-10-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>436 rows × 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Sec1                                      Sec2  \\\n",
       "0                      3M_CO            iShares_Russell_1000_Value_ETF   \n",
       "1        ABBOTT_LABORATORIES            iShares_Russell_1000_Value_ETF   \n",
       "2    ACTIVISION_BLIZZARD_INC           iShares_Russell_1000_Growth_ETF   \n",
       "3          ADOBE_SYSTEMS_INC           iShares_Russell_1000_Growth_ETF   \n",
       "4     ADVANCED_MICRO_DEVICES           iShares_Russell_1000_Growth_ETF   \n",
       "..                       ...                                       ...   \n",
       "431             Vallourec_SA                  Vanguard_FTSE_Europe_ETF   \n",
       "432  Veolia_Environnement_SA             EDP_-_Energias_de_Portugal_SA   \n",
       "433              Investor_AB                                Allianz_SE   \n",
       "434       JM_SMUCKER_COMPANY  Consumer_Staples_Select_Sector_SPDR_Fund   \n",
       "435  ProSiebenSat.1_Media_AG                                L'Oreal_SA   \n",
       "\n",
       "                                Sec3                            Sec4  \\\n",
       "0    iShares_Russell_1000_Growth_ETF    iShares_Core_S&P_Mid-Cap_ETF   \n",
       "1    iShares_Russell_1000_Growth_ETF    iShares_Core_S&P_Mid-Cap_ETF   \n",
       "2     iShares_Russell_1000_Value_ETF    iShares_Core_S&P_Mid-Cap_ETF   \n",
       "3                       AUTODESK_INC  iShares_Russell_1000_Value_ETF   \n",
       "4     iShares_Russell_1000_Value_ETF    iShares_Core_S&P_Mid-Cap_ETF   \n",
       "..                               ...                             ...   \n",
       "431              EMERSON_ELECTRIC_CO              PPG_INDUSTRIES_INC   \n",
       "432       United_Utilities_Group_PLC                         E.ON_SE   \n",
       "433                   Atlas_Copco_AB                         BASF_SE   \n",
       "434                CONAGRA_FOODS_INC               GENERAL_MILLS_INC   \n",
       "435     BANK_OF_NEW_YORK_MELLON_CORP                  Air_Liquide_SA   \n",
       "\n",
       "                                 Sec5                         Sec6  \\\n",
       "0                   STATE_STREET_CORP                    BB&T_CORP   \n",
       "1                   STATE_STREET_CORP  DISCOVER_FINANCIAL_SERVICES   \n",
       "2            Vanguard_FTSE_Europe_ETF                   XILINX_INC   \n",
       "3        iShares_Core_S&P_Mid-Cap_ETF       T_ROWE_PRICE_GROUP_INC   \n",
       "4                          XILINX_INC     Vanguard_FTSE_Europe_ETF   \n",
       "..                                ...                          ...   \n",
       "431                               NaN                          NaN   \n",
       "432                      Centrica_PLC           Gas_Natural_SDG_SA   \n",
       "433  Skandinaviska_Enskilda_Banken_AB              Europe_50_Index   \n",
       "434                        KELLOGG_CO                  PEPSICO_INC   \n",
       "435                               NaN                          NaN   \n",
       "\n",
       "                         Sec7                          Sec8  \\\n",
       "0      T_ROWE_PRICE_GROUP_INC   DISCOVER_FINANCIAL_SERVICES   \n",
       "1     BECTON_DICKINSON_AND_CO                     BB&T_CORP   \n",
       "2                AUTODESK_INC           ELECTRONIC_ARTS_INC   \n",
       "3    Vanguard_FTSE_Europe_ETF                     BB&T_CORP   \n",
       "4           STATE_STREET_CORP            SUNTRUST_BANKS_INC   \n",
       "..                        ...                           ...   \n",
       "431                       NaN                           NaN   \n",
       "432                   SSE_PLC                         Engie   \n",
       "433            Air_Liquide_SA                BNP_Paribas_SA   \n",
       "434   PROCTER_&_GAMBLE_CO_THE  MONDELEZ_INTERNATIONAL_INC-A   \n",
       "435                       NaN                           NaN   \n",
       "\n",
       "                                    Sec9      R2_2  ...  TotalEndDate7  \\\n",
       "0             STANLEY_BLACK_&_DECKER_INC  0.660427  ...     2019-12-19   \n",
       "1               PRUDENTIAL_FINANCIAL_INC  0.682331  ...     2019-12-17   \n",
       "2                 T_ROWE_PRICE_GROUP_INC  0.617346  ...     2019-12-19   \n",
       "3                      STATE_STREET_CORP  0.768484  ...     2019-12-19   \n",
       "4                              BB&T_CORP  0.462562  ...     2019-12-18   \n",
       "..                                   ...       ...  ...            ...   \n",
       "431                                  NaN  0.097994  ...            NaT   \n",
       "432                     Severn_Trent_PLC  0.380371  ...     2019-12-24   \n",
       "433  LVMH_Moet_Hennessy_Louis_Vuitton_SA  0.370738  ...     2019-12-23   \n",
       "434                     COCA-COLA_CO_THE  0.456070  ...     2019-12-13   \n",
       "435                                  NaN  0.078682  ...            NaT   \n",
       "\n",
       "     TotalStartDate8  TotalEndDate8  TotalStartDate9  TotalEndDate9  \\\n",
       "0         2018-02-01     2019-12-13       2018-02-01     2019-12-18   \n",
       "1         2018-01-31     2019-12-06       2018-02-01     2019-12-13   \n",
       "2         2017-12-11     2019-12-13       2018-02-01     2019-12-19   \n",
       "3         2018-01-31     2019-12-06       2018-02-01     2019-12-18   \n",
       "4         2018-02-01     2019-12-06       2018-01-31     2019-12-06   \n",
       "..               ...            ...              ...            ...   \n",
       "431              NaT            NaT              NaT            NaT   \n",
       "432       2016-11-14     2020-01-03       2016-11-14     2019-12-24   \n",
       "433       2017-12-01     2019-12-23       2017-12-01     2019-12-23   \n",
       "434       2017-12-11     2019-12-19       2017-12-11     2019-12-16   \n",
       "435              NaT            NaT              NaT            NaT   \n",
       "\n",
       "     LastStartDate  FirstEndDate TrainStartDate ValStartDate  ValEndDate  \n",
       "0       2018-02-01    2019-12-06     2018-02-01   2019-04-21  2019-10-07  \n",
       "1       2018-02-01    2019-12-06     2018-02-01   2019-04-21  2019-10-07  \n",
       "2       2018-02-01    2019-12-10     2018-02-01   2019-04-23  2019-10-11  \n",
       "3       2018-02-01    2019-12-06     2018-02-01   2019-04-21  2019-10-07  \n",
       "4       2018-02-01    2019-12-06     2018-02-01   2019-04-21  2019-10-07  \n",
       "..             ...           ...            ...          ...         ...  \n",
       "431     2018-02-02    2019-12-13     2018-02-02   2019-04-26  2019-10-14  \n",
       "432     2016-11-14    2019-12-24     2016-11-14   2018-12-03  2019-10-25  \n",
       "433     2017-12-11    2019-12-10     2017-12-11   2019-04-06  2019-10-11  \n",
       "434     2018-02-01    2019-12-09     2018-02-01   2019-04-23  2019-10-10  \n",
       "435     2017-12-11    2019-12-13     2017-12-11   2019-04-08  2019-10-14  \n",
       "\n",
       "[436 rows x 72 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_training_days = 90\n",
    "min_val_days = 60\n",
    "train_pct = .66 #percent of train+val\n",
    "max_val_date = '2019-09-01'\n",
    "num_secondary_val_days = 60\n",
    "\n",
    "combDF['TrainStartDate'], combDF['ValStartDate'], combDF['ValEndDate'] = np.nan, np.nan, np.nan\n",
    "\n",
    "min_num_days_total = min_training_days + min_val_days + num_secondary_val_days\n",
    "for i in combDF.index:\n",
    "    num_days = (pd.to_datetime(combDF.loc[i, 'FirstEndDate']) - pd.to_datetime(combDF.loc[i, 'LastStartDate'])).days\n",
    "    if num_days < min_num_days_total:\n",
    "        print('\\n'+'='*50+'num_days < min_num_days_total! Skipping row '+str(i)+':\\n'+str(combDF.loc[i]))\n",
    "    else:\n",
    "        combDF.loc[i, 'TrainStartDate'] = combDF.loc[i, 'LastStartDate']\n",
    "        combDF.loc[i, 'ValEndDate'] = combDF.loc[i, 'FirstEndDate'] - pd.Timedelta(days=num_secondary_val_days)\n",
    "        combDF.loc[i, 'ValStartDate'] = combDF.loc[i, 'TrainStartDate'] + pd.Timedelta(days=int(num_days*train_pct))        \n",
    "combDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Determine types of models to try </h1>\n",
    "<br>\n",
    "This entails choosing both the feature set and the type of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Timescales (one-value and PDF?)\n",
    "# - 1 min\n",
    "# - 3 min\n",
    "# - 5 min\n",
    "# - 10 min\n",
    "\n",
    "### Features\n",
    "# - Time of day\n",
    "# - Tick Imbalance\n",
    "# - last N minutes\n",
    "# - change on day\n",
    "# - change on hour\n",
    "# - change on 5 day\n",
    "# - relative prices BOD\n",
    "# - (optional: Dividends and Earnings)\n",
    "\n",
    "### Models \n",
    "# - Linear regression\n",
    "#   - with and without regularization\n",
    "# - Random Forest\n",
    "# - XGBoost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Create folder system and summary sheet </h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -US_Dollar_Index\n",
    "# --Config1\n",
    "# ---Data (temporary)\n",
    "# ---summary_sheet (human readable html?)\n",
    "# ---- == Results Summary ==\n",
    "# ---- == Configuration ==\n",
    "# ---- == Model Details ==\n",
    "# ---progress_notes (machine readable text)\n",
    "# ---- == Configuration ==\n",
    "# ---- == Model Iteration ==\n",
    "# ---log?\n",
    "# ---Plots\n",
    "# ---Deep_Models\n",
    "# ----model_name.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createConfigDir(comb_row, sec1_dir):\n",
    "    \"\"\"Creates the file structure AND the progress notes file and summary sheet file\"\"\"\n",
    "    print('Creating config file structure for:\\n'+sec1_dir+'\\n')\n",
    "    configs = [f for f in os.listdir(sec1_dir) if f[:6]=='Config']\n",
    "    config_nums = [f.split('Config')[1] for f in configs if f.split('Config')[1].isdigit()]\n",
    "    new_config_num = 1 if len(config_nums)==0 else max([int(i) for i in config_nums])+1\n",
    "    config_dir = sec1_dir+'Config'+str(new_config_num)+'/'\n",
    "    os.mkdir(config_dir)\n",
    "    data_dir = config_dir+'Data/'\n",
    "    os.mkdir(data_dir)\n",
    "    deep_models_dir = config_dir+'Deep_Models/'\n",
    "    os.mkdir(deep_models_dir)\n",
    "    plots_dir = config_dir+'Plots/'\n",
    "    os.mkdir(plots_dir)\n",
    "    # [x] log\n",
    "    log_fn = config_dir+'log.txt'\n",
    "    with open(log_fn, 'a+') as log_file:\n",
    "        log_file.write('='*50+'\\nCreated log file at '+getChiTimeNow().strftime(format='%Y-%m-%d %H:%M:%S %Z%z')+' (Chicago time)\\n'+'='*50+'\\n')\n",
    "    \n",
    "    # [x] progress_notes.txt\n",
    "    progress_notes_fn = config_dir+'progress_notes.txt'\n",
    "    other_secs_cols = [col for col in comb_row.index if (col[:3]=='Sec' and col != 'Sec1')]\n",
    "    other_secs = comb_row[other_secs_cols]\n",
    "    other_secs = [sec for sec in other_secs if type(sec)==str]\n",
    "    other_secs_idx = [i for i in range(2, len(other_secs)+2)]\n",
    "    with open(progress_notes_fn, 'a+') as progress_notes_file:\n",
    "        progress_notes_file.write('== Configuration ==\\n')\n",
    "        progress_notes_file.write('other_secs: '+','.join(other_secs)+'\\n')\n",
    "        progress_notes_file.write('TrainStartDate: '+comb_row.TrainStartDate.strftime(format='%Y-%m-%d')+'\\n')\n",
    "        progress_notes_file.write('ValStartDate: '+comb_row.ValStartDate.strftime(format='%Y-%m-%d')+'\\n')\n",
    "        progress_notes_file.write('ValEndDate: '+comb_row.ValEndDate.strftime(format='%Y-%m-%d')+'\\n')\n",
    "        progress_notes_file.write('== Model Iteration ==\\n')\n",
    "    # [x] summary_sheet.html\n",
    "    col_stems = ['TotalStartDate', 'TotalEndDate', 'R2_', 'p_value', 'StartDate', 'EndDate', 'NumDatapoints']\n",
    "    summaryDF = pd.DataFrame(index=['TotalStartDate', 'TotalEndDate', 'R2', 'p_value', 'StartDate', 'EndDate', 'NumDatapoints'])\n",
    "    for sec_idx in other_secs_idx:\n",
    "        summaryDF[comb_row['Sec'+str(sec_idx)]] = comb_row[[col+str(sec_idx) for col in col_stems]].values                                         \n",
    "    summary_sheet_fn = config_dir+'summary_sheet.html'\n",
    "    with open(summary_sheet_fn, 'a+') as summary_sheet_file:\n",
    "        summary_sheet_file.write('<h1>Results Summary</h1>')\n",
    "        summary_sheet_file.write('<h1>Configuration</h1>')\n",
    "        summary_sheet_file.write('<p>TrainStartDate: '+comb_row.TrainStartDate.strftime(format='%Y-%m-%d')+\n",
    "                                 '<br>ValStartDate: '+comb_row.ValStartDate.strftime(format='%Y-%m-%d')+\n",
    "                                 '<br>ValEndDate: '+comb_row.ValEndDate.strftime(format='%Y-%m-%d')+'</p>')\n",
    "        # [x] save config table\n",
    "        # [x] specify line_width in to_html below\n",
    "        summary_sheet_file.write(summaryDF.to_html(line_width=200))\n",
    "        summary_sheet_file.write('<h1>Model Details</h1>')\n",
    "    print('Config file structure creation complete.\\n')\n",
    "    return(config_dir, data_dir, summary_sheet_fn, progress_notes_fn, log_fn, deep_models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locateConfigDir(comb_row, omni_dir):\n",
    "    sec1_dir = omni_dir + comb_row.Sec1 +'/'\n",
    "    if not os.path.exists(sec1_dir):\n",
    "        print('Creating '+sec1_dir)\n",
    "        os.mkdir(sec1_dir)\n",
    "    config_dir = 'not set yet'\n",
    "    configs = [f for f in os.listdir(sec1_dir) if f[:6]=='Config']\n",
    "    # search for the appropriate configuration\n",
    "    configs.sort()\n",
    "    for config in configs:\n",
    "        temp_notes_fn = sec1_dir+config+'/progress_notes.txt'\n",
    "        if not os.path.exists(temp_notes_fn):\n",
    "            print(temp_notes_fn+' doesnt exist!')\n",
    "        else:\n",
    "            temp_notes = 'not set yet'\n",
    "            with open(temp_notes_fn,'r') as fh:\n",
    "                temp_notes = fh.read()\n",
    "            config_section = temp_notes.split('== Configuration ==\\n')[1].split('\\n== Model Iteration ==')[0]\n",
    "            # [] TODO: Identify whether this is the correct configuration\n",
    "            train_start_date = config_section.split('TrainStartDate: ')[1].split('\\n')[0]\n",
    "            val_start_date = config_section.split('ValStartDate: ')[1].split('\\n')[0]\n",
    "            val_end_date = config_section.split('ValEndDate: ')[1].split('\\n')[0]\n",
    "            other_secs = config_section.split('other_secs: ')[1].split('\\n')[0].split(',')\n",
    "            comb_row_other_secs_cols = [col for col in comb_row.index if (col[:3]=='Sec' and col != 'Sec1')]\n",
    "            comb_row_other_secs = comb_row[comb_row_other_secs_cols]\n",
    "            comb_row_other_secs = [sec for sec in comb_row_other_secs if type(sec)==str]\n",
    "            if ((comb_row.TrainStartDate.strftime(format='%Y-%m-%d') == train_start_date) and \n",
    "                (comb_row.ValStartDate.strftime(format='%Y-%m-%d') == val_start_date) and \n",
    "                (comb_row.ValEndDate.strftime(format='%Y-%m-%d') == val_end_date) and \n",
    "                (set(comb_row_other_secs) == set(other_secs))):\n",
    "                config_dir = sec1_dir+config+'/'\n",
    "                print('Existing matching config dir found: '+sec1_dir+config)\n",
    "                return(config_dir, config_dir+'Data/', config_dir+'summary_sheet.html', config_dir+'progress_notes.txt', config_dir+'log.txt', config_dir+'Data/')               \n",
    "    return(createConfigDir(comb_row, sec1_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Iterate Learning </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Format/Process Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getExpectedPreprocessedYCols(y_min_incs):\n",
    "    return_cols = ['Minute']\n",
    "    for t in y_min_incs:\n",
    "        return_cols.append('y_B'+str(t))\n",
    "        return_cols.append('y_A'+str(t))\n",
    "    return(return_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_secs = 9\n",
    "day_chg_incs = [1, 3, 5]\n",
    "minute_incs = [5, 15, 30, 60]\n",
    "preprocessed_data_dir = '/home/andrew/All_Trading/Studies/Omni_Project/US_Dollar_Index/Config1/Preprocessed_Data/'\n",
    "X_cols_set_actual =  set(pd.read_csv(preprocessed_data_dir+'train_minutesDF_X.csv', nrows=0).columns.tolist())\n",
    "X_cols_set_expected = set(getExpectedPreprocessedXCols(num_secs, day_chg_incs, minute_incs))\n",
    "X_cols_set_actual.difference(X_cols_set_expected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getExpectedPreprocessedXCols(num_secs, day_chg_incs, minute_incs):\n",
    "    # there is probably a more elegant way to do this\n",
    "    return_cols = ['Minute']\n",
    "    # outer loop is always SEC for [DAY], [MINUTE] is always the outer loop for [SEC]. ...I think\n",
    "    # [SEC1] means starts at sec1, [SEC2] means starts at sec2\n",
    "    chunk1 = ['O_B[SEC1]', 'O_A[SEC1]', 'H_B[SEC1]', 'H_A[SEC1]', 'L_B[SEC1]', 'L_A[SEC1]', 'C_B[SEC1]', 'C_A[SEC1]',\n",
    "              'Count[SEC1]', 'B_TickImb[SEC1]', 'A_TickImb[SEC1]', 'M_TickImb[SEC1]']\n",
    "    chunk2 = ['Sec[SEC1]_Open_B', 'Sec[SEC1]_Open_A', \n",
    "              'Sec[SEC1]_Open_B_chg[DAY]', 'Sec[SEC1]_Open_A_chg[DAY]']\n",
    "    chunk3 = ['Sec[SEC2]_Open_B_Quotient', 'Sec[SEC2]_Open_A_Quotient']\n",
    "    chunk4 = ['O_B[SEC1]_diff[MINUTE]', 'O_A[SEC1]_diff[MINUTE]', 'H_B[SEC1]_diff[MINUTE]', 'H_A[SEC1]_diff[MINUTE]', 'L_B[SEC1]_diff[MINUTE]', 'L_A[SEC1]_diff[MINUTE]', 'C_B[SEC1]_diff[MINUTE]', 'C_A[SEC1]_diff[MINUTE]']\n",
    "    chunk5 = ['Count[SEC1]_sum[MINUTE]', 'B_TickImb[SEC1]_sum[MINUTE]', 'A_TickImb[SEC1]_sum[MINUTE]', 'M_TickImb[SEC1]_sum[MINUTE]']\n",
    "    chunks = [chunk1, chunk2, chunk3, chunk4, chunk5]\n",
    "    for chunk in chunks:\n",
    "        out_chunk = []\n",
    "        for t_col in chunk:\n",
    "            if ('[SEC1]' in t_col) and ('[DAY]' in t_col):\n",
    "                for s in range(1, (num_secs+1)):\n",
    "                    for d in day_chg_incs:\n",
    "                        out_chunk.append(t_col.replace('[SEC1]', str(s)).replace('[DAY]', str(d)))\n",
    "            elif ('[SEC1]' in t_col) and ('[MINUTE]' in t_col):\n",
    "                for s in range(1, (num_secs+1)):\n",
    "                    for m in minute_incs:\n",
    "                        out_chunk.append(t_col.replace('[SEC1]', str(s)).replace('[MINUTE]', str(m)))\n",
    "            elif ('[SEC2]' in t_col) and ('[DAY]' in t_col):\n",
    "                for s in range(2, (num_secs+1)):\n",
    "                    for d in day_chg_incs:\n",
    "                        out_chunk.append(t_col.replace('[SEC2]', str(s)).replace('[DAY]', str(d)))\n",
    "            elif ('[SEC2]' in t_col) and ('[MINUTE]' in t_col):\n",
    "                for s in range(2, (num_secs+1)):\n",
    "                    for m in minute_incs:\n",
    "                        out_chunk.append(t_col.replace('[SEC2]', str(s)).replace('[MINUTE]', str(m)))\n",
    "            elif '[SEC1]' in t_col:\n",
    "                for s in range(1, (num_secs+1)):\n",
    "                    out_chunk.append(t_col.replace('[SEC1]', str(s)))\n",
    "            elif '[SEC2]' in t_col:\n",
    "                for s in range(2, (num_secs+1)):\n",
    "                    out_chunk.append(t_col.replace('[SEC2]', str(s)))\n",
    "            else:\n",
    "                raise ValueError('Unrecognized template col: '+t_col)\n",
    "        return_cols += out_chunk\n",
    "    return(return_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date,Minute,O_B1,O_A1,H_B1,H_A1,L_B1,L_A1,C_B1,C_A1,Count1,B_TickImb1,A_TickImb1,M_TickImb1,O_B2,O_A2,H_B2,H_A2,L_B2,L_A2,C_B2,C_A2,Count2,B_TickImb2,A_TickImb2,M_TickImb2,O_B3,O_A3,H_B3,H_A3,L_B3,L_A3,C_B3,C_A3,Count3,B_TickImb3,A_TickImb3,M_TickImb3,O_B4,O_A4,H_B4,H_A4,L_B4,L_A4,C_B4,C_A4,Count4,B_TickImb4,A_TickImb4,M_TickImb4,O_B5,O_A5,H_B5,H_A5,L_B5,L_A5,C_B5,C_A5,Count5,B_TickImb5,A_TickImb5,M_TickImb5,O_B6,O_A6,H_B6,H_A6,L_B6,L_A6,C_B6,C_A6,Count6,B_TickImb6,A_TickImb6,M_TickImb6,O_B7,O_A7,H_B7,H_A7,L_B7,L_A7,C_B7,C_A7,Count7,B_TickImb7,A_TickImb7,M_TickImb7,O_B8,O_A8,H_B8,H_A8,L_B8,L_A8,C_B8,C_A8,Count8,B_TickImb8,A_TickImb8,M_TickImb8,O_B9,O_A9,H_B9,H_A9,L_B9,L_A9,C_B9,C_A9,Count9,B_TickImb9,A_TickImb9,M_TickImb9\r\n",
      "2017-12-11,2017-12-11 00:00:00,92.814,92.99600000000001,92.82600000000001,93.03,92.78399999999999,92.97,92.82600000000001,92.99600000000001,9.0,2.0,2.0,2.0,1.17675,1.17677,1.17692,1.17696,1.17674,1.17675,1.17691,1.17694,63.0,6.0,9.0,10.0,0.99294,0.99308,0.993,0.99312,0.99292,0.99301,0.99292,0.99302,24.0,-2.0,-3.0,-5.0,0.75066,0.75076,0.75073,0.7508199999999999,0.75066,0.75076,0.75067,0.7507699999999999,26.0,-3.0,1.0,-3.0,0.68409,0.6842,0.68416,0.68425,0.68408,0.6842,0.68415,0.68425,48.0,3.0,4.0,3.0,8.45996,8.46652,8.46038,8.46739,8.45838,8.46316,8.45882,8.46316,89.0,11.0,-13.0,-2.0,113.561,113.564,113.56299999999999,113.56700000000001,113.52,113.525,113.521,113.52600000000001,75.0,-20.0,-16.0,-21.0,1.339,1.33915,1.33936,1.33949,1.339,1.33915,1.33933,1.33946,36.0,0.0,5.0,2.0,1.2861200000000002,1.28624,1.2861200000000002,1.28625,1.286,1.28611,1.28607,1.28611,37.0,-3.0,-4.0,-2.0\r\n",
      "2017-12-11,2017-12-11 00:01:00,92.8,93.01700000000001,92.825,93.024,92.773,92.971,92.811,93.005,11.0,-1.0,1.0,-3.0,1.17691,1.17695,1.17692,1.17697,1.17687,1.1769,1.17692,1.17694,32.0,0.0,1.0,0.0,0.99292,0.99304,0.99293,0.99306,0.99292,0.99304,0.99293,0.99305,3.0,1.0,1.0,2.0,0.75069,0.75079,0.75074,0.75083,0.75069,0.75079,0.75071,0.7508199999999999,19.0,1.0,1.0,-2.0,0.68415,0.68425,0.68415,0.68425,0.68402,0.68413,0.6841,0.6842,38.0,-2.0,-4.0,-5.0,8.45907,8.46284,8.45907,8.46284,8.457930000000001,8.46214,8.457939999999999,8.46232,8.0,-3.0,-4.0,-6.0,113.521,113.52600000000001,113.527,113.531,113.51700000000001,113.521,113.525,113.529,48.0,4.0,5.0,3.0,1.33933,1.33946,1.33934,1.33946,1.3390799999999998,1.33922,1.33918,1.33922,38.0,-6.0,-6.0,-9.0,1.28606,1.28611,1.28607,1.28614,1.28601,1.28611,1.28601,1.28614,15.0,-3.0,2.0,0.0\r\n",
      "2017-12-11,2017-12-11 00:02:00,92.76799999999999,92.975,92.819,93.023,92.734,92.958,92.772,92.973,12.0,0.0,0.0,-4.0,1.17693,1.17696,1.17703,1.17706,1.17692,1.17696,1.17697,1.1770200000000002,36.0,1.0,7.0,4.0,0.99293,0.99304,0.99293,0.99304,0.99281,0.99295,0.99283,0.99297,6.0,-3.0,-3.0,-4.0,0.75071,0.7508100000000001,0.75076,0.75088,0.75071,0.7508100000000001,0.75076,0.75088,13.0,3.0,2.0,5.0,0.6841,0.6842,0.68416,0.6842600000000001,0.68408,0.6842,0.6841,0.68423,22.0,-2.0,-1.0,-3.0,8.45769,8.46185,8.45769,8.46185,8.45528,8.4601,8.45625,8.46055,27.0,-4.0,-2.0,-1.0,113.524,113.52799999999999,113.525,113.529,113.509,113.51299999999999,113.515,113.51799999999999,56.0,-3.0,-4.0,-9.0,1.33916,1.33922,1.3395,1.3396,1.33914,1.33922,1.33946,1.33958,59.0,11.0,9.0,11.0,1.28605,1.28613,1.28606,1.28615,1.28594,1.28603,1.28594,1.28607,30.0,-4.0,-2.0,-5.0\r\n",
      "2017-12-11,2017-12-11 00:03:00,92.792,92.962,92.79799999999999,92.99600000000001,92.76299999999999,92.962,92.773,92.99600000000001,4.0,0.0,0.0,2.0,1.17697,1.17703,1.17698,1.17703,1.17682,1.17685,1.17682,1.17685,33.0,-7.0,-10.0,-15.0,0.99283,0.99297,0.99295,0.99307,0.99283,0.99296,0.99292,0.99307,14.0,4.0,4.0,5.0,0.7507699999999999,0.75088,0.7507699999999999,0.75088,0.75067,0.7507699999999999,0.75067,0.7507699999999999,9.0,-4.0,-4.0,-7.0,0.68411,0.6842199999999999,0.68411,0.68424,0.684,0.68413,0.68406,0.68419,19.0,-3.0,-4.0,-4.0,8.45621,8.46055,8.4571,8.46143,8.45621,8.460360000000001,8.4571,8.46143,6.0,2.0,1.0,4.0,113.515,113.51700000000001,113.52,113.524,113.507,113.512,113.52,113.524,52.0,1.0,4.0,4.0,1.33945,1.33956,1.33945,1.33959,1.33932,1.33946,1.33932,1.3394700000000002,26.0,-10.0,-2.0,-10.0,1.28593,1.28607,1.28607,1.28617,1.28593,1.28607,1.28605,1.28617,27.0,5.0,3.0,5.0\r\n",
      "2017-12-11,2017-12-11 00:04:00,92.792,92.995,92.831,93.025,92.76899999999999,92.975,92.777,93.00299999999999,7.0,3.0,-3.0,1.0,1.17682,1.17687,1.17682,1.17687,1.17676,1.1768,1.17676,1.1768,17.0,-4.0,-2.0,-1.0,0.99293,0.99307,0.99295,0.99307,0.99292,0.99306,0.99293,0.99306,7.0,0.0,-1.0,-1.0,0.75068,0.7507699999999999,0.75068,0.7507699999999999,0.75062,0.75073,0.75065,0.75075,7.0,0.0,-2.0,-1.0,0.68405,0.68416,0.68409,0.68421,0.68399,0.68412,0.68399,0.68412,22.0,-8.0,-4.0,-8.0,8.457,8.46181,8.45778,8.46182,8.457,8.46136,8.45766,8.46146,10.0,1.0,1.0,2.0,113.51899999999999,113.524,113.527,113.531,113.514,113.51700000000001,113.522,113.52600000000001,49.0,-1.0,5.0,-1.0,1.33935,1.3394700000000002,1.3394,1.33952,1.33934,1.33946,1.33938,1.3395,25.0,0.0,5.0,4.0,1.28606,1.28617,1.28607,1.28619,1.28605,1.28617,1.28607,1.28619,8.0,1.0,1.0,2.0\r\n",
      "2017-12-11,2017-12-11 00:05:00,92.809,93.00299999999999,92.82,93.03,92.759,92.95299999999999,92.762,92.963,12.0,0.0,0.0,-2.0,1.17676,1.17679,1.17694,1.17699,1.17676,1.17679,1.17691,1.17694,57.0,3.0,11.0,9.0,0.99291,0.99303,0.99291,0.99304,0.99279,0.99293,0.99286,0.993,21.0,-3.0,-4.0,-5.0,0.75066,0.7507699999999999,0.75069,0.75078,0.75064,0.75076,0.75064,0.7507699999999999,18.0,0.0,1.0,0.0,0.68399,0.68412,0.6841,0.68419,0.68399,0.68412,0.68407,0.68419,40.0,-2.0,4.0,-1.0,8.4573,8.46146,8.457310000000001,8.46146,8.45622,8.46038,8.45622,8.46038,16.0,-3.0,-2.0,-6.0,113.51899999999999,113.521,113.51899999999999,113.521,113.471,113.475,113.485,113.48899999999999,90.0,-9.0,-4.0,-9.0,1.33939,1.3395,1.3394700000000002,1.33957,1.33938,1.33949,1.33944,1.33955,32.0,3.0,0.0,2.0,1.28607,1.2862,1.28609,1.2862,1.286,1.28614,1.28601,1.28615,42.0,-3.0,-4.0,1.0\r\n",
      "2017-12-11,2017-12-11 00:06:00,92.79899999999999,92.993,92.809,93.01,92.766,92.946,92.766,93.009,9.0,-1.0,-1.0,1.0,1.17691,1.17693,1.17693,1.17696,1.17685,1.1769,1.17693,1.17696,60.0,1.0,-1.0,0.0,0.99284,0.99298,0.99288,0.99301,0.99284,0.99298,0.99285,0.99298,16.0,0.0,-1.0,-2.0,0.75064,0.75075,0.75064,0.7507699999999999,0.75063,0.75075,0.75064,0.75075,10.0,0.0,0.0,0.0,0.68406,0.68419,0.68409,0.6842,0.68406,0.68418,0.68408,0.68419,21.0,-1.0,0.0,0.0,8.45658,8.46074,8.45694,8.46074,8.45622,8.46037,8.45622,8.46038,5.0,1.0,1.0,-1.0,113.485,113.48899999999999,113.494,113.49799999999999,113.475,113.479,113.478,113.48200000000001,67.0,-1.0,-6.0,-2.0,1.33944,1.33955,1.33945,1.33955,1.33932,1.33942,1.33939,1.33951,42.0,-1.0,3.0,-2.0,1.28601,1.28614,1.28606,1.28616,1.286,1.28614,1.28601,1.28614,26.0,-4.0,-1.0,-5.0\r\n",
      "2017-12-11,2017-12-11 00:07:00,92.79,93.01299999999999,92.82700000000001,93.01299999999999,92.765,92.95,92.803,92.95700000000001,20.0,-2.0,4.0,0.0,1.17693,1.17696,1.17693,1.17698,1.17682,1.17687,1.17688,1.17692,48.0,-3.0,4.0,2.0,0.99285,0.99296,0.99301,0.99314,0.9928,0.99293,0.99301,0.99313,30.0,6.0,5.0,6.0,0.75065,0.75075,0.75073,0.75083,0.75063,0.75075,0.75073,0.75083,32.0,0.0,2.0,2.0,0.68407,0.68419,0.6842199999999999,0.68434,0.68403,0.68414,0.68418,0.6843,48.0,-1.0,8.0,0.0,8.45658,8.4611,8.45692,8.4611,8.45622,8.46033,8.456560000000001,8.460339999999999,16.0,2.0,-2.0,-1.0,113.479,113.48200000000001,113.49600000000001,113.5,113.478,113.48100000000001,113.48700000000001,113.491,74.0,15.0,15.0,19.0,1.3394,1.3395,1.33943,1.33951,1.33932,1.33943,1.3394,1.33951,54.0,1.0,4.0,4.0,1.28601,1.28615,1.28608,1.2862,1.28601,1.2861200000000002,1.28601,1.28615,43.0,-2.0,-3.0,-3.0\r\n",
      "2017-12-11,2017-12-11 00:08:00,92.773,92.98200000000001,92.77799999999999,92.99799999999999,92.75299999999999,92.948,92.76100000000001,92.948,5.0,-1.0,-1.0,-4.0,1.17688,1.17692,1.17692,1.17697,1.17685,1.1769,1.17688,1.17692,24.0,-3.0,-1.0,-5.0,0.99302,0.99313,0.99302,0.99313,0.99289,0.99304,0.99298,0.99311,17.0,1.0,-3.0,-1.0,0.75073,0.7508199999999999,0.75083,0.75092,0.75073,0.7508199999999999,0.75078,0.75088,17.0,1.0,1.0,-1.0,0.68419,0.6843,0.68425,0.68435,0.68416,0.68428,0.6842,0.68433,26.0,-2.0,-1.0,-5.0,8.45692,8.460339999999999,8.45716,8.46048,8.456439999999999,8.46012,8.4568,8.46048,7.0,1.0,1.0,1.0,113.486,113.491,113.491,113.495,113.473,113.478,113.479,113.484,42.0,-5.0,-4.0,-6.0,1.33942,1.33951,1.33943,1.33953,1.3394,1.3395,1.33942,1.33952,62.0,2.0,2.0,2.0,1.28601,1.28614,1.2860200000000002,1.28616,1.28594,1.28605,1.28594,1.28607,23.0,-3.0,-2.0,-4.0\r\n"
     ]
    }
   ],
   "source": [
    "! head /home/andrew/All_Trading/Studies/Omni_Project/Primary_Assets/US_Dollar_Index/Config1/Data/all_minutes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Features\n",
    "# [x] - Time of day\n",
    "# [NA] - Tick Imbalance\n",
    "# [x] - last [5, 15, 30, 60] minutes\n",
    "# [NA] - change on day\n",
    "# [x] - change on [1,3,5] day\n",
    "# [x] - relative prices BOD\n",
    "\n",
    "### Timescales (one-value and PDF?)\n",
    "# - 1 min\n",
    "# - 3 min\n",
    "# - 5 min\n",
    "# - 10 min\n",
    "\n",
    "def processData(all_minutesDF, dailyDF, sec_guideDF, val_start_date, config_dir, saveProcessed=False):\n",
    "    saveProcessed = True\n",
    "    day_chg_incs = [1, 3, 5]\n",
    "    minute_incs = [5, 15, 30, 60]\n",
    "    y_min_incs = [1, 3, 5, 10]\n",
    "    num_secs = len(sec_guideDF) \n",
    "    print('In processData. Checking for preprocessedData...') #might want to move this upstream depending on speed\n",
    "    #look through preprocessed data folder\n",
    "    preprocessed_data_dir = config_dir + 'Preprocessed_Data/'\n",
    "    if os.path.exists(preprocessed_data_dir):\n",
    "        if len(os.listdir(preprocessed_data_dir)) >= 4:\n",
    "            print('preprocessedData found. Checking columns and returning')\n",
    "            # need to check the columns\n",
    "            y_cols_expected = getExpectedPreprocessedYCols(y_min_incs)\n",
    "            y_cols_actual =  pd.read_csv(preprocessed_data_dir+'train_minutesDF_y.csv', nrows=0).columns.tolist()\n",
    "            assert y_cols_expected == y_cols_actual\n",
    "            # check that the X_col sets are the same. Assume the ordering is correct.\n",
    "            X_cols_set_expected = set(getExpectedPreprocessedXCols(num_secs, day_chg_incs, minute_incs))\n",
    "            X_cols_set_actual =  set(pd.read_csv(preprocessed_data_dir+'train_minutesDF_X.csv', nrows=0).columns.tolist())\n",
    "            assert X_cols_set_expected == X_cols_set_actual, str(X_cols_set_expected.difference(X_cols_set_actual))+'\\n'+str(X_cols_set_actual.difference(X_cols_set_expected))\n",
    "            print('loading train_minutesDF_X...')\n",
    "            train_minutesDF_X = pd.read_csv(preprocessed_data_dir+'train_minutesDF_X.csv', parse_dates=['Minute'])\n",
    "            print('loading train_minutesDF_y...')\n",
    "            train_minutesDF_y = pd.read_csv(preprocessed_data_dir+'train_minutesDF_y.csv', parse_dates=['Minute'])\n",
    "            print('loading val_minutesDF_X...')\n",
    "            val_minutesDF_X = pd.read_csv(preprocessed_data_dir+'val_minutesDF_X.csv', parse_dates=['Minute'])\n",
    "            print('loading val_minutesDF_y...')\n",
    "            val_minutesDF_y = pd.read_csv(preprocessed_data_dir+'val_minutesDF_y.csv', parse_dates=['Minute'])\n",
    "            return(train_minutesDF_X, train_minutesDF_y, val_minutesDF_X, val_minutesDF_y)\n",
    "    print('Preprocessed data not found. Processing data from all_minutesDF...')\n",
    "    assert(np.all(all_minutesDF.index == range(len(all_minutesDF))))\n",
    "    # [x] - Time of day\n",
    "    all_minutesDF.TimeNumeric = all_minutesDF.Minute.dt.hour*60+all_minutesDF.Minute.dt.minute\n",
    "    # [x] fill in the NAs\n",
    "    print('filling in NAs...')\n",
    "    ffill_col_stems = ['C_B', 'C_A']\n",
    "    take_filled_close_col_stems = ['O_B', 'O_A', 'H_B', 'H_A', 'L_B', 'L_A']\n",
    "    zerofill_col_stems = ['Count', 'B_TickImb', 'A_TickImb', 'M_TickImb']\n",
    "    for sec_num in range(1, num_secs+1):\n",
    "        ffill_cols = [s+str(sec_num) for s in ffill_col_stems]\n",
    "        take_filled_close_cols = [s+str(sec_num) for s in take_filled_close_col_stems]\n",
    "        zerofill_cols = [s+str(sec_num) for s in zerofill_col_stems]\n",
    "        all_minutesDF.loc[:,ffill_cols] = all_minutesDF.loc[:,ffill_cols].ffill()\n",
    "        all_minutesDF.loc[:,zerofill_cols] = all_minutesDF.loc[:,zerofill_cols].fillna(0)\n",
    "        B_take_filled_close_cols = [col for col in take_filled_close_cols if '_B' in col]\n",
    "        A_take_filled_close_cols = [col for col in take_filled_close_cols if '_A' in col]\n",
    "        # use fillna below to achieve successful broadcasting of C_B# and C_A# columns\n",
    "        all_minutesDF.loc[all_minutesDF[B_take_filled_close_cols[0]].isna(), B_take_filled_close_cols] = \\\n",
    "            all_minutesDF.loc[all_minutesDF[B_take_filled_close_cols[0]].isna(), B_take_filled_close_cols].fillna(0).add(\n",
    "                all_minutesDF.loc[all_minutesDF[B_take_filled_close_cols[0]].isna(), 'C_B'+str(sec_num)], axis=0)\n",
    "        all_minutesDF.loc[all_minutesDF[A_take_filled_close_cols[0]].isna(), A_take_filled_close_cols] = \\\n",
    "            all_minutesDF.loc[all_minutesDF[A_take_filled_close_cols[0]].isna(), A_take_filled_close_cols].fillna(0).add(\n",
    "                all_minutesDF.loc[all_minutesDF[A_take_filled_close_cols[0]].isna(), 'C_A'+str(sec_num)], axis=0)\n",
    "    # [x] - change on [1,3,5] day\n",
    "    print('calculating change on '+str(day_chg_incs)+' days...')\n",
    "    dailyDF['OpenDT'] = pd.to_datetime(dailyDF.Date.astype(str)+' '+dailyDF.Open, format='%Y-%m-%d %H:%M')\n",
    "    dailyDF['CloseDT'] = pd.to_datetime(dailyDF.Date.astype(str)+' '+dailyDF.Close, format='%Y-%m-%d %H:%M')\n",
    "    for sec_num in range(1, num_secs+1):\n",
    "        dailyDF['Sec'+str(sec_num)+'_Open_B'], dailyDF['Sec'+str(sec_num)+'_Open_A'] = np.nan, np.nan\n",
    "        dailyDF[['Sec'+str(sec_num)+'_Open_B', 'Sec'+str(sec_num)+'_Open_A']] = \\\n",
    "            pd.merge(dailyDF[['OpenDT']], all_minutesDF[['Minute', 'C_B'+str(sec_num), 'C_A'+str(sec_num)]],\n",
    "                     left_on=['OpenDT'], right_on=['Minute'], how='left')[['C_B'+str(sec_num), 'C_A'+str(sec_num)]]\n",
    "        for day_inc in day_chg_incs:\n",
    "            # today's open is the reference point for change on X day\n",
    "            dailyDF['Sec'+str(sec_num)+'_Open_B_chg'+str(day_inc)] = \\\n",
    "                dailyDF['Sec'+str(sec_num)+'_Open_B'].shift(day_inc) - dailyDF['Sec'+str(sec_num)+'_Open_B']\n",
    "            dailyDF['Sec'+str(sec_num)+'_Open_A_chg'+str(day_inc)] = \\\n",
    "                dailyDF['Sec'+str(sec_num)+'_Open_A'].shift(day_inc) - dailyDF['Sec'+str(sec_num)+'_Open_A']\n",
    "    # [x] - relative prices BOD (quotient)\n",
    "    for sec_num in range(2, num_secs+1):\n",
    "        dailyDF['Sec'+str(sec_num)+'_Open_B_Quotient'] = dailyDF['Sec'+str(sec_num)+'_Open_B']/dailyDF['Sec1_Open_B']\n",
    "        dailyDF['Sec'+str(sec_num)+'_Open_A_Quotient'] = dailyDF['Sec'+str(sec_num)+'_Open_A']/dailyDF['Sec1_Open_A']\n",
    "    new_cols = [col for col in dailyDF.columns if col not in ['Date', 'Open', 'Close', 'OpenDT', 'CloseDT', 'Unnamed: 0']]\n",
    "    for col in new_cols: all_minutesDF[col] = np.nan\n",
    "    all_minutesDF[new_cols] = pd.merge(all_minutesDF[['Date']], dailyDF[['Date']+new_cols],\n",
    "                                       on='Date', how='left')[new_cols]\n",
    "    # [x] make all prices relative to BOD\n",
    "    print('making all prices relative to BOD...')\n",
    "    col_stems_to_make_relative = ['O_B', 'O_A', 'H_B', 'H_A', 'L_B', 'L_A', 'C_B', 'C_A']\n",
    "    for sec_num in range(1, num_secs+1):\n",
    "        B_cols_to_make_relative = [s+str(sec_num) for s in col_stems_to_make_relative if '_B' in s]\n",
    "        A_cols_to_make_relative = [s+str(sec_num) for s in col_stems_to_make_relative if '_A' in s]\n",
    "        all_minutesDF[B_cols_to_make_relative] = all_minutesDF[B_cols_to_make_relative].subtract(\n",
    "            pd.merge(all_minutesDF[['Date']], dailyDF[['Date', 'Sec'+str(sec_num)+'_Open_B']],\n",
    "                     on='Date', how='left')['Sec'+str(sec_num)+'_Open_B'], axis=0)\n",
    "        all_minutesDF[A_cols_to_make_relative] = all_minutesDF[A_cols_to_make_relative].subtract(\n",
    "            pd.merge(all_minutesDF[['Date']], dailyDF[['Date', 'Sec'+str(sec_num)+'_Open_A']],\n",
    "                     on='Date', how='left')['Sec'+str(sec_num)+'_Open_A'], axis=0)\n",
    "    # [x] - last [5, 15, 30, 60] minutes\n",
    "    print('getting data for last '+str(minute_incs)+' minutes')\n",
    "    col_stems_to_add = ['Count', 'B_TickImb', 'A_TickImb', 'M_TickImb']\n",
    "    col_stems_to_diff = ['O_B', 'O_A', 'H_B', 'H_A', 'L_B', 'L_A', 'C_B', 'C_A']\n",
    "    cols_to_add, cols_to_diff = [], [] \n",
    "    for sec_num in range(1, num_secs+1):\n",
    "        cols_to_add += [s+str(sec_num) for s in col_stems_to_add]\n",
    "        cols_to_diff += [s+str(sec_num) for s in col_stems_to_diff]\n",
    "    new_cols_add, new_cols_diff = [], []\n",
    "    for min_inc in minute_incs:\n",
    "        new_cols_add += [col+'_sum'+str(min_inc) for col in cols_to_add]\n",
    "        new_cols_diff += [col+'_diff'+str(min_inc) for col in cols_to_diff]\n",
    "    # diff the diff columns, then fill in the first min_inc rows of each date with the BOD value\n",
    "    print('creating minute diff cols...')\n",
    "    for min_inc in minute_incs:\n",
    "        all_minutesDF[[col+'_diff'+str(min_inc) for col in cols_to_diff]] = all_minutesDF[cols_to_diff].shift(min_inc)\n",
    "        for date in all_minutesDF.Date.unique():\n",
    "            date_subDF = all_minutesDF.loc[all_minutesDF.Date == date]\n",
    "            repl_subDF = date_subDF.iloc[:min_inc]\n",
    "            all_minutesDF.loc[repl_subDF.index, [col+'_diff'+str(min_inc) for col in cols_to_diff]] = \\\n",
    "                repl_subDF[cols_to_diff].iloc[0].values\n",
    "    # add the add columns, making sure not to go past the current date   \n",
    "    print('creating minute add cols...')\n",
    "    for min_inc in minute_incs:\n",
    "        for col in cols_to_add: all_minutesDF[col+'_sum'+str(min_inc)] = np.nan\n",
    "    for date in all_minutesDF.Date.unique():\n",
    "        date_subDF = all_minutesDF.loc[all_minutesDF.Date == date]\n",
    "        for min_inc in minute_incs:\n",
    "            all_minutesDF.loc[date_subDF.index, [col+'_sum'+str(min_inc) for col in cols_to_add]] = \\\n",
    "                date_subDF[cols_to_add].rolling(min_inc, min_periods=1).sum().values     \n",
    "    # [x] create y columns\n",
    "    print('creating y columns for future '+str(y_min_incs)+' minutes...')\n",
    "    for y_min_inc in y_min_incs:\n",
    "        all_minutesDF['y_B'+str(y_min_inc)] = np.nan\n",
    "        all_minutesDF['y_A'+str(y_min_inc)] = np.nan\n",
    "        for date in all_minutesDF.Date.unique():\n",
    "            date_subDF = all_minutesDF.loc[all_minutesDF.Date == date]\n",
    "            all_minutesDF.loc[date_subDF.index, 'y_B'+str(y_min_inc)] = date_subDF.C_B1.shift(-y_min_inc)\n",
    "            all_minutesDF.loc[date_subDF.index, 'y_A'+str(y_min_inc)] = date_subDF.C_A1.shift(-y_min_inc)\n",
    "            # fill NAs at the end of the day with the latest available data that day\n",
    "            date_subDF = all_minutesDF.loc[all_minutesDF.Date == date] # need this for the line below\n",
    "            na_subDF = date_subDF.loc[date_subDF['y_B'+str(y_min_inc)].isna()]\n",
    "            all_minutesDF.loc[na_subDF.index, ['y_B'+str(y_min_inc), 'y_A'+str(y_min_inc)]] = na_subDF[['C_B1', 'C_A1']].iloc[-1].values\n",
    "        # make y columns relative to current close\n",
    "        all_minutesDF['y_B'+str(y_min_inc)] = all_minutesDF['y_B'+str(y_min_inc)] - all_minutesDF.C_B1\n",
    "        all_minutesDF['y_A'+str(y_min_inc)] = all_minutesDF['y_A'+str(y_min_inc)] - all_minutesDF.C_A1\n",
    "    # [x] subset data and return\n",
    "    print('subsetting data and returning...')\n",
    "    days_to_delete = max(day_chg_incs)\n",
    "    train_start_date = all_minutesDF.Date.unique()[days_to_delete]\n",
    "    train_end_date = val_start_date\n",
    "    # add val_start_date into all_dates, sort, find the index, then remove the addition\n",
    "    all_dates = list(all_minutesDF.Date.unique())\n",
    "    all_dates.append(val_start_date)\n",
    "    all_dates.sort()\n",
    "    val_start_date_idx = all_dates.index(val_start_date)\n",
    "    all_dates.pop(val_start_date_idx)\n",
    "    assert(val_start_date_idx > days_to_delete)\n",
    "    val_start_date = all_minutesDF.Date.unique()[val_start_date_idx+days_to_delete]\n",
    "    train_minutesDF = all_minutesDF.loc[(all_minutesDF.Date >= train_start_date) & (all_minutesDF.Date <= train_end_date)].reset_index(drop=True)\n",
    "    val_minutesDF = all_minutesDF.loc[(all_minutesDF.Date >= val_start_date)].reset_index(drop=True)\n",
    "    y_cols = ['y_B'+str(y_min_inc) for y_min_inc in y_min_incs] + ['y_A'+str(y_min_inc) for y_min_inc in y_min_incs]\n",
    "    y_cols.sort(key=lambda x: int(x[3:])) #puts them in y_min_inc order\n",
    "    unused_cols = ['Unnamed: 0', 'Product', 'Date', 'Minute', 'First', 'Last']\n",
    "    X_cols = [col for col in all_minutesDF.columns if (col not in y_cols+unused_cols)]\n",
    "    # UNCOMMENT BELOW!!\n",
    "    assert(np.all(~train_minutesDF[X_cols].isna()))\n",
    "    assert(np.all(~val_minutesDF[X_cols].isna()))\n",
    "    assert(np.all(~train_minutesDF[y_cols].isna()))\n",
    "    assert(np.all(~val_minutesDF[y_cols].isna()))\n",
    "    X_cols, y_cols = ['Minute']+list(X_cols), ['Minute']+list(y_cols)# We still need the minutes\n",
    "    if saveProcessed:\n",
    "        print('Saving processed data')\n",
    "        if not os.path.exists(preprocessed_data_dir): os.mkdir(preprocessed_data_dir)\n",
    "        train_minutesDF[X_cols].to_csv(preprocessed_data_dir+'train_minutesDF_X.csv', index=False)\n",
    "        train_minutesDF[y_cols].to_csv(preprocessed_data_dir+'train_minutesDF_y.csv', index=False)\n",
    "        val_minutesDF[X_cols].to_csv(preprocessed_data_dir+'val_minutesDF_X.csv', index=False)\n",
    "        val_minutesDF[y_cols].to_csv(preprocessed_data_dir+'val_minutesDF_y.csv', index=False)\n",
    "    print('Returning train with '+'{:,}'.format(len(train_minutesDF))+' rows and val with '+'{:,}'.format(len(val_minutesDF))+' rows.')\n",
    "    return(train_minutesDF[X_cols], train_minutesDF[y_cols], val_minutesDF[X_cols], val_minutesDF[y_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(comb_row, omni_dir, config_dir, minute_dir):\n",
    "    data_dir = config_dir+'Data/'\n",
    "    # [x] check if we already have data loaded\n",
    "    daily_fn = data_dir+'daily_summary.csv'\n",
    "    all_minutes_fn = data_dir+'all_minutes.csv'\n",
    "    sec_guide_fn = data_dir+'sec_guide.csv'\n",
    "    all_minutesDF = 'not set yet'\n",
    "    if os.path.exists(daily_fn) and os.path.exists(all_minutes_fn) and os.path.exists(sec_guide_fn):\n",
    "        print('Data found! Loading data...')\n",
    "        # [x] check the order of other_secs\n",
    "        sec_guideDF = pd.read_csv(sec_guide_fn)\n",
    "        assert(comb_row.Sec1 == sec_guideDF.iloc[0].Sec)\n",
    "        other_secs_comb_row = comb_row[[col for col in comb_row.index if (col[:3]=='Sec' and col!='Sec1')]].values\n",
    "        other_secs_sec_guide = sec_guideDF.iloc[1:].Sec.values\n",
    "        assert(np.all(other_secs_comb_row == other_secs_sec_guide))\n",
    "        # [x] check the dates  \n",
    "        dailyDF = pd.read_csv(daily_fn, parse_dates=['Date'])\n",
    "        dailyDF.Date = dailyDF.Date.dt.date\n",
    "        first_date = dailyDF.Date.iloc[0]\n",
    "        last_date = dailyDF.Date.iloc[-1]\n",
    "        assert(abs((pd.to_datetime(first_date) - pd.to_datetime(comb_row.TrainStartDate)).days) < 1) # may want to relax these\n",
    "        assert(abs((pd.to_datetime(last_date) - pd.to_datetime(comb_row.ValEndDate)).days) < 1)\n",
    "        all_minutesDF = pd.read_csv(all_minutes_fn, parse_dates=['Date', 'Minute'])\n",
    "        all_minutesDF.Date = all_minutesDF.Date.dt.date\n",
    "        print('Data load complete.')\n",
    "    else:\n",
    "        other_secs = comb_row[[col for col in comb_row.index if (col[:3]=='Sec' and col!='Sec1')]].values\n",
    "        print('No pre-loaded data found. Loading data for '+comb_row.Sec1+' and '+','.join(other_secs))\n",
    "        sec1_minuteDF = pd.read_csv(minute_dir+comb_row.Sec1+'.csv', parse_dates=['Date', 'Minute'])\n",
    "        sec1_minuteDF.Date = sec1_minuteDF.Date.dt.date\n",
    "        sec1_minuteDF = sec1_minuteDF.loc[(sec1_minuteDF.Date >= comb_row.TrainStartDate) & (sec1_minuteDF.Date <= comb_row.ValEndDate)].reset_index(drop=True)\n",
    "        \n",
    "        print('Loading minuteDF for '+comb_row.Sec2)\n",
    "        \n",
    "        other_secs_minuteDF = pd.read_csv(minute_dir+comb_row.Sec2+'.csv', parse_dates=['Date', 'Minute'])[['Product', 'Date', 'Minute', 'O_B', 'O_A', 'H_B', 'H_A', 'L_B', 'L_A', 'C_B', 'C_A', 'Count', 'B_TickImb', 'A_TickImb', 'M_TickImb']]\n",
    "        for sec in other_secs[1:]:\n",
    "            other_secs_minuteDF = other_secs_minuteDF.append(pd.read_csv(minute_dir+sec+'.csv', parse_dates=['Date', 'Minute'])[['Product', 'Date', 'Minute', 'O_B', 'O_A', 'H_B', 'H_A', 'L_B', 'L_A', 'C_B', 'C_A', 'Count', 'B_TickImb', 'A_TickImb', 'M_TickImb']], ignore_index=True)\n",
    "        other_secs_minuteDF.Date = other_secs_minuteDF.Date.dt.date\n",
    "        other_secs_minuteDF = other_secs_minuteDF.loc[(other_secs_minuteDF.Date >= comb_row.TrainStartDate) & (other_secs_minuteDF.Date <= comb_row.ValEndDate)].reset_index(drop=True)\n",
    "        print('other_secs_minuteDF has '+str(len(other_secs_minuteDF))+' rows.')\n",
    "        print('sec1_minuteDF has '+str(len(sec1_minuteDF))+' rows.')\n",
    "        print(\"pd.read_csvs complete. Subsetting dates...\")\n",
    "        # [x] subset for dates\n",
    "        dates_in_common = set(sec1_minuteDF.Date.unique())\n",
    "        for sec in other_secs: \n",
    "            dates_in_common = dates_in_common.intersection(set(other_secs_minuteDF.loc[other_secs_minuteDF.Product == sec].Date.unique()))\n",
    "        sec1_dates_to_remove = set(sec1_minuteDF.Date.unique()).difference(dates_in_common)\n",
    "        print(str(len(dates_in_common))+' dates_in_common')\n",
    "        if len(sec1_dates_to_remove) > 0:\n",
    "            print('- removing '+str(len(sec1_dates_to_remove))+' dates from '+comb_row.Sec1)\n",
    "            sec1_minuteDF = sec1_minuteDF.loc[sec1_minuteDF.Date.isin(sec1_dates_to_remove)].reset_index(drop=True)\n",
    "        for sec in other_secs:\n",
    "            sec_dates_to_remove = set(other_secs_minuteDF.loc[other_secs_minuteDF.Product==sec].Date.unique()).difference(dates_in_common)\n",
    "            if len(sec_dates_to_remove) > 0:\n",
    "                print('- removing '+str(len(sec_dates_to_remove))+' dates from '+sec)\n",
    "        other_secs_minuteDF = other_secs_minuteDF.loc[other_secs_minuteDF.Date.isin(dates_in_common)].reset_index(drop=True)\n",
    "        print('other_secs_minuteDF has '+str(len(other_secs_minuteDF))+' rows.')\n",
    "        print('sec1_minuteDF has '+str(len(sec1_minuteDF))+' rows.')        \n",
    "        print(\"Date subset complete. Determining each day's Open/Closes...\")\n",
    "        # [x] determine each day's open and close\n",
    "        dailyDF = pd.DataFrame(columns=['Date', 'Open', 'Close'])\n",
    "        dailyDF['Date'] = sec1_minuteDF.Date.unique()\n",
    "        for i in tqdm.tqdm_notebook(range(len(dailyDF))):\n",
    "            date = dailyDF.loc[i].Date\n",
    "            lastOpen = sec1_minuteDF.loc[sec1_minuteDF.Date==date].Minute.min()\n",
    "            firstClose = sec1_minuteDF.loc[sec1_minuteDF.Date==date].Minute.max()\n",
    "            other_sec_date_subDF = other_secs_minuteDF.loc[other_secs_minuteDF.Date == date]\n",
    "            for sec in other_secs:\n",
    "                lastOpen = max(lastOpen, other_sec_date_subDF.loc[other_sec_date_subDF.Product==sec].Minute.min())\n",
    "                firstClose = min(firstClose, other_sec_date_subDF.loc[other_sec_date_subDF.Product==sec].Minute.max())\n",
    "            dailyDF.loc[i, 'Open'] = lastOpen\n",
    "            dailyDF.loc[i, 'Close'] = firstClose\n",
    "        dailyDF.Open = dailyDF.Open.dt.strftime(date_format='%H:%M')\n",
    "        dailyDF.Close = dailyDF.Close.dt.strftime(date_format='%H:%M')\n",
    "        dailyDF.to_csv(data_dir+'daily_summary.csv', index=False)\n",
    "        print(\"Each day's Open/Closes determination complete. Creating all_minutesDF...\")\n",
    "        # [x] create all_minutesDF\n",
    "        all_minutesDF = pd.DataFrame(columns=['Date', 'Minute'])\n",
    "        # enumerate minutes\n",
    "        for i in range(len(dailyDF)):\n",
    "            open_dt = pd.to_datetime(dailyDF.loc[i].Date.strftime(format='%Y-%m-%d')+' '+dailyDF.loc[i].Open, format='%Y-%m-%d %H:%M')\n",
    "            close_dt = pd.to_datetime(dailyDF.loc[i].Date.strftime(format='%Y-%m-%d')+' '+dailyDF.loc[i].Close, format='%Y-%m-%d %H:%M')\n",
    "            minute_range = pd.date_range(start=open_dt, end=close_dt, freq='T')\n",
    "            day_minutesDF = pd.DataFrame({'Date': minute_range.date, 'Minute': minute_range.values})\n",
    "            all_minutesDF = all_minutesDF.append(day_minutesDF, ignore_index=True)\n",
    "        #populate minute data     \n",
    "        col_stems = ['O_B', 'O_A', 'H_B', 'H_A', 'L_B', 'L_A', 'C_B', 'C_A', 'Count', 'B_TickImb', 'A_TickImb', 'M_TickImb']\n",
    "        first_minute_populate_stems = ['O_B', 'O_A', 'H_B', 'H_A', 'L_B', 'L_A', 'C_B', 'C_A']\n",
    "        for sec_num in range(1, len(other_secs)+2):\n",
    "            sec_cols = [col_stem+str(sec_num) for col_stem in col_stems]\n",
    "            for sec_col in sec_cols: all_minutesDF[sec_col] = np.nan\n",
    "        all_minutesDF[[c+'1' for c in col_stems]] = pd.merge(all_minutesDF[['Minute']], sec1_minuteDF[['Minute']+col_stems], on='Minute', how='left')[col_stems]\n",
    "        print('Merging into all_minutesDF...')\n",
    "        for sec_num in range(2, len(other_secs)+2):\n",
    "            other_sec = other_secs[sec_num-2]\n",
    "            all_minutesDF[[c+str(sec_num) for c in col_stems]] = pd.merge(all_minutesDF[['Minute']], other_secs_minuteDF[['Minute']+col_stems].loc[other_secs_minuteDF.Product==other_sec], on='Minute', how='left')[col_stems]\n",
    "        print('Getting the first datapoint of each day...')\n",
    "        #get first datapoint of each day        \n",
    "        for i in tqdm.tqdm_notebook(dailyDF.index):\n",
    "            date = dailyDF.loc[i].Date\n",
    "            open_dt = pd.to_datetime(dailyDF.loc[i].Date.strftime(format='%Y-%m-%d')+' '+dailyDF.loc[i].Open, format='%Y-%m-%d %H:%M')\n",
    "            sec1_last_row = sec1_minuteDF.loc[(sec1_minuteDF.Date == date) & (sec1_minuteDF.Minute <= open_dt)].iloc[-1]\n",
    "            if sec1_last_row.Minute < open_dt:\n",
    "                if (open_dt - sec1_last_row.Minute).seconds/60 > 20:\n",
    "                    raise ValueError('Too much time has elapsed. '+comb_row.Sec1+' open quote is stale at '+open_dt.strftime(format='%Y-%m-%d %H:%M')+' by '+str((open_dt - sec1_last_row.Minute).seconds/60)+' minutes.')\n",
    "                else:\n",
    "                    all_minutesDF.loc[all_minutesDF.Minute == open_dt, [c+'1' for c in col_stems]] = 0\n",
    "                    all_minutesDF.loc[all_minutesDF.Minute == open_dt, [c+'1' for c in first_minute_populate_stems]] = sec1_last_row[first_minute_populate_stems]\n",
    "            other_secs_subDF = other_secs_minuteDF.loc[(other_secs_minuteDF.Date == date) & (other_secs_minuteDF.Minute <= open_dt)]\n",
    "            for sec_num in range(2, len(other_secs)+2):\n",
    "                other_sec = other_secs[sec_num-2]\n",
    "                other_sec_last_row = other_secs_subDF.loc[other_secs_subDF.Product==other_sec].iloc[-1]\n",
    "                if other_sec_last_row.Minute < open_dt:\n",
    "                    if (open_dt - other_sec_last_row.Minute).seconds/60 > 20:\n",
    "                        raise ValueError(\"Too much time has elapsed. \"+other_sec+\" open quote is stale at \"+date.strftime(open_dt='%Y-%m-%d %H:%M')+' by '+str((open_dt - other_sec_last_row.Minute).seconds/60)+' minutes.')\n",
    "                    else:\n",
    "                        all_minutesDF.loc[all_minutesDF.Minute == open_dt, [c+str(sec_num) for c in col_stems]] = 0\n",
    "                        all_minutesDF.loc[all_minutesDF.Minute == open_dt, [c+str(sec_num) for c in first_minute_populate_stems]] = other_sec_last_row[first_minute_populate_stems]  \n",
    "        print('Saving all_minutesDF...')\n",
    "        all_minutesDF.to_csv(data_dir+'all_minutes.csv', index=False)\n",
    "        print('Save complete.')\n",
    "        sec_guideDF = pd.DataFrame({'Sec': [comb_row.Sec1]+list(other_secs)})\n",
    "        sec_guideDF.to_csv(data_dir+'sec_guide.csv', index=False)\n",
    "    return(processData(all_minutesDF, dailyDF, sec_guideDF, comb_row.ValStartDate, config_dir))              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Iterate on models</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_order = ['ZM', 'simple_LM', 'ridge_LM', 'norm_ridge_LM', 'lasso_LM', 'norm_lasso_LM', 'random_forest', 'XGBoost']\n",
    "model_order_dict = {}\n",
    "for i,mod in enumerate(model_order): model_order_dict[mod]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Models \n",
    "# - Linear regression\n",
    "#   - with and without regularization\n",
    "#   - with and without normalization\n",
    "# - Random Forest\n",
    "# - XGBoost?\n",
    "\n",
    "def getModelToDoDict(config_dir, trainY, model_names=model_order):\n",
    "    y_min_incs = [int(i.split('y_B')[1]) for i in trainY.columns if i[:3]=='y_B']\n",
    "    to_do_dict = {}; \n",
    "    for y_min_inc in y_min_incs: to_do_dict[y_min_inc] = model_names[:]\n",
    "    done_dict = {}\n",
    "    print('Searching for current model progress')\n",
    "    summary_sheet_fn = config_dir+'summary_sheet.html'\n",
    "    summary_sheet_html = 'not set yet'\n",
    "    with open(summary_sheet_fn,'r') as fh:\n",
    "        summary_sheet_html = fh.read()\n",
    "    # [x] rewrite with soup\n",
    "    summary_soup = Soup(summary_sheet_html)\n",
    "    all_models_soup = summary_soup.select('#all_models_table')\n",
    "    if len(all_models_soup) == 0:\n",
    "        print('No all_models_soup. Assuming no models have been done.')\n",
    "    else:\n",
    "        all_models_table = all_models_soup[0]\n",
    "        all_modelsDF = pd.read_html(str(all_models_table))[0] #don't know why [0] is necessary\n",
    "        for y in all_modelsDF.y.unique():\n",
    "            done_dict[y] = list(all_modelsDF.loc[all_modelsDF.y == y].Name.values)\n",
    "            done_dict[y] = [m for m in done_dict[y] if m[0]!='['] # does not get Keras models this way\n",
    "                \n",
    "    for done_y_min_inc_key in done_dict.keys():\n",
    "        for model_done in done_dict[done_y_min_inc_key]:\n",
    "            if model_done in to_do_dict[done_y_min_inc_key]:\n",
    "                to_do_dict[done_y_min_inc_key].remove(model_done)\n",
    "            else: \n",
    "                print('\\nWeird! For y_minute='+str(y_min_inc)+' we have done an extra model: '+model_done+'\\n')\n",
    "    if len(done_dict.keys()) == 0:\n",
    "        print('\\nNo models found. Models to go: '+str(to_do_dict)+'...')\n",
    "    else:\n",
    "        print('Some models already done: '+str(done_dict)+'\\n\\nModels to go: '+str(to_do_dict))\n",
    "    return(to_do_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatYData(trainY, valY, y_min_inc):\n",
    "    trainY['y'+str(y_min_inc)] = (trainY['y_B'+str(y_min_inc)] + trainY['y_A'+str(y_min_inc)])/2.0\n",
    "    valY['y'+str(y_min_inc)] = (valY['y_B'+str(y_min_inc)] + valY['y_A'+str(y_min_inc)])/2.0\n",
    "    return(trainY[['Minute', 'y'+str(y_min_inc)]], valY[['Minute', 'y'+str(y_min_inc)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRegressionSummary(model, X, y):\n",
    "    \"\"\"\n",
    "    https://stackoverflow.com/questions/27928275/find-p-value-significance-in-scikit-learn-linearregression\n",
    "    \"\"\"\n",
    "    params = np.append(model.intercept_,model.coef_)\n",
    "    predictions = model.predict(X)\n",
    "\n",
    "#     newX = pd.DataFrame({\"Constant\":np.ones(len(X))}).join(pd.DataFrame(X))\n",
    "#     newX = pd.DataFrame({\"Constant\":np.ones(len(X))}).join(pd.DataFrame(X.reset_index(drop=True)))\n",
    "#     MSE = (sum((y-predictions)**2))/(len(newX)-len(newX.columns))\n",
    "\n",
    "    # Note if you don't want to use a DataFrame replace the two lines above with\n",
    "    newX = np.append(np.ones((len(X),1)), X, axis=1)\n",
    "    MSE = (sum((y-predictions)**2))/(len(newX)-len(newX[0]))\n",
    "\n",
    "    var_b = MSE*(np.linalg.inv(np.dot(newX.T,newX)).diagonal())\n",
    "    sd_b = np.sqrt(var_b)\n",
    "    ts_b = params/ sd_b\n",
    "    p_values =[2*(1-stats.t.cdf(np.abs(i),(len(newX)-1))) for i in ts_b]\n",
    "    sd_b = np.round(sd_b,3)\n",
    "    ts_b = np.round(ts_b,3)\n",
    "    p_values = np.round(p_values,3)\n",
    "    params = np.round(params,4)\n",
    "    myDF3 = pd.DataFrame()\n",
    "    myDF3['Var'] = ['Const']+list(X.columns) #my modification to add the variable names\n",
    "    myDF3[\"Coefficients\"],myDF3[\"Standard Errors\"],myDF3[\"t values\"],myDF3[\"PValue\"] = [params,sd_b,ts_b,p_values]\n",
    "#     print(myDF3)\n",
    "    return(myDF3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSKModelResults(model, trainX, trainY_min_inc, valX, valY_min_inc, train_minutes, val_minutes, x_scaler=None, y_scaler=None):\n",
    "    try:\n",
    "        summaryDF = getRegressionSummary(model, trainX, trainY_min_inc)\n",
    "    except:\n",
    "        print('Error getting summaryDF')\n",
    "        summaryDF = None\n",
    "    train_preds = model.predict(trainX)\n",
    "    val_preds = model.predict(valX)\n",
    "    if x_scaler is not None:\n",
    "        trainX, trainY_min_inc, valX, valY_min_inc = unnormalizeData(trainX, trainY_min_inc, valX, valY_min_inc, x_scaler, y_scaler)\n",
    "        train_preds = unnormalizeIndividualData(train_preds, y_scaler)\n",
    "        val_preds = unnormalizeIndividualData(val_preds, y_scaler)\n",
    "    train_R2 = r2_score(trainY_min_inc, train_preds)\n",
    "    val_R2 = r2_score(valY_min_inc, val_preds)\n",
    "    train_resid = train_preds - trainY_min_inc\n",
    "    val_resid = val_preds - valY_min_inc\n",
    "    train_mse = np.sum(np.square(train_resid))/len(train_preds)\n",
    "    val_mse = np.sum(np.square(val_resid))/len(val_preds)\n",
    "    train_seDF = pd.DataFrame({'Minute': train_minutes.values, 'SE': np.square(train_resid)})\n",
    "    train_seDF['Date'] = train_seDF.Minute.dt.date\n",
    "    train_mse_by_dateDF = train_seDF.groupby('Date').agg({'SE': 'mean'})\n",
    "    val_seDF = pd.DataFrame({'Minute': val_minutes.values, 'SE': np.square(val_resid)})\n",
    "    val_seDF['Date'] = val_seDF.Minute.dt.date\n",
    "    val_mse_by_dateDF = val_seDF.groupby('Date').agg({'SE': 'mean'})\n",
    "    train_mse_by_dateDF.rename(columns={'SE': 'MSE'}, inplace=True); val_mse_by_dateDF.rename(columns={'SE': 'MSE'}, inplace=True)\n",
    "    return(train_R2, val_R2, train_mse, train_mse_by_dateDF, val_mse, val_mse_by_dateDF, summaryDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepDataForSKModel(trainX, trainY_min_inc, valX, valY_min_inc):\n",
    "    train_minutes, val_minutes = trainY_min_inc.Minute, valY_min_inc.Minute\n",
    "    trainX = trainX.drop(columns=['Minute']).astype(float)\n",
    "    trainY_min_inc = trainY_min_inc.drop(columns=['Minute']).values.astype(float)\n",
    "    trainY_min_inc = trainY_min_inc.reshape(len(trainY_min_inc))\n",
    "    valX = valX.drop(columns=['Minute']).astype(float)\n",
    "    valY_min_inc = valY_min_inc.drop(columns=['Minute']).values.astype(float)    \n",
    "    valY_min_inc = valY_min_inc.reshape(len(valY_min_inc)) \n",
    "    return(train_minutes, trainX, trainY_min_inc, val_minutes, valX, valY_min_inc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroModel(trainX, trainY_min_inc, valX, valY_min_inc):\n",
    "    print('in zeroModel')\n",
    "    train_minutes, trainX, trainY_min_inc, val_minutes, valX, valY_min_inc = \\\n",
    "        prepDataForSKModel(trainX, trainY_min_inc, valX, valY_min_inc)\n",
    "    train_R2 = r2_score(trainY_min_inc, np.zeros_like(trainY_min_inc))\n",
    "    train_seDF = pd.DataFrame({'Minute': train_minutes.values, 'SE': np.square(trainY_min_inc)})\n",
    "    train_seDF['Date'] = train_seDF.Minute.dt.date\n",
    "    train_mse_by_dateDF = train_seDF.groupby('Date').agg({'SE': 'mean'})\n",
    "    val_seDF = pd.DataFrame({'Minute': val_minutes.values, 'SE': np.square(valY_min_inc)})\n",
    "    val_seDF['Date'] = val_seDF.Minute.dt.date\n",
    "    val_mse_by_dateDF = val_seDF.groupby('Date').agg({'SE': 'mean'})\n",
    "    summaryDF = None\n",
    "    train_mse = np.sum(np.square(trainY_min_inc))/len(trainY_min_inc)\n",
    "    val_mse = np.sum(np.square(valY_min_inc))/len(valY_min_inc)\n",
    "    train_mse_by_dateDF.rename(columns={'SE': 'MSE'}, inplace=True); val_mse_by_dateDF.rename(columns={'SE': 'MSE'}, inplace=True)\n",
    "    return(train_R2, train_mse, train_mse_by_dateDF, val_mse, val_mse_by_dateDF, summaryDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleLM(trainX, trainY_min_inc, valX, valY_min_inc):\n",
    "    print('in simpleLM')\n",
    "    train_minutes, trainX, trainY_min_inc, val_minutes, valX, valY_min_inc = \\\n",
    "        prepDataForSKModel(trainX, trainY_min_inc, valX, valY_min_inc)\n",
    "    model = LinearRegression().fit(trainX, trainY_min_inc)\n",
    "    train_R2, val_R2, train_mse, train_mse_by_dateDF, val_mse, val_mse_by_dateDF, summaryDF = \\\n",
    "        getSKModelResults(model, trainX, trainY_min_inc, valX, valY_min_inc, train_minutes, val_minutes)\n",
    "    print('simpleLM gives:   R2='+str(round(train_R2, 4))+'   train_mse='+str(round(train_mse, 4))+'   val_mse='+str(round(val_mse, 4)))\n",
    "    return(train_R2, train_mse, train_mse_by_dateDF, val_mse, val_mse_by_dateDF, summaryDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateMSEDerivative(lambda_val, trainX, trainY_min_inc, num_k_folds, sk_func, epsilon=.3):\n",
    "    min_lambda_val = max(0, lambda_val-epsilon)\n",
    "    max_lambda_val = lambda_val+epsilon\n",
    "    print('calculating MSE and Derivative for lambda_val='+str(lambda_val))\n",
    "    mses, mses_minus_epsilon, mses_plus_epsilon = [], [], []\n",
    "    for k in tqdm.tqdm_notebook(range(num_k_folds)):\n",
    "        OOS_begin = int((k/num_k_folds)*len(trainX))\n",
    "        OOS_end = int(((k+1)/num_k_folds)*len(trainX))\n",
    "        OOS_mask = np.zeros(len(trainX))\n",
    "        OOS_mask[OOS_begin:OOS_end] = 1\n",
    "        OOS_mask = OOS_mask == 1\n",
    "        X_OOS = trainX[OOS_mask]\n",
    "        y_OOS = trainY_min_inc[OOS_mask] \n",
    "        X_sample = trainX[~OOS_mask]\n",
    "        y_sample = trainY_min_inc[~OOS_mask]\n",
    "        model = sk_func(alpha=lambda_val).fit(X_sample, y_sample)\n",
    "        OOS_preds = model.predict(X_OOS)\n",
    "        OOS_resid = OOS_preds - trainY_min_inc[OOS_mask]\n",
    "        mses.append(np.sum(np.square(OOS_resid))/len(OOS_resid))\n",
    "        model_minus_epsilon = sk_func(alpha=min_lambda_val).fit(X_sample, y_sample)\n",
    "        OOS_preds = model_minus_epsilon.predict(X_OOS)\n",
    "        OOS_resid = OOS_preds - trainY_min_inc[OOS_mask]\n",
    "        mses_minus_epsilon.append(np.sum(np.square(OOS_resid))/len(OOS_resid))\n",
    "        model_plus_epsilon = sk_func(alpha=max_lambda_val).fit(X_sample, y_sample)\n",
    "        OOS_preds = model_plus_epsilon.predict(X_OOS)\n",
    "        OOS_resid = OOS_preds - trainY_min_inc[OOS_mask]\n",
    "        mses_plus_epsilon.append(np.sum(np.square(OOS_resid))/len(OOS_resid))\n",
    "    mse = np.mean(mses); mse_minus_epsilon = np.mean(mses_minus_epsilon); mse_plus_epsilon = np.mean(mses_plus_epsilon)\n",
    "    if (abs(np.sign(mse-mse_minus_epsilon) - np.sign(mse_plus_epsilon-mse)) == 2):\n",
    "        print('Caution! Both signs point the same way: '+str(np.sign(mse_plus_epsilon-mse)))\n",
    "    derivative = (mse_plus_epsilon - mse_minus_epsilon)/(max_lambda_val-min_lambda_val)\n",
    "    return(mse, derivative)\n",
    "\n",
    "def findLambdaVal(trainX, trainY_min_inc, reg_type='ridge'):\n",
    "    print('Finding optimal lambda_val')\n",
    "    assert(reg_type in ['ridge', 'lasso'])\n",
    "    sk_func = Ridge\n",
    "    if reg_type=='lasso': sk_funk=Lasso\n",
    "    lambda_val = 1.0\n",
    "    learning_rate = .5\n",
    "    num_k_folds = 5\n",
    "    mse_tolerance = .0001\n",
    "    last_mse = 999; curr_mse = 998\n",
    "    while last_mse - curr_mse > mse_tolerance:\n",
    "        last_mse = curr_mse\n",
    "        curr_mse, derivative = calculateMSEDerivative(lambda_val, trainX, trainY_min_inc, num_k_folds, sk_func)\n",
    "        print('(curr_mse, derivative): '+str((curr_mse, derivative)))\n",
    "        if derivative==0: return(lambda_val)\n",
    "        lambda_val -= derivative*learning_rate\n",
    "    return(lambda_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeData(trainX, trainY_min_inc, valX, valY_min_inc):\n",
    "    print('Normalizing data')\n",
    "    #need to reshape in order to do the scaling. Will unreshape at the end\n",
    "    trainY_min_inc = trainY_min_inc.reshape(-1, 1); valY_min_inc = valY_min_inc.reshape(-1, 1)\n",
    "    x_scaler = preprocessing.StandardScaler().fit(trainX)\n",
    "    y_scaler = preprocessing.StandardScaler().fit(trainY_min_inc)\n",
    "    trainX_normed = x_scaler.transform(trainX)\n",
    "    valX_normed = x_scaler.transform(valX)\n",
    "    trainY_normed = y_scaler.transform(trainY_min_inc)\n",
    "    valY_normed = y_scaler.transform(valY_min_inc)\n",
    "    trainY_normed = trainY_normed.reshape(len(trainY_normed)); valY_normed = valY_normed.reshape(len(valY_normed)) \n",
    "    return(trainX_normed, trainY_normed, valX_normed, valY_normed, x_scaler, y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridgeLM(trainX, trainY_min_inc, valX, valY_min_inc, lambda_val=None, normalize=False):\n",
    "    print('in ridgeLM with lambda_val='+str(lambda_val)+' and normalize='+str(normalize))\n",
    "    train_minutes, trainX, trainY_min_inc, val_minutes, valX, valY_min_inc = \\\n",
    "        prepDataForSKModel(trainX, trainY_min_inc, valX, valY_min_inc)\n",
    "    if lambda_val is None: lambda_val = findLambdaVal(trainX, trainY_min_inc, reg_type='ridge')\n",
    "    model = Ridge(alpha=lambda_val).fit(trainX, trainY_min_inc)\n",
    "    train_R2, val_R2, train_mse, train_mse_by_dateDF, val_mse, val_mse_by_dateDF, summaryDF = \\\n",
    "        getSKModelResults(model, trainX, trainY_min_inc, valX, valY_min_inc, train_minutes, val_minutes)\n",
    "    print('ridgeLM gives:   R2='+str(round(train_R2, 4))+'   train_mse='+str(round(train_mse, 4))+'   val_mse='+str(round(val_mse, 4)))\n",
    "    return(train_R2, train_mse, train_mse_by_dateDF, val_mse, val_mse_by_dateDF, summaryDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lassoLM(trainX, trainY_min_inc, valX, valY_min_inc, lambda_val=None, normalize=False):\n",
    "    print('in lassoLM with lambda_val='+str(lambda_val)+' and normalize='+str(normalize))\n",
    "    train_minutes, trainX, trainY_min_inc, val_minutes, valX, valY_min_inc = \\\n",
    "        prepDataForSKModel(trainX, trainY_min_inc, valX, valY_min_inc)\n",
    "    if lambda_val is None: lambda_val = findLambdaVal(trainX, trainY_min_inc, reg_type='lasso')\n",
    "    model = Lasso(alpha=lambda_val).fit(trainX, trainY_min_inc)\n",
    "    train_R2, val_R2, train_mse, train_mse_by_dateDF, val_mse, val_mse_by_dateDF, summaryDF = \\\n",
    "        getSKModelResults(model, trainX, trainY_min_inc, valX, valY_min_inc, train_minutes, val_minutes)\n",
    "    print('lassoLM gives:   R2='+str(round(train_R2, 4))+'   train_mse='+str(round(train_mse, 4))+'   val_mse='+str(round(val_mse, 4)))\n",
    "    return(train_R2, train_mse, train_mse_by_dateDF, val_mse, val_mse_by_dateDF, summaryDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normRidgeLM(trainX, trainY_min_inc, valX, valY_min_inc, lambda_val=None):\n",
    "    return(ridgeLM(trainX, trainY_min_inc, valX, valY_min_inc, lambda_val, normalize=True))\n",
    "\n",
    "def normLassoLM(trainX, trainY_min_inc, valX, valY_min_inc, lambda_val=None):\n",
    "    return(lassoLM(trainX, trainY_min_inc, valX, valY_min_inc, lambda_val, normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomForest(trainX, trainY_min_inc, valX, valY_min_inc):\n",
    "    print('in randomForest')\n",
    "    train_minutes, trainX, trainY_min_inc, val_minutes, valX, valY_min_inc = \\\n",
    "        prepDataForSKModel(trainX, trainY_min_inc, valX, valY_min_inc)\n",
    "    model = DecisionTreeRegressor(random_state=12).fit(trainX, trainY_min_inc)\n",
    "    train_R2, val_R2, train_mse, train_mse_by_dateDF, val_mse, val_mse_by_dateDF, summaryDF = \\\n",
    "        getSKModelResults(model, trainX, trainY_min_inc, valX, valY_min_inc, train_minutes, val_minutes)\n",
    "    print('randomForest gives:   R2='+str(round(train_R2, 4))+'   train_mse='+str(round(train_mse, 4))+'   val_mse='+str(round(val_mse, 4)))\n",
    "    return(train_R2, train_mse, train_mse_by_dateDF, val_mse, val_mse_by_dateDF, summaryDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalizeIndividualData(scale, normed_data):\n",
    "    means = scale.mean_\n",
    "    stds = scale.scale_\n",
    "    unstd_data = np.multiply(normed_data, stds)\n",
    "    return(np.add(unstd_data, means))\n",
    "\n",
    "def unnormalizeData(trainX_normed, trainY_normed, valX_normed, valY_normed, x_scaler, y_scaler):\n",
    "    return(unnormalizeIndividualData(x_scaler, trainX_normed), unnormalizeIndividualData(y_scaler, trainY_normed),\n",
    "           unnormalizeIndividualData(x_scaler, valY_normed), unnormalizeIndividualData(y_scaler, valY_normed))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Keras Models </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define these models as a list of dictionaries. The dictionaries can be as follows.\n",
    "\n",
    "- Dropout\n",
    " - rate\n",
    "\n",
    "- Dense\n",
    " - activation\n",
    " - units\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model_name = '[Dense_64_rel][Drop_0.2][Dense_32_rel][Dense_1_lin]'\n",
    "def getHTMLIDFromKerasModelName(keras_model_name):\n",
    "    out_str = ''\n",
    "    layer_reps = [i.split('[')[1] for i in keras_model_name.split(']') if i!='']\n",
    "    for l in layer_reps:\n",
    "        out_str += l.replace('.', 'dp')+'___'\n",
    "    return out_str\n",
    "\n",
    "def getKerasModelNameFromHTMLID(html_id):\n",
    "    out_str = ''\n",
    "    layer_reps = [i for i in html_id.split('___') if i!='']\n",
    "    for l in layer_reps:\n",
    "        out_str += '['+l.replace('dp', '.')+']'\n",
    "    return out_str\n",
    "\n",
    "print(keras_model_name)\n",
    "print(getHTMLIDFromKerasModelName(keras_model_name))\n",
    "print(getKerasModelNameFromHTMLID(getHTMLIDFromKerasModelName(keras_model_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kerasModelArrayToStr(model_array):\n",
    "    return_str = ''\n",
    "    for layer_dict in model_array:\n",
    "        layer_str = 'not set yet'\n",
    "        if layer_dict['layer'] == 'Dense':\n",
    "            layer_str = 'Dense_'+str(layer_dict['units'])+'_'+layer_dict['activation'][:3]\n",
    "        elif layer_dict['layer'] == 'Dropout':\n",
    "            layer_str = 'Drop_'+str(layer_dict['rate'])\n",
    "        else:\n",
    "            raise ValueError(layer_dict['layer']+' not recognized.')\n",
    "        return_str += '['+layer_str+']'\n",
    "    return(return_str)\n",
    "\n",
    "def kerasModelArrayToModel(model_array, input_size):\n",
    "    assert(model_array[0]['layer'] == 'Dense')\n",
    "    assert((model_array[-1]['layer'] == 'Dense') and (model_array[-1]['units'] == 1) and (model_array[-1]['activation'] == 'linear') )\n",
    "    output_model = Sequential()\n",
    "    output_model.add(Dense(units=model_array[0]['units'], activation=model_array[0]['activation'], input_shape=(input_size,)))\n",
    "    for layer_dict in model_array[1:]:\n",
    "        if layer_dict['layer'] == 'Dense':\n",
    "            output_model.add(Dense(units=layer_dict['units'], activation=layer_dict['activation']))\n",
    "        elif layer_dict['layer'] == 'Dropout':\n",
    "            output_model.add(Dropout(rate=layer_dict['rate']))\n",
    "        else:\n",
    "            raise ValueError(layer_dict['layer']+' not recognized.')\n",
    "    output_model.compile(loss='mse', optimizer='adam')\n",
    "    return(output_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_models = [\n",
    "    [{'layer': 'Dense', 'activation': 'relu', 'units': 64}, \n",
    "        {'layer': 'Dropout', 'rate': .2},\n",
    "        {'layer': 'Dense', 'activation': 'relu', 'units': 32}, \n",
    "        {'layer': 'Dense', 'activation': 'linear', 'units': 1}],\n",
    "    [{'layer': 'Dense', 'activation': 'leaky_relu', 'units': 64}, \n",
    "        {'layer': 'Dropout', 'rate': .2},\n",
    "        {'layer': 'Dense', 'activation': 'leaky_relu', 'units': 32}, \n",
    "        {'layer': 'Dense', 'activation': 'linear', 'units': 1}],\n",
    "    [{'layer': 'Dense', 'activation': 'relu', 'units': 64}, \n",
    "        {'layer': 'Dropout', 'rate': .2},\n",
    "        {'layer': 'Dense', 'activation': 'relu', 'units': 32}, \n",
    "        {'layer': 'Dense', 'activation': 'linear', 'units': 1}],\n",
    "    [{'layer': 'Dense', 'activation': 'relu', 'units': 256}, \n",
    "        {'layer': 'Dense', 'activation': 'relu', 'units': 128}, \n",
    "        {'layer': 'Dense', 'activation': 'relu', 'units': 64}, \n",
    "        {'layer': 'Dense', 'activation': 'relu', 'units': 32}, \n",
    "        {'layer': 'Dense', 'activation': 'linear', 'units': 1}],    \n",
    "    [{'layer': 'Dense', 'activation': 'relu', 'units': 256}, \n",
    "        {'layer': 'Dense', 'activation': 'relu', 'units': 128}, \n",
    "        {'layer': 'Dropout', 'rate': .1},\n",
    "        {'layer': 'Dense', 'activation': 'relu', 'units': 64}, \n",
    "        {'layer': 'Dense', 'activation': 'relu', 'units': 32}, \n",
    "        {'layer': 'Dense', 'activation': 'linear', 'units': 1}],    \n",
    "    [{'layer': 'Dense', 'activation': 'relu', 'units': 256}, \n",
    "        {'layer': 'Dense', 'activation': 'relu', 'units': 128}, \n",
    "        {'layer': 'Dropout', 'rate': .2},\n",
    "        {'layer': 'Dense', 'activation': 'relu', 'units': 64}, \n",
    "        {'layer': 'Dense', 'activation': 'relu', 'units': 32}, \n",
    "        {'layer': 'Dense', 'activation': 'linear', 'units': 1}],    \n",
    "    [{'layer': 'Dense', 'activation': 'relu', 'units': 256}, \n",
    "        {'layer': 'Dense', 'activation': 'relu', 'units': 128}, \n",
    "        {'layer': 'Dropout', 'rate': .2},\n",
    "        {'layer': 'Dense', 'activation': 'relu', 'units': 64}, \n",
    "        {'layer': 'Dropout', 'rate': .2},\n",
    "        {'layer': 'Dense', 'activation': 'relu', 'units': 32}, \n",
    "        {'layer': 'Dense', 'activation': 'linear', 'units': 1}]]\n",
    "\n",
    "keras_model_strs = [kerasModelArrayToStr(keras_model) for keras_model in keras_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-c9036c77f275>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mgetKerasModelToDoDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeras_models\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mkeras_model_strs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkerasModelArrayToStr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeras_models\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0my_min_incs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y_B'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'y_B'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mto_do_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0my_min_inc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_min_incs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mto_do_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_min_inc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras_model_strs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras_models' is not defined"
     ]
    }
   ],
   "source": [
    "def getKerasModelToDoDict(config_dir, trainY, keras_models=keras_models):\n",
    "    keras_model_strs = [kerasModelArrayToStr(m) for m in keras_models]\n",
    "    y_min_incs = [int(i.split('y_B')[1]) for i in trainY.columns if i[:3]=='y_B']\n",
    "    to_do_dict = {}; \n",
    "    for y_min_inc in y_min_incs: to_do_dict[y_min_inc] = keras_model_strs[:]\n",
    "    done_dict = {}\n",
    "    print('Searching for current Keras model progress')\n",
    "    summary_sheet_fn = config_dir+'summary_sheet.html'\n",
    "    summary_sheet_html = 'not set yet'\n",
    "    with open(summary_sheet_fn,'r') as fh:\n",
    "        summary_sheet_html = fh.read()\n",
    "    # [x] rewrite with soup\n",
    "    summary_soup = Soup(summary_sheet_html)\n",
    "    all_models_soup = summary_soup.select('#all_models_table')\n",
    "    if len(all_models_soup) == 0:\n",
    "        print('No all_models_soup. Assuming no models have been done.')\n",
    "    else:\n",
    "        all_models_table = all_models_soup[0]\n",
    "        all_modelsDF = pd.read_html(str(all_models_table))[0] #don't know why [0] is necessary\n",
    "        for y in all_modelsDF.y.unique():\n",
    "            done_dict[y] = list(all_modelsDF.loc[all_modelsDF.y == y].Name.values)\n",
    "            done_dict[y] = [m for m in done_dict[y] if m[0]=='['] # only gets Keras models this way\n",
    "            if len(done_dict[y]) == 0: del done_dict[y]\n",
    "                \n",
    "    for done_y_min_inc_key in done_dict.keys():\n",
    "        for model_done in done_dict[done_y_min_inc_key]:\n",
    "            if model_done in to_do_dict[done_y_min_inc_key]:\n",
    "                to_do_dict[done_y_min_inc_key].remove(model_done)\n",
    "            else: \n",
    "                print('\\nWeird! For y_minute='+str(y_min_inc)+' we have done an extra model: '+model_done+'\\n')\n",
    "    if len(done_dict.keys()) == 0:\n",
    "        print('\\nNo models found. Models to go: '+str(to_do_dict)+'...')\n",
    "    else:\n",
    "        print('Some models already done: '+str(done_dict)+'\\n\\nModels to go: '+str(to_do_dict))\n",
    "    return(to_do_dict)\n",
    "\n",
    "# getKerasModelToDoDict(config_dir, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMSEInfoByChunking(train_preds, trainY_min_inc, val_preds, valY_min_inc, train_minutes, val_minutes):\n",
    "    \"\"\"\n",
    "    Obtain the MSE Info using chunking because we are running into MemoryErrors otherwise.\n",
    "    \"\"\"\n",
    "    chunk_size = 2048*2*2\n",
    "    train_seDF = pd.DataFrame(columns=['Date', 'SE_sum', 'count'])\n",
    "    val_seDF = pd.DataFrame(columns=['Date', 'SE_sum', 'count'])\n",
    "    assert train_preds.shape==trainY_min_inc.shape, 'train_preds.shape='+str(train_preds.shape)+' trainY_min_inc.shape='+str(trainY_min_inc.shape)\n",
    "    assert val_preds.shape==valY_min_inc.shape, 'val_preds.shape='+str(val_preds.shape)+' valY_min_inc.shape='+str(valY_min_inc.shape)\n",
    "    print('train_preds.shape: '+str(train_preds.shape))\n",
    "    num_chunks = int(np.ceil(len(train_preds)/chunk_size))\n",
    "    print('chunking train...')\n",
    "    for i in range(num_chunks):\n",
    "        cs = i*chunk_size; ce = (i+1)*chunk_size #chunk start and chunk end\n",
    "        print(train_minutes.values[cs:ce][0])\n",
    "        chunk_trainDF = pd.DataFrame({'Minute': train_minutes.values[cs:ce],\n",
    "                                      'Pred': train_preds[cs:ce],\n",
    "                                      'TrueY': trainY_min_inc[cs:ce]})\n",
    "        chunk_trainDF['Resid'] = chunk_trainDF.Pred - chunk_trainDF.TrueY\n",
    "        chunk_trainDF['SE'] = chunk_trainDF.Resid * chunk_trainDF.Resid\n",
    "        chunk_trainDF['Date'] = chunk_trainDF.Minute.dt.date\n",
    "        chunk_agg = chunk_trainDF.groupby('Date').agg({'SE': 'sum', 'Pred': 'count'})\n",
    "        chunk_agg['Date'] = chunk_agg.index\n",
    "        chunk_agg.rename(columns={'SE': 'SE_sum', 'Pred': 'count'}, inplace=True)\n",
    "        train_seDF = train_seDF.append(chunk_agg, ignore_index=True)\n",
    "    num_chunks = int(np.ceil(len(val_preds)/chunk_size))\n",
    "    print('chunking val...')\n",
    "    for i in range(num_chunks):\n",
    "        cs = i*chunk_size; ce = (i+1)*chunk_size #chunk start and chunk end\n",
    "        print(val_minutes.values[cs:ce][0])\n",
    "        chunk_valDF = pd.DataFrame({'Minute': val_minutes.values[cs:ce],\n",
    "                                      'Pred': val_preds[cs:ce],\n",
    "                                      'TrueY': valY_min_inc[cs:ce]})\n",
    "        chunk_valDF['Resid'] = chunk_valDF.Pred - chunk_valDF.TrueY\n",
    "        chunk_valDF['SE'] = chunk_valDF.Resid * chunk_valDF.Resid\n",
    "        chunk_valDF['Date'] = chunk_valDF.Minute.dt.date\n",
    "        chunk_agg = chunk_valDF.groupby('Date').agg({'SE': 'sum', 'Pred': 'count'})\n",
    "        chunk_agg['Date'] = chunk_agg.index\n",
    "        chunk_agg.rename(columns={'SE': 'SE_sum', 'Pred': 'count'}, inplace=True)\n",
    "        val_seDF = val_seDF.append(chunk_agg, ignore_index=True)\n",
    "    train_mse = train_seDF.SE_sum.sum()/train_seDF['count'].sum()\n",
    "    val_mse = val_seDF.SE_sum.sum()/val_seDF['count'].sum()\n",
    "    train_mse_by_dateDF = train_seDF.groupby('Date').agg({'SE_sum': 'sum', 'count': 'sum'})\n",
    "    train_mse_by_dateDF['Date'] = train_mse_by_dateDF.index\n",
    "    train_mse_by_dateDF['MSE'] = train_mse_by_dateDF.SE_sum/train_mse_by_dateDF['count']\n",
    "    val_mse_by_dateDF = val_seDF.groupby('Date').agg({'SE_sum': 'sum', 'count': 'sum'})\n",
    "    val_mse_by_dateDF['Date'] = val_mse_by_dateDF.index\n",
    "    val_mse_by_dateDF['MSE'] = val_mse_by_dateDF.SE_sum/val_mse_by_dateDF['count']\n",
    "    return(train_mse, train_mse_by_dateDF[['Date', 'MSE']], val_mse, val_mse_by_dateDF[['Date', 'MSE']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKerasModelResults(keras_model, trainX, trainY_min_inc, valX, valY_min_inc, train_minutes, val_minutes):\n",
    "    print('in getKerasModelResults')\n",
    "    summaryDF = None\n",
    "    train_preds = keras_model.predict(trainX)\n",
    "    train_preds = train_preds.reshape(len(train_preds))\n",
    "    val_preds = keras_model.predict(valX)\n",
    "    val_preds = val_preds.reshape(len(val_preds))\n",
    "    train_R2 = r2_score(trainY_min_inc, train_preds)\n",
    "    val_R2 = r2_score(valY_min_inc, val_preds)\n",
    "#     train_resid = train_preds - trainY_min_inc\n",
    "#     val_resid = val_preds - valY_min_inc\n",
    "#     train_mse = np.sum(np.square(train_resid))/len(train_preds)\n",
    "#     val_mse = np.sum(np.square(val_resid))/len(val_preds)\n",
    "#     train_seDF = pd.DataFrame({'Minute': train_minutes.values, 'SE': np.square(train_resid)})\n",
    "#     train_seDF['Date'] = train_seDF.Minute.dt.date\n",
    "#     train_mse_by_dateDF = train_seDF.groupby('Date').agg({'SE': 'mean'})\n",
    "#     val_seDF = pd.DataFrame({'Minute': val_minutes.values, 'SE': np.square(val_resid)})\n",
    "#     val_seDF['Date'] = val_seDF.Minute.dt.date\n",
    "#     val_mse_by_dateDF = val_seDF.groupby('Date').agg({'SE': 'mean'})\n",
    "#     train_mse_by_dateDF.rename(columns={'SE': 'MSE'}, inplace=True); val_mse_by_dateDF.rename(columns={'SE': 'MSE'}, inplace=True)\n",
    "    train_mse, train_mse_by_dateDF, val_mse, val_mse_by_dateDF = \\\n",
    "        getMSEInfoByChunking(train_preds, trainY_min_inc, val_preds, valY_min_inc, train_minutes, val_minutes)\n",
    "    return(train_R2, val_R2, train_mse, train_mse_by_dateDF, val_mse, val_mse_by_dateDF, summaryDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isMonotonicIncreasing(input_array):\n",
    "    return(all(input_array[i] <= input_array[i + 1] for i in range(len(input_array) - 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainKerasModel(keras_model, model_str, trainX, trainY_min_inc, y_min_inc, train_minutes, config_dir):\n",
    "    \"\"\"\n",
    "    1. Splits train data into a validation set\n",
    "    2. Trains the model, using an algorithm to decide when to stop training\n",
    "    3. Saves progress along the way\n",
    "    \"\"\"\n",
    "    print('in trainKerasModel')\n",
    "    y_min_inc_dir = config_dir + 'Deep_Models/'+str(y_min_inc)+'/'\n",
    "    model_dir = y_min_inc_dir+model_str+'/'\n",
    "    starting_epoch, epoch = 0, 0\n",
    "    if not os.path.exists(y_min_inc_dir): os.mkdir(y_min_inc_dir)\n",
    "    if not os.path.exists(model_dir): os.mkdir(model_dir)\n",
    "    if len(os.listdir(model_dir)) > 0:\n",
    "        starting_epoch = max(int(fn.split('.')[0]) for fn in os.listdir(model_dir))+1\n",
    "        epoch = starting_epoch + 0 \n",
    "    # [x] 1. Splits train data into a validation set\n",
    "    split_pct = .7\n",
    "    dates = train_minutes.dt.date.unique()\n",
    "    tr_dates = dates[:int(split_pct*len(dates))]\n",
    "    tr_mask = train_minutes.dt.date.isin(tr_dates)\n",
    "    tr_X = trainX[tr_mask]; tr_Y = trainY_min_inc[tr_mask]\n",
    "    vl_X = trainX[~tr_mask]; vl_Y = trainY_min_inc[~tr_mask]\n",
    "    # [x] 2. Trains the model, using an algorithm to decide when to stop training\n",
    "    tr_mses, vl_mses, = [],[]\n",
    "       \n",
    "    min_epochs = 5\n",
    "    max_epochs = 50\n",
    "    num_prev_models_to_consider = 3\n",
    "    \n",
    "    if starting_epoch > 0:\n",
    "        print('reconstructing '+str(starting_epoch)+' vl_mses')\n",
    "        # [x] reconstruct vl_mses\n",
    "        epochs_to_reconstruct = range(starting_epoch)\n",
    "        for ep in epochs_to_reconstruct:\n",
    "            reconstructed_model_fn = model_dir+str(ep)+'.h5'\n",
    "            reconstructed_model = load_model(reconstructed_model_fn)\n",
    "            reconstructed_model_vl_mse = reconstructed_model.evaluate(vl_X, vl_Y)\n",
    "            vl_mses.append((ep, reconstructed_model_vl_mse))\n",
    "        print(vl_mses)\n",
    "\n",
    "    # If we notice val_mse is increasing across num_prev_models_to_consider, we stop and return.\n",
    "    # Else, return the min val_mse model\n",
    "    while epoch < max_epochs:\n",
    "        if epoch > 0: keras_model = load_model(model_dir+str(epoch-1)+'.h5') # I think we need to do this but not sure\n",
    "        if epoch % 3 == 0: print('\\nepoch: '+str(epoch))\n",
    "        history = keras_model.fit(tr_X, tr_Y, validation_data=(vl_X, vl_Y), shuffle=False, epochs=1, verbose=1)\n",
    "        vl_mse = history.history['val_loss'][0]\n",
    "        vl_mses.append((epoch, vl_mse))\n",
    "#         tr_mse = keras_model.evaluate(tr_X, tr_Y)\n",
    "        model_fn = model_dir+str(epoch)+'.h5'\n",
    "        keras_model.save(model_fn)\n",
    "        if len(vl_mses) > 2*num_prev_models_to_consider:\n",
    "            if isMonotonicIncreasing([i[1] for i in vl_mses[-num_prev_models_to_consider:]]):\n",
    "                print('val mse has been monotonic increasing for the past '+str(num_prev_models_to_consider)+' epochs. Breaking.')\n",
    "                break\n",
    "            min_idx = np.argmin([t[1] for t in vl_mses])\n",
    "            if epoch - min_idx > 2*num_prev_models_to_consider:\n",
    "                print('We saw the best vl_mse '+str(epoch - min_idx)+' epochs ago. Breaking.')\n",
    "                break\n",
    "        epoch += 1\n",
    "    # return min model in stack\n",
    "    min_idx = np.argmin([t[1] for t in vl_mses])\n",
    "    print('The best model is from epoch '+str(vl_mses[min_idx][0])+' with a val_mse of '+str(vl_mses[min_idx][1]))\n",
    "    best_model = load_model(model_dir+str(vl_mses[min_idx][0])+'.h5')\n",
    "    return(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kerasModel(model_array, trainX, trainY_min_inc, y_min_inc, valX, valY_min_inc, config_dir):\n",
    "    model_str = kerasModelArrayToStr(model_array)\n",
    "    print('in kerasModel with: '+model_str)\n",
    "    train_minutes, trainX, trainY_min_inc, val_minutes, valX, valY_min_inc = \\\n",
    "        prepDataForSKModel(trainX, trainY_min_inc, valX, valY_min_inc)\n",
    "    keras_model = kerasModelArrayToModel(model_array, input_size=trainX.shape[1])\n",
    "    keras_model = trainKerasModel(keras_model, model_str, trainX, trainY_min_inc, y_min_inc, train_minutes, config_dir)\n",
    "    train_R2, val_R2, train_mse, train_mse_by_dateDF, val_mse, val_mse_by_dateDF, summaryDF = \\\n",
    "        getKerasModelResults(keras_model, trainX, trainY_min_inc, valX, valY_min_inc, train_minutes, val_minutes)\n",
    "    print('kerasModel gives:   R2='+str(round(train_R2, 4))+'   train_mse='+str(round(train_mse, 4))+'   val_mse='+str(round(val_mse, 4)))\n",
    "    return(train_R2, train_mse, train_mse_by_dateDF, val_mse, val_mse_by_dateDF, summaryDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphDailyMSE(mse_by_dateDF, zero_model_mse_by_dateDF, graph_title='untitled'):\n",
    "    assert np.all(zero_model_mse_by_dateDF.index.values == mse_by_dateDF.index.values), 'Dates do not match up with zeromodel for graphing.'\n",
    "    mergeDF = pd.merge(mse_by_dateDF, zero_model_mse_by_dateDF, suffixes=('_model', '_zero'), left_index=True, right_index=True)\n",
    "    return(mergeDF.plot(title=graph_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMod(bigger_soup, elem_type, search_string):\n",
    "    \"\"\"\n",
    "    Guards against not finding the element in find() because of newlines\n",
    "    \"\"\"\n",
    "    elems = bigger_soup.find_all(elem_type)\n",
    "    for elem in elems:\n",
    "        if search_string in elem.string:\n",
    "            return elem\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateResultsSummary(summary_soup, model_name, config_dir, y_min_inc,\n",
    "                       train_R2, train_mse, train_mse_by_dateDF, val_mse, val_mse_by_dateDF, summaryDF,\n",
    "                      zm_train_R2, zm_train_mse, zm_train_mse_by_dateDF, zm_val_mse, zm_val_mse_by_dateDF):\n",
    "    print('in updateResultsSummary')      \n",
    "    res_sum = findMod(summary_soup, 'h1', 'Results Summary')\n",
    "    config_soup = findMod(summary_soup, 'h1', 'Configuration')\n",
    "    best_models = summary_soup.select(\"#best_models\")\n",
    "    if len(best_models) == 0:\n",
    "        best_models = summary_soup.new_tag('h3', id='best_models')\n",
    "        best_models.string = 'Best Models'\n",
    "        res_sum.insert_after(best_models)\n",
    "    else:\n",
    "        best_models = best_models[0]\n",
    "\n",
    "    best_models_table  = summary_soup.select(\"#best_models_table\")\n",
    "    new_potential_row =  {'y':[y_min_inc], 'Name':[model_name], '% Imprv ZM':[round((zm_val_mse-val_mse)/zm_val_mse, 3)], 'Val MSE':[round(val_mse, 4)], 'ZM Val MSE':[round(zm_val_mse, 4)], 'Train R2':[round(train_R2, 4)], 'Val R2':[np.nan], 'Train MSE':[round(train_mse,4)], 'ZM Train MSE':[round(zm_train_mse,4)]}\n",
    "    col_order = pd.DataFrame(new_potential_row).columns.values\n",
    "    if len(best_models_table) == 0:\n",
    "        # [x] create new best_models_table\n",
    "        print('No best models table found. Creating best models')\n",
    "        best_modelsDF = pd.DataFrame(new_potential_row)\n",
    "        best_models_html = best_modelsDF.to_html(table_id = 'best_models_table', index=False, col_space=50, justify='center')\n",
    "        best_models_soup = Soup(best_models_html)\n",
    "        best_models.insert_after(best_models_soup)\n",
    "        best_models_soup = summary_soup.select('#best_models_table')[0] #I don't know why we have to redefine this after the insert but we do\n",
    "        # [x] create plots div\n",
    "        best_plots_div = summary_soup.new_tag('div', id='best_plots')\n",
    "        best_models_soup.insert_after(best_plots_div)\n",
    "        model_plot = summary_soup.new_tag('img', id=str(y_min_inc)+'min_plot', \n",
    "                                  src=config_dir+'Plots/'+model_name+'_'+str(y_min_inc)+'min_val_MSE.png',\n",
    "                                  height='400')\n",
    "        best_plots_div.append(model_plot)\n",
    "    else:\n",
    "        best_models_table = best_models_table[0]\n",
    "        best_modelsDF = pd.read_html(str(best_models_table))[0] #don't know why the [0] is necessary\n",
    "        # [x] update existing best_models_table\n",
    "        y_min_inc_subDF = best_modelsDF.loc[best_modelsDF.y == y_min_inc]\n",
    "        best_plots_div = summary_soup.select('#best_plots')[0]\n",
    "        if len(y_min_inc_subDF) == 0: #we have no best_model for this y_min_inc yet\n",
    "            best_modelsDF = best_modelsDF.append(pd.DataFrame(new_potential_row), ignore_index=True)\n",
    "            # [x] add plot\n",
    "            model_plot = summary_soup.new_tag('img', id=str(y_min_inc)+'min_plot', \n",
    "                                      src=config_dir+'Plots/'+model_name+'_'+str(y_min_inc)+'min_val_MSE.png',\n",
    "                                      height='400')\n",
    "            best_plots_div.append(model_plot)\n",
    "            # re-sort plots accd to y_min_inc?\n",
    "        else:\n",
    "            best_val_mse_imprv = y_min_inc_subDF['% Imprv ZM'].iloc[0]\n",
    "            if new_potential_row['% Imprv ZM'] > best_val_mse_imprv:\n",
    "                print('Updating best models for y='+str(y_min_inc)+': '+\n",
    "                      new_potential_row['Name'][0]+' ('+str(new_potential_row['Val MSE'][0])+') beats '+\n",
    "                      y_min_inc_subDF.Name.iloc[0]+' ('+str(y_min_inc_subDF['Val MSE'].values[0])+')')\n",
    "                # [x] update table\n",
    "                best_modelsDF = best_modelsDF.loc[best_modelsDF.y != y_min_inc]\n",
    "                best_modelsDF = best_modelsDF.append(pd.DataFrame(new_potential_row), ignore_index=True)\n",
    "                # [x] update plots\n",
    "                y_min_inc_plot = best_plots_div.select('#min'+str(y_min_inc)+'_plot')\n",
    "                y_min_inc_plot['src']=config_dir+'Plots/'+model_name+'_'+str(y_min_inc)+'min_val_MSE.png' #this update step isn't working\n",
    "        # re-sort DF and replace the html\n",
    "        print(best_modelsDF)\n",
    "        best_modelsDF = best_modelsDF.sort_values('y').reset_index(drop=True)[col_order]\n",
    "        best_models_html = best_modelsDF.to_html(table_id = 'best_models_table', index=False, col_space=50, justify='center')\n",
    "        best_models_soup = Soup(best_models_html, 'html.parser')\n",
    "        best_models_table.replace_with(best_models_soup)\n",
    "    # All Models\n",
    "    all_models_table = summary_soup.select(\"#all_models_table\")\n",
    "    if len(all_models_table) == 0:\n",
    "        # [x] create All Models if it doesn't exist\n",
    "        print('No all_models_table found. Creating.')\n",
    "        all_models = summary_soup.new_tag('h3', id='all_models')\n",
    "        all_models.string = 'All Models'\n",
    "        config_soup = findMod(summary_soup, 'h1', 'Configuration')\n",
    "        config_soup.insert_before(all_models) \n",
    "        all_models = summary_soup.select('#all_models')[0] #I don't know why we have to redefine this after the insert but we do\n",
    "        all_modelsDF = pd.DataFrame(new_potential_row)\n",
    "        all_models_html = all_modelsDF.to_html(table_id = 'all_models_table', index=False, col_space=50, justify='center')\n",
    "        all_models_soup = Soup(all_models_html)\n",
    "        all_models.insert_after(all_models_soup)\n",
    "    else:\n",
    "        # [x] add to All Models\n",
    "        all_models_table = all_models_table[0]\n",
    "        all_modelsDF = pd.read_html(str(all_models_table))[0] #don't know why the [0] is necessary\n",
    "        all_modelsDF = all_modelsDF.append(pd.DataFrame(new_potential_row), ignore_index=True)\n",
    "        all_modelsDF.iloc[all_modelsDF['Name'].map(model_order_dict).argsort()] #sort first by model_name\n",
    "        all_modelsDF = all_modelsDF.sort_values('y').reset_index(drop=True)[col_order]\n",
    "        all_models_soup = Soup(all_modelsDF.to_html(table_id = 'all_models_table', index=False, col_space=50, justify='center'))\n",
    "        all_models_table.replace_with(all_models_soup)\n",
    "    # print(summary_soup.prettify())\n",
    "    return(summary_soup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateModelDetails(summary_soup, model_name, config_dir, y_min_inc,\n",
    "                       train_R2, train_mse, train_mse_by_dateDF, val_mse, val_mse_by_dateDF, summaryDF,\n",
    "                      zm_train_R2, zm_train_mse, zm_train_mse_by_dateDF, zm_val_mse, zm_val_mse_by_dateDF):\n",
    "    print('in updateModelDetails')\n",
    "    # [x] create model results html\n",
    "    html_id_friendly_model_name = model_name if model_name[0] != '[' else getHTMLIDFromKerasModelName(model_name)\n",
    "    \n",
    "    name_tag = summary_soup.new_tag('h4', id=html_id_friendly_model_name+str(y_min_inc))\n",
    "    name_tag.string=model_name\n",
    "    new_row =  {'y':[y_min_inc], 'Name':[model_name], '% Imprv ZM':[round((zm_val_mse-val_mse)/zm_val_mse, 3)], 'Val MSE':[round(val_mse, 4)], 'ZM Val MSE':[round(zm_val_mse, 4)], 'Train R2':[round(train_R2, 4)], 'Val R2':[np.nan], 'Train MSE':[round(train_mse,4)], 'ZM Train MSE':[round(zm_train_mse,4)]}\n",
    "    results_table_html = pd.DataFrame(new_row).to_html(index=False, col_space=50, justify='center')\n",
    "    # [x] plot\n",
    "    model_plot = summary_soup.new_tag('img', \n",
    "                                  src=config_dir+'Plots/'+model_name+'_'+str(y_min_inc)+'min_val_MSE.png',\n",
    "                                  height='400')\n",
    "    # [x] summary\n",
    "    has_summary = (summaryDF is not None)\n",
    "    model_summary_html = summaryDF.to_html() if has_summary else '<p> No model summaryDF available </p>'\n",
    "    model_summary_soup = Soup(model_summary_html)\n",
    "    if has_summary:\n",
    "        # make the sumamry table scrollable\n",
    "        model_summary_soup.thead['style'] = 'display:block;'\n",
    "        model_summary_soup.tbody['style'] = 'height:300px; overflow-y:scroll; display:block;'    \n",
    "    min_chg_h3 = findMod(summary_soup, 'h3', str(y_min_inc)+' Minute Chg')\n",
    "    if min_chg_h3 is None:\n",
    "        # [x] create h3\n",
    "        new_min_chg_h3_tag = summary_soup.new_tag('h3')\n",
    "        new_min_chg_h3_tag.string=str(y_min_inc)+' Minute Chg'\n",
    "        #insert new h3 to the end\n",
    "        summary_soup.body.insert(len(summary_soup.body.contents), new_min_chg_h3_tag)\n",
    "        new_min_chg_h3_tag = findMod(summary_soup, 'h3', str(y_min_inc)+' Minute Chg')\n",
    "        new_min_chg_h3_tag.insert_after(name_tag)        \n",
    "    else:\n",
    "        # [x] add model results onto the end of h3\n",
    "        next_h3 = min_chg_h3.find_next_sibling('h3')\n",
    "        if next_h3 is None:\n",
    "            #just add to the end of the document. We are at the end.\n",
    "            summary_soup.body.insert(len(summary_soup.body.contents), name_tag)\n",
    "        else:\n",
    "            next_h3.insert_before(name_tag)\n",
    "    # find name_tag and insert results table, plot, and summary\n",
    "    name_tag = summary_soup.select('#'+html_id_friendly_model_name+str(y_min_inc))[0]\n",
    "    name_tag.insert_after(model_summary_soup)\n",
    "    name_tag = summary_soup.select('#'+html_id_friendly_model_name+str(y_min_inc))[0]\n",
    "    name_tag.insert_after(model_plot)\n",
    "    name_tag = summary_soup.select('#'+html_id_friendly_model_name+str(y_min_inc))[0]\n",
    "    name_tag.insert_after(Soup(results_table_html))\n",
    "    return summary_soup\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordModelResults(model_name, config_dir, y_min_inc,\n",
    "                       train_R2, train_mse, train_mse_by_dateDF, val_mse, val_mse_by_dateDF, summaryDF,\n",
    "                      zm_train_R2, zm_train_mse, zm_train_mse_by_dateDF, zm_val_mse, zm_val_mse_by_dateDF):\n",
    "    print('recording '+model_name+' results')\n",
    "    model_plot = graphDailyMSE(val_mse_by_dateDF, zm_val_mse_by_dateDF, graph_title='y='+str(y_min_inc)+' Val: '+model_name+' vs ZM')\n",
    "    model_plot.figure.savefig(config_dir+'Plots/'+model_name+'_'+str(y_min_inc)+'min_val_MSE.png', dpi=256)\n",
    "    summary_sheet_fn = config_dir+'summary_sheet.html'\n",
    "    summary_sheet_html = 'not set yet'\n",
    "    with open(summary_sheet_fn,'r') as fh:\n",
    "        summary_sheet_html = fh.read()\n",
    "    summary_soup = Soup(summary_sheet_html)\n",
    "    # [x] update Results Summary\n",
    "    summary_soup = updateResultsSummary(summary_soup, model_name, config_dir, y_min_inc,\n",
    "                       train_R2, train_mse, train_mse_by_dateDF, val_mse, val_mse_by_dateDF, summaryDF,\n",
    "                      zm_train_R2, zm_train_mse, zm_train_mse_by_dateDF, zm_val_mse, zm_val_mse_by_dateDF)\n",
    "    # [] update Model Details\n",
    "    summary_soup = updateModelDetails(summary_soup, model_name, config_dir, y_min_inc,\n",
    "                       train_R2, train_mse, train_mse_by_dateDF, val_mse, val_mse_by_dateDF, summaryDF,\n",
    "                      zm_train_R2, zm_train_mse, zm_train_mse_by_dateDF, zm_val_mse, zm_val_mse_by_dateDF)\n",
    "    print('Rewriting html to '+summary_sheet_fn)\n",
    "    with open(summary_sheet_fn,'w') as fh:\n",
    "        fh.write(summary_soup.prettify())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# progressKerasModels(trainX, trainY, valX, valY, config_dir, keras_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressModels(trainX, trainY, valX, valY, config_dir):\n",
    "#     'simple_LM', 'ridge_LM', 'norm_ridge_LM', 'lasso_LM', norm_lasso_LM', 'random_forest', 'XGBoost'\n",
    "    func_dict = {'ZM': zeroModel, 'simple_LM': simpleLM, 'ridge_LM': ridgeLM, 'lasso_LM': lassoLM,\n",
    "                 'norm_ridge_LM': normRidgeLM, 'norm_lasso_LM': normLassoLM}#, 'random_forest': randomForest}\n",
    "    to_do_dict = getModelToDoDict(config_dir, trainY)\n",
    "    y_min_incs_to_go = sorted(to_do_dict.keys())\n",
    "    for y_min_inc in y_min_incs_to_go:\n",
    "        print('\\n'+'='*7+' y Minute '+str(y_min_inc)+' '+'='*7)\n",
    "        trainY_min_inc, valY_min_inc = formatYData(trainY, valY, y_min_inc)\n",
    "        zm_train_R2, zm_train_mse, zm_train_mse_by_dateDF, zm_val_mse, zm_val_mse_by_dateDF, _ = \\\n",
    "            zeroModel(trainX, trainY_min_inc, valX, valY_min_inc)\n",
    "        for model_to_go in to_do_dict[y_min_inc]:\n",
    "            if model_to_go in func_dict.keys():\n",
    "                train_R2, train_mse, train_mse_by_dateDF, val_mse, val_mse_by_dateDF, summaryDF = \\\n",
    "                    func_dict[model_to_go](trainX, trainY_min_inc, valX, valY_min_inc) # finds the appropriate fxn and calls it\n",
    "                recordModelResults(model_to_go, config_dir, y_min_inc,\n",
    "                                   train_R2, train_mse, train_mse_by_dateDF, val_mse, val_mse_by_dateDF, summaryDF,\n",
    "                                  zm_train_R2, zm_train_mse, zm_train_mse_by_dateDF, zm_val_mse, zm_val_mse_by_dateDF)\n",
    "            else:\n",
    "                print('ALERT!: '+model_to_go+' not defined in func_dict!')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressKerasModels(trainX, trainY, valX, valY, config_dir, keras_models):\n",
    "    keras_model_strs = [kerasModelArrayToStr(m) for m in keras_models]\n",
    "    to_do_dict = getKerasModelToDoDict(config_dir, trainY)\n",
    "    y_min_incs_to_go = sorted(to_do_dict.keys())\n",
    "    for y_min_inc in y_min_incs_to_go:\n",
    "        print('\\n'+'='*7+' y Minute '+str(y_min_inc)+' '+'='*7)\n",
    "        trainY_min_inc, valY_min_inc = formatYData(trainY, valY, y_min_inc)\n",
    "        zm_train_R2, zm_train_mse, zm_train_mse_by_dateDF, zm_val_mse, zm_val_mse_by_dateDF, _ = \\\n",
    "            zeroModel(trainX, trainY_min_inc, valX, valY_min_inc)\n",
    "        for model_to_go in to_do_dict[y_min_inc]:\n",
    "            model_array = keras_models[keras_model_strs.index(model_to_go)]\n",
    "            train_R2, train_mse, train_mse_by_dateDF, val_mse, val_mse_by_dateDF, summaryDF = \\\n",
    "                kerasModel(model_array, trainX, trainY_min_inc, y_min_inc, valX, valY_min_inc, config_dir)\n",
    "            recordModelResults(model_to_go, config_dir, y_min_inc,\n",
    "                                   train_R2, train_mse, train_mse_by_dateDF, val_mse, val_mse_by_dateDF, summaryDF,\n",
    "                                  zm_train_R2, zm_train_mse, zm_train_mse_by_dateDF, zm_val_mse, zm_val_mse_by_dateDF)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterateLearning(sec, combDF, omni_dir=omni_dir, email_progress=False):\n",
    "    \"\"\"\n",
    "    1. Creates the file structure if it doesn't exist.\n",
    "    2. Locates or creates the data in the appropriate format\n",
    "    3. Finds the most recent model progress, if any, and progresses remaining models.\n",
    "    4. Saves results\n",
    "    5. Optionally emails results\n",
    "    \"\"\"\n",
    "    comb_subDF = combDF.loc[combDF.Sec1 == sec].reset_index(drop=True)\n",
    "    print('='*50+'\\nIterating learning on '+sec+' with '+str(len(comb_subDF))+' rows:\\n\\n'+str(comb_subDF.head())+'\\n'+'='*50)\n",
    "    for i in comb_subDF.index:\n",
    "        # [x] 1. Creates the file structure if it doesn't exist.\n",
    "        config_dir, data_dir, summary_sheet_fn, progress_notes_fn, log_fn, deep_models_dir = locateConfigDir(comb_subDF.loc[i], omni_dir)\n",
    "        # [x] 2. Locates or creates the data in the appropriate format\n",
    "        trainX, trainY, valX, valY = loadData(comb_subDF.loc[i], omni_dir, config_dir, tdm_dir+'Minute_Files/')\n",
    "        # [x] 3. Finds the most recent model progress, if any, and progresses remaining models.\n",
    "        # [x] 4. Saves results\n",
    "        progressModels(trainX, trainY, valX, valY, config_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing matching config dir found: /home/andrew/All_Trading/Studies/Omni_Project/US_Dollar_Index/Config1\n",
      "No pre-loaded data found. Loading data for US_Dollar_Index and EURUSD,USDCHF,AUDUSD,NZDUSD,USDSEK,USDJPY,GBPUSD,USDCAD\n",
      "Loading minuteDF for EURUSD\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "MemoryError()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/env1/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getbool_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1442\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdetail\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env1/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[1;32m   3600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3601\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env1/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5252\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env1/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_protect_consolidate\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   5240\u001b[0m         \u001b[0mblocks_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5241\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mblocks_before\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env1/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[1;32m   5249\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5250\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env1/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mconsolidate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env1/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    936\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env1/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_consolidate\u001b[0;34m(blocks)\u001b[0m\n\u001b[1;32m   1912\u001b[0m         merged_blocks = _merge_blocks(\n\u001b[0;32m-> 1913\u001b[0;31m             \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_can_consolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1914\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/env1/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36m_merge_blocks\u001b[0;34m(blocks, dtype, _can_consolidate)\u001b[0m\n\u001b[1;32m   3322\u001b[0m         \u001b[0margsort\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3323\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3324\u001b[0m         \u001b[0mnew_mgr_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-40e0ad61cd03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m190\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mconfig_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_sheet_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_notes_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeep_models_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocateConfigDir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0momni_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0momni_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtdm_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'Minute_Files/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-58-7ff83d2bca2a>\u001b[0m in \u001b[0;36mloadData\u001b[0;34m(comb_row, omni_dir, config_dir, minute_dir)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mother_secs_minuteDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mother_secs_minuteDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminute_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msec\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Minute'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Product'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Minute'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'O_B'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'O_A'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'H_B'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'H_A'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'L_B'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'L_A'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'C_B'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'C_A'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'B_TickImb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'A_TickImb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'M_TickImb'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mother_secs_minuteDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mother_secs_minuteDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mother_secs_minuteDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mother_secs_minuteDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother_secs_minuteDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDate\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mcomb_row\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainStartDate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mother_secs_minuteDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDate\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcomb_row\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mValEndDate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'other_secs_minuteDF has '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother_secs_minuteDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' rows.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sec1_minuteDF has '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msec1_minuteDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' rows.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env1/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1424\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env1/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1797\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1798\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1799\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getbool_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1800\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env1/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getbool_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1441\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdetail\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetail\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: MemoryError()"
     ]
    }
   ],
   "source": [
    "idx = 190 \n",
    "config_dir, data_dir, summary_sheet_fn, progress_notes_fn, log_fn, deep_models_dir = locateConfigDir(combDF.iloc[idx], omni_dir)\n",
    "trainX, trainY, valX, valY = loadData(combDF.iloc[idx], omni_dir, config_dir, tdm_dir+'Minute_Files/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY_min_inc, valY_min_inc = formatYData(trainY, valY, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progressKerasModels(trainX, trainY, valX, valY, config_dir, keras_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env1)",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
